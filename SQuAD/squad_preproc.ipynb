{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, nltk, io, pickle\n",
    "import numpy as np\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open('/pio/data/data/squad/train-v1.1.json', 'r', encoding='utf-8') as f:\n",
    "    train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open('/pio/data/data/squad/dev-v1.1.json', 'r', encoding='utf-8') as f:\n",
    "    dev = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'answer_start': 177, u'text': u'Denver Broncos'},\n",
       " {u'answer_start': 177, u'text': u'Denver Broncos'},\n",
       " {u'answer_start': 177, u'text': u'Denver Broncos'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev['data'][0]['paragraphs'][0]['qas'][0]['answers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dev['data'][0]['paragraphs'][0]['qas'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'question', u'id', u'answers']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev['data'][0]['paragraphs'][0]['qas'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"The State Council declared a three-day period of national mourning for the quake victims starting from May 19 , 2008 ; the PRC 's National Flag and Regional Flags of Hong Kong and Macau Special Administrative Regions flown at half mast\",\n",
       " u'It was the first time that a national mourning period had been declared for something other than the death of a state leader , and many have called it the biggest display of mourning since the death of Mao Zedong',\n",
       " u'At 14:28 CST on May 19 , 2008 , a week after the earthquake , the Chinese public held a moment of silence',\n",
       " u'People stood silent for three minutes while air defense , police and fire sirens , and the horns of vehicles , vessels and trains sounded',\n",
       " u\"Cars and trucks on Beijing 's roads also came to a halt\",\n",
       " u\"People spontaneously burst into cheering `` Zhongguo jiayou ! '' ( Let 's go , China ! ) and `` Sichuan jiayou '' ( Let 's go , Sichuan ! ) afterwards .\"]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(nltk.word_tokenize(train['data'][10]['paragraphs'][60]['context'])).split(' . ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save glove vectors as npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vec = []\n",
    "\n",
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        glove_vec.append(np.matrix(str(' '.join(line.split()[1:]))))\n",
    "        \n",
    "glove_vec = np.vstack(glove_vec).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/pio/data/data/glove_vec/6B/glove.6B.300d', glove_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a glove wordlist\n",
    "\n",
    "glove_words = []\n",
    "\n",
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.300d.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        glove_words.append(line.split()[0])\n",
    "        \n",
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.wordlist.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in glove_words:\n",
    "        f.write(unicode(w + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess wikipedia negative examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose neg example for (almost) each question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with io.open('/pio/data/data/squad/train-v1.1.json', 'r', encoding='utf-8') as f:\n",
    "    train = json.load(f)\n",
    "    \n",
    "data_simple = []\n",
    "\n",
    "for par in train['data']:    \n",
    "    for con in par['paragraphs']:        \n",
    "        for q in con['qas']:\n",
    "            question = q['question']            \n",
    "            answers = []\n",
    "            \n",
    "            for ans in q['answers']:\n",
    "                text = ans['text']\n",
    "                ans_start = ans['answer_start']\n",
    "                \n",
    "                answers.append((ans_start, text))\n",
    "                \n",
    "            data_simple.append([answers[0][1], question])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Saint Bernadette Soubirous',\n",
       " u'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_simple[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_pars = np.load('/pio/data/data/squad/wiki_negative_train/wiki_train_pars_prototype.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wiki_pars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No negative example for question 191\n",
      "No negative example for question 200\n",
      "No negative example for question 458\n",
      "No negative example for question 764\n",
      "No negative example for question 1090\n",
      "No negative example for question 1653\n",
      "No negative example for question 2491\n",
      "No negative example for question 2626\n",
      "No negative example for question 2628\n",
      "No negative example for question 2976\n",
      "No negative example for question 2988\n",
      "No negative example for question 3040\n",
      "No negative example for question 3286\n",
      "No negative example for question 4510\n",
      "No negative example for question 4729\n",
      "No negative example for question 5195\n",
      "No negative example for question 5231\n",
      "No negative example for question 5411\n",
      "No negative example for question 5620\n",
      "No negative example for question 5686\n",
      "No negative example for question 5699\n",
      "No negative example for question 6329\n",
      "No negative example for question 6847\n",
      "No negative example for question 7242\n",
      "No negative example for question 7437\n",
      "No negative example for question 7455\n",
      "No negative example for question 7788\n",
      "No negative example for question 7789\n",
      "No negative example for question 8429\n",
      "No negative example for question 10699\n",
      "No negative example for question 10787\n",
      "No negative example for question 10837\n",
      "No negative example for question 10842\n",
      "No negative example for question 11616\n",
      "No negative example for question 11803\n",
      "No negative example for question 12015\n",
      "No negative example for question 12045\n",
      "No negative example for question 13458\n",
      "No negative example for question 14473\n",
      "No negative example for question 14730\n",
      "No negative example for question 15304\n",
      "No negative example for question 15414\n",
      "No negative example for question 15420\n",
      "No negative example for question 15481\n",
      "No negative example for question 15612\n",
      "No negative example for question 15865\n",
      "No negative example for question 15930\n",
      "No negative example for question 16582\n",
      "No negative example for question 16679\n",
      "No negative example for question 16820\n",
      "No negative example for question 16822\n",
      "No negative example for question 16888\n",
      "No negative example for question 16998\n",
      "No negative example for question 17435\n",
      "No negative example for question 18138\n",
      "No negative example for question 18521\n",
      "No negative example for question 19096\n",
      "No negative example for question 19242\n",
      "No negative example for question 19266\n",
      "No negative example for question 19272\n",
      "No negative example for question 19676\n",
      "No negative example for question 19849\n",
      "No negative example for question 19898\n",
      "No negative example for question 19937\n",
      "No negative example for question 20071\n",
      "No negative example for question 20256\n",
      "No negative example for question 20495\n",
      "No negative example for question 20504\n",
      "No negative example for question 20819\n",
      "No negative example for question 21128\n",
      "No negative example for question 21303\n",
      "No negative example for question 21326\n",
      "No negative example for question 21452\n",
      "No negative example for question 21456\n",
      "No negative example for question 21476\n",
      "No negative example for question 21681\n",
      "No negative example for question 22184\n",
      "No negative example for question 22331\n",
      "No negative example for question 22645\n",
      "No negative example for question 22646\n",
      "No negative example for question 22650\n",
      "No negative example for question 22670\n",
      "No negative example for question 22694\n",
      "No negative example for question 22700\n",
      "No negative example for question 22702\n",
      "No negative example for question 22778\n",
      "No negative example for question 22785\n",
      "No negative example for question 23013\n",
      "No negative example for question 23305\n",
      "No negative example for question 23476\n",
      "No negative example for question 23526\n",
      "No negative example for question 23612\n",
      "No negative example for question 23631\n",
      "No negative example for question 23949\n",
      "No negative example for question 24154\n",
      "No negative example for question 24426\n",
      "No negative example for question 24503\n",
      "No negative example for question 24515\n",
      "No negative example for question 24517\n",
      "No negative example for question 24529\n",
      "No negative example for question 24599\n",
      "No negative example for question 24799\n",
      "No negative example for question 24809\n",
      "No negative example for question 24869\n",
      "No negative example for question 25089\n",
      "No negative example for question 26012\n",
      "No negative example for question 26119\n",
      "No negative example for question 26342\n",
      "No negative example for question 26423\n",
      "No negative example for question 26915\n",
      "No negative example for question 27207\n",
      "No negative example for question 27238\n",
      "No negative example for question 27359\n",
      "No negative example for question 27596\n",
      "No negative example for question 28099\n",
      "No negative example for question 28124\n",
      "No negative example for question 28609\n",
      "No negative example for question 28705\n",
      "No negative example for question 28753\n",
      "No negative example for question 28827\n",
      "No negative example for question 28828\n",
      "No negative example for question 28829\n",
      "No negative example for question 29841\n",
      "No negative example for question 30027\n",
      "No negative example for question 30105\n",
      "No negative example for question 30774\n",
      "No negative example for question 30777\n",
      "No negative example for question 30786\n",
      "No negative example for question 30805\n",
      "No negative example for question 30812\n",
      "No negative example for question 30820\n",
      "No negative example for question 30866\n",
      "No negative example for question 30898\n",
      "No negative example for question 31313\n",
      "No negative example for question 31427\n",
      "No negative example for question 31463\n",
      "No negative example for question 31569\n",
      "No negative example for question 31772\n",
      "No negative example for question 31776\n",
      "No negative example for question 32184\n",
      "No negative example for question 33051\n",
      "No negative example for question 33617\n",
      "No negative example for question 33938\n",
      "No negative example for question 33944\n",
      "No negative example for question 34104\n",
      "No negative example for question 34150\n",
      "No negative example for question 34341\n",
      "No negative example for question 34521\n",
      "No negative example for question 35055\n",
      "No negative example for question 35093\n",
      "No negative example for question 35494\n",
      "No negative example for question 35872\n",
      "No negative example for question 36548\n",
      "No negative example for question 38123\n",
      "No negative example for question 38217\n",
      "No negative example for question 38317\n",
      "No negative example for question 38603\n",
      "No negative example for question 38705\n",
      "No negative example for question 38711\n",
      "No negative example for question 38928\n",
      "No negative example for question 39024\n",
      "No negative example for question 39192\n",
      "No negative example for question 40224\n",
      "No negative example for question 40402\n",
      "No negative example for question 40425\n",
      "No negative example for question 40896\n",
      "No negative example for question 41364\n",
      "No negative example for question 41465\n",
      "No negative example for question 42680\n",
      "No negative example for question 42884\n",
      "No negative example for question 44025\n",
      "No negative example for question 45404\n",
      "No negative example for question 45738\n",
      "No negative example for question 45951\n",
      "No negative example for question 45975\n",
      "No negative example for question 46012\n",
      "No negative example for question 46013\n",
      "No negative example for question 46031\n",
      "No negative example for question 46056\n",
      "No negative example for question 46083\n",
      "No negative example for question 46155\n",
      "No negative example for question 46178\n",
      "No negative example for question 46225\n",
      "No negative example for question 46595\n",
      "No negative example for question 46602\n",
      "No negative example for question 47102\n",
      "No negative example for question 47777\n",
      "No negative example for question 49135\n",
      "No negative example for question 49158\n",
      "No negative example for question 49293\n",
      "No negative example for question 49370\n",
      "No negative example for question 49948\n",
      "No negative example for question 50158\n",
      "No negative example for question 50257\n",
      "No negative example for question 50605\n",
      "No negative example for question 50809\n",
      "No negative example for question 51263\n",
      "No negative example for question 51285\n",
      "No negative example for question 51322\n",
      "No negative example for question 51838\n",
      "No negative example for question 53480\n",
      "No negative example for question 54116\n",
      "No negative example for question 54176\n",
      "No negative example for question 54472\n",
      "No negative example for question 54480\n",
      "No negative example for question 54481\n",
      "No negative example for question 54537\n",
      "No negative example for question 54783\n",
      "No negative example for question 54840\n",
      "No negative example for question 55764\n",
      "No negative example for question 56115\n",
      "No negative example for question 57248\n",
      "No negative example for question 58452\n",
      "No negative example for question 58757\n",
      "No negative example for question 58758\n",
      "No negative example for question 59779\n",
      "No negative example for question 60525\n",
      "No negative example for question 61199\n",
      "No negative example for question 61905\n",
      "No negative example for question 62107\n",
      "No negative example for question 62699\n",
      "No negative example for question 62944\n",
      "No negative example for question 62956\n",
      "No negative example for question 63004\n",
      "No negative example for question 63030\n",
      "No negative example for question 63168\n",
      "No negative example for question 63238\n",
      "No negative example for question 63307\n",
      "No negative example for question 63717\n",
      "No negative example for question 64015\n",
      "No negative example for question 64027\n",
      "No negative example for question 64166\n",
      "No negative example for question 64469\n",
      "No negative example for question 64599\n",
      "No negative example for question 64791\n",
      "No negative example for question 64796\n",
      "No negative example for question 64962\n",
      "No negative example for question 65830\n",
      "No negative example for question 66199\n",
      "No negative example for question 66653\n",
      "No negative example for question 67443\n",
      "No negative example for question 67455\n",
      "No negative example for question 67457\n",
      "No negative example for question 67579\n",
      "No negative example for question 67794\n",
      "No negative example for question 68217\n",
      "No negative example for question 68719\n",
      "No negative example for question 69482\n",
      "No negative example for question 69494\n",
      "No negative example for question 69522\n",
      "No negative example for question 69946\n",
      "No negative example for question 70082\n",
      "No negative example for question 70771\n",
      "No negative example for question 70870\n",
      "No negative example for question 70912\n",
      "No negative example for question 71212\n",
      "No negative example for question 71268\n",
      "No negative example for question 71324\n",
      "No negative example for question 71353\n",
      "No negative example for question 72092\n",
      "No negative example for question 72093\n",
      "No negative example for question 72489\n",
      "No negative example for question 74443\n",
      "No negative example for question 74533\n",
      "No negative example for question 74866\n",
      "No negative example for question 74964\n",
      "No negative example for question 76525\n",
      "No negative example for question 76597\n",
      "No negative example for question 77110\n",
      "No negative example for question 77396\n",
      "No negative example for question 77604\n",
      "No negative example for question 77979\n",
      "No negative example for question 78628\n",
      "No negative example for question 78652\n",
      "No negative example for question 78657\n",
      "No negative example for question 78678\n",
      "No negative example for question 79213\n",
      "No negative example for question 79324\n",
      "No negative example for question 79405\n",
      "No negative example for question 79547\n",
      "No negative example for question 79611\n",
      "No negative example for question 79710\n",
      "No negative example for question 80396\n",
      "No negative example for question 80570\n",
      "No negative example for question 81534\n",
      "No negative example for question 81896\n",
      "No negative example for question 82155\n",
      "No negative example for question 82449\n",
      "No negative example for question 82507\n",
      "No negative example for question 82569\n",
      "No negative example for question 82694\n",
      "No negative example for question 82730\n",
      "No negative example for question 82821\n",
      "No negative example for question 83115\n",
      "No negative example for question 83195\n",
      "No negative example for question 83562\n",
      "No negative example for question 84546\n",
      "No negative example for question 85284\n",
      "No negative example for question 85285\n",
      "No negative example for question 85340\n",
      "No negative example for question 85341\n",
      "No negative example for question 85342\n",
      "No negative example for question 85345\n",
      "No negative example for question 86147\n",
      "No negative example for question 86200\n",
      "No negative example for question 86606\n",
      "No negative example for question 86734\n",
      "No negative example for question 86735\n",
      "No negative example for question 87353\n"
     ]
    }
   ],
   "source": [
    "data_neg = []\n",
    "\n",
    "for i in xrange(len(data_simple)):\n",
    "    a, q = data_simple[i]\n",
    "    found = False\n",
    "    for _, p, _ in wiki_pars[i]:\n",
    "        p = p.decode('utf8')\n",
    "        if a not in p:\n",
    "            found = True\n",
    "            data_neg.append([i, q, p])\n",
    "            break\n",
    "    if not found and wiki_pars[i]:\n",
    "        print \"No negative example for question\", i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84882, 87599)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_neg), len(data_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[698,\n",
       " u'Beyonce was coached for her Spanish songs by which American?',\n",
       " u\"Just 2 weeks into the American Civil War ,  Alton played an important part in the infamous Camp Jackson Affair ,  which in large part led to the eviction of Missouri Governor Claiborne Fox Jackson from office .  The State of Missouri's nominal neutrality was tested in a conflict over the St .  Louis Arsenal .  The Federal Government reinforced the Arsenal's tiny garrison with several detachments ,  most notably a force from the 2nd Infantry under Captain Nathaniel Lyon .  Concerned by widespread reports that Governor Jackson intended to use the Missouri Volunteer Militia to attack the Arsenal  ( and capture its 39 , 000 small arms )  ,  Secretary of War Simon Cameron ordered Lyon  ( by that time in acting command )  to evacuate the majority of the munitions to Illinois .  21 , 000 guns were secretly evacuated to Alton ,  IL on the evening of April 29 ,  1861 .\"]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_neg[672]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Rudy Perez',\n",
       " u'Beyonce was coached for her Spanish songs by which American?']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_simple[698]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/wiki_negative_train/negative_paragraphs.pkl', 'w') as f:\n",
    "    pickle.dump(data_neg, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add not_a_word token to existing pkls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_with_glove_vocab = np.load('/pio/data/data/squad/train_with_glove_vocab.pkl')\n",
    "train_bin_feats = np.load('/pio/data/data/squad/train_bin_feats.pkl')\n",
    "train_char_ascii = np.load('/pio/data/data/squad/train_char_ascii.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_a_word_Word = 400001\n",
    "not_a_word_Char = 3\n",
    "\n",
    "for i in xrange(len(data_simple)):\n",
    "    train_with_glove_vocab[i][2].append(not_a_word_Word)\n",
    "    train_bin_feats[i].append(False)\n",
    "    train_char_ascii[i][1].append([1, not_a_word_Char, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " False,\n",
       " True,\n",
       " False,\n",
       " False]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bin_feats[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/wiki_negative_train/train_neg_with_glove_vocab.pkl', 'w') as f:\n",
    "    pickle.dump(train_with_glove_vocab, f)\n",
    "    \n",
    "with open('/pio/data/data/squad/wiki_negative_train/train_neg_bin_feats.pkl', 'w') as f:\n",
    "    pickle.dump(train_bin_feats, f)\n",
    "    \n",
    "with open('/pio/data/data/squad/wiki_negative_train/train_char_ascii.pkl', 'w') as f:\n",
    "    pickle.dump(train_char_ascii, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add not_a_word to dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_a_word = u'<not_a_word>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_main = np.load('/pio/data/data/squad/dev.pkl')\n",
    "dev_ascii = np.load('/pio/data/data/squad/dev_char_ascii.pkl')\n",
    "dev_bin = np.load('/pio/data/data/squad/dev_bin_feats.pkl')\n",
    "dev_set = np.load('/pio/data/data/squad/dev_with_glove_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(dev_main)):\n",
    "    dev_main[i][2] = dev_main[i][2][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(dev_main)):\n",
    "    dev_main[i][2].append(not_a_word)\n",
    "    dev_set[i][2].append(not_a_word_Word)\n",
    "    dev_bin[i].append(False)\n",
    "    dev_ascii[i][1].append([1, not_a_word_Char, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/wiki_negative_dev/dev.pkl', 'w') as f:\n",
    "    pickle.dump(dev_main, f)\n",
    "    \n",
    "with open('/pio/data/data/squad/wiki_negative_dev/dev_char_ascii.pkl', 'w') as f:\n",
    "    pickle.dump(dev_ascii, f)\n",
    "\n",
    "with open('/pio/data/data/squad/wiki_negative_dev/dev_bin_feats.pkl', 'w') as f:\n",
    "    pickle.dump(dev_bin, f)\n",
    "    \n",
    "with open('/pio/data/data/squad/wiki_negative_dev/dev_with_glove_vocab.pkl', 'w') as f:\n",
    "    pickle.dump(dev_set, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'super',\n",
       " u'bowl',\n",
       " u'50',\n",
       " u'was',\n",
       " u'an',\n",
       " u'american',\n",
       " u'football',\n",
       " u'game',\n",
       " u'to',\n",
       " u'determine',\n",
       " u'the',\n",
       " u'champion',\n",
       " u'of',\n",
       " u'the',\n",
       " u'national',\n",
       " u'football',\n",
       " u'league',\n",
       " u'(',\n",
       " u'nfl',\n",
       " u')',\n",
       " u'for',\n",
       " u'the',\n",
       " u'2015',\n",
       " u'season',\n",
       " u'.',\n",
       " u'the',\n",
       " u'american',\n",
       " u'football',\n",
       " u'conference',\n",
       " u'(',\n",
       " u'afc',\n",
       " u')',\n",
       " u'champion',\n",
       " u'denver',\n",
       " u'broncos',\n",
       " u'defeated',\n",
       " u'the',\n",
       " u'national',\n",
       " u'football',\n",
       " u'conference',\n",
       " u'(',\n",
       " u'nfc',\n",
       " u')',\n",
       " u'champion',\n",
       " u'carolina',\n",
       " u'panthers',\n",
       " u'24\\u201310',\n",
       " u'to',\n",
       " u'earn',\n",
       " u'their',\n",
       " u'third',\n",
       " u'super',\n",
       " u'bowl',\n",
       " u'title',\n",
       " u'.',\n",
       " u'the',\n",
       " u'game',\n",
       " u'was',\n",
       " u'played',\n",
       " u'on',\n",
       " u'february',\n",
       " u'7',\n",
       " u',',\n",
       " u'2016',\n",
       " u',',\n",
       " u'at',\n",
       " u'levi',\n",
       " u\"'s\",\n",
       " u'stadium',\n",
       " u'in',\n",
       " u'the',\n",
       " u'san',\n",
       " u'francisco',\n",
       " u'bay',\n",
       " u'area',\n",
       " u'at',\n",
       " u'santa',\n",
       " u'clara',\n",
       " u',',\n",
       " u'california',\n",
       " u'.',\n",
       " u'as',\n",
       " u'this',\n",
       " u'was',\n",
       " u'the',\n",
       " u'50th',\n",
       " u'super',\n",
       " u'bowl',\n",
       " u',',\n",
       " u'the',\n",
       " u'league',\n",
       " u'emphasized',\n",
       " u'the',\n",
       " u'``',\n",
       " u'golden',\n",
       " u'anniversary',\n",
       " u\"''\",\n",
       " u'with',\n",
       " u'various',\n",
       " u'gold-themed',\n",
       " u'initiatives',\n",
       " u',',\n",
       " u'as',\n",
       " u'well',\n",
       " u'as',\n",
       " u'temporarily',\n",
       " u'suspending',\n",
       " u'the',\n",
       " u'tradition',\n",
       " u'of',\n",
       " u'naming',\n",
       " u'each',\n",
       " u'super',\n",
       " u'bowl',\n",
       " u'game',\n",
       " u'with',\n",
       " u'roman',\n",
       " u'numerals',\n",
       " u'(',\n",
       " u'under',\n",
       " u'which',\n",
       " u'the',\n",
       " u'game',\n",
       " u'would',\n",
       " u'have',\n",
       " u'been',\n",
       " u'known',\n",
       " u'as',\n",
       " u'``',\n",
       " u'super',\n",
       " u'bowl',\n",
       " u'l',\n",
       " u\"''\",\n",
       " u')',\n",
       " u',',\n",
       " u'so',\n",
       " u'that',\n",
       " u'the',\n",
       " u'logo',\n",
       " u'could',\n",
       " u'prominently',\n",
       " u'feature',\n",
       " u'the',\n",
       " u'arabic',\n",
       " u'numerals',\n",
       " u'50',\n",
       " u'.']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_main[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pkls for negative examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in xrange(len(data_neg)):\n",
    "    data_neg[i][1] = map(lower, word_tokenize(data_neg[i][1]))\n",
    "    data_neg[i][2] = map(lower, word_tokenize(data_neg[i][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in xrange(len(data_neg)):\n",
    "    data_neg[i][2].append(not_a_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(data_neg)):\n",
    "    data_neg[i][0] = [[len(data_neg[i][2]) - 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wiki_neg_bin_feats = []\n",
    "\n",
    "for _, q, c in data_neg:\n",
    "    train_wiki_neg_bin_feats.append(make_bin_feats([q, c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_wiki_neg_ascii_chars = []\n",
    "\n",
    "chars = [unichr(i) for i in xrange(128)]\n",
    "i_to_c = chars\n",
    "c_to_i = {v:k for (k,v) in list(enumerate(chars))}\n",
    "\n",
    "for _, q, x in data_neg:\n",
    "    q_char = [[1] + [c_to_i.get(c, 0) for c in w] + [2] for w in q]\n",
    "    x_char = [[1] + [c_to_i.get(c, 0) for c in w] + [2] for w in x[:-1]] + [[1, 3, 2]]\n",
    "    train_wiki_neg_ascii_chars.append([q_char, x_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/wiki_negative_train/train_neg_bin_feats.pkl', 'w') as f:\n",
    "    pickle.dump(train_wiki_neg_bin_feats, f)\n",
    "    \n",
    "with open('/pio/data/data/squad/wiki_negative_train/train_neg_char_ascii.pkl', 'w') as f:\n",
    "    pickle.dump(train_wiki_neg_ascii_chars, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_neg_with_glove_vocab = []\n",
    "\n",
    "for a, q, x in data_neg:\n",
    "    q_num = [glove_w_to_i.get(w, 0) for w in q]\n",
    "    x_num = [glove_w_to_i.get(w, 0) for w in x]\n",
    "    train_neg_with_glove_vocab.append([a, q_num, x_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/wiki_negative_train/train_neg_with_glove_vocab.pkl', 'w') as f:\n",
    "    pickle.dump(train_neg_with_glove_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [len(w[2]) for w in train_neg_with_glove_vocab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "875"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for i in a if i > 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_wiki_neg_bin_feats = np.load('/pio/data/data/squad/wiki_negative_train/train_neg_bin_feats.pkl')\n",
    "train_wiki_neg_ascii_chars = np.load('/pio/data/data/squad/wiki_negative_train/train_neg_char_ascii.pkl')\n",
    "train_neg_with_glove_vocab = np.load('/pio/data/data/squad/wiki_negative_train/train_neg_with_glove_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(y) for x in train_wiki_neg_ascii_chars for y in x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "too_long = []\n",
    "\n",
    "for i, (_, x) in enumerate(train_wiki_neg_ascii_chars):\n",
    "    for w in x:\n",
    "        if len(w) > 35:\n",
    "            too_long.append(i)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wiki_neg_bin_feats_cut = []\n",
    "train_wiki_neg_ascii_chars_cut = []\n",
    "train_neg_with_glove_vocab_cut = []\n",
    "\n",
    "for i in xrange(len(train_neg_with_glove_vocab)):\n",
    "    if i not in too_long:\n",
    "        train_wiki_neg_bin_feats_cut.append(train_wiki_neg_bin_feats[i])\n",
    "        train_wiki_neg_ascii_chars_cut.append(train_wiki_neg_ascii_chars[i])\n",
    "        train_neg_with_glove_vocab_cut.append(train_neg_with_glove_vocab[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(y) for x in train_wiki_neg_ascii_chars_cut for y in x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84833"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_neg_with_glove_vocab_cut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/wiki_negative_train/train_neg_bin_feats.pkl', 'w') as f:\n",
    "    pickle.dump(train_wiki_neg_bin_feats_cut, f)\n",
    "    \n",
    "with open('/pio/data/data/squad/wiki_negative_train/train_neg_char_ascii.pkl', 'w') as f:\n",
    "    pickle.dump(train_wiki_neg_ascii_chars_cut, f)\n",
    "    \n",
    "with open('/pio/data/data/squad/wiki_negative_train/train_neg_with_glove_vocab.pkl', 'w') as f:\n",
    "    pickle.dump(train_neg_with_glove_vocab_cut, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab all the question-answer pairs and create a wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "data = []\n",
    "lower = lambda x: x.lower()\n",
    "\n",
    "for par in train['data']:\n",
    "    title = par['title']\n",
    "    \n",
    "    for con in par['paragraphs']:\n",
    "        context = con['context']\n",
    "        context_tok = map(lower, nltk.word_tokenize(context))\n",
    "        words |= set(context_tok)\n",
    "        \n",
    "        for q in con['qas']:\n",
    "            question = q['question']\n",
    "            question_tok = map(lower, nltk.word_tokenize(question))\n",
    "            words |= set(question_tok)\n",
    "            \n",
    "            Id = q['id']\n",
    "            \n",
    "            answers = []\n",
    "            \n",
    "            for ans in q['answers']:\n",
    "                text = ans['text']\n",
    "                text_tok = map(lower, nltk.word_tokenize(text))\n",
    "                ans_start = ans['answer_start']\n",
    "                \n",
    "                answers.append((ans_start, text_tok))\n",
    "                \n",
    "            data.append([answers, question_tok, context_tok])\n",
    "            \n",
    "words.add('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/train.pkl', 'w') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(515, [u'saint', u'bernadette', u'soubirous'])],\n",
       " [u'to',\n",
       "  u'whom',\n",
       "  u'did',\n",
       "  u'the',\n",
       "  u'virgin',\n",
       "  u'mary',\n",
       "  u'allegedly',\n",
       "  u'appear',\n",
       "  u'in',\n",
       "  u'1858',\n",
       "  u'in',\n",
       "  u'lourdes',\n",
       "  u'france',\n",
       "  u'?'],\n",
       " [u'architecturally',\n",
       "  u',',\n",
       "  u'the',\n",
       "  u'school',\n",
       "  u'has',\n",
       "  u'a',\n",
       "  u'catholic',\n",
       "  u'character',\n",
       "  u'.',\n",
       "  u'atop',\n",
       "  u'the',\n",
       "  u'main',\n",
       "  u'building',\n",
       "  u\"'s\",\n",
       "  u'gold',\n",
       "  u'dome',\n",
       "  u'is',\n",
       "  u'a',\n",
       "  u'golden',\n",
       "  u'statue',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'virgin',\n",
       "  u'mary',\n",
       "  u'.',\n",
       "  u'immediately',\n",
       "  u'in',\n",
       "  u'front',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'main',\n",
       "  u'building',\n",
       "  u'and',\n",
       "  u'facing',\n",
       "  u'it',\n",
       "  u',',\n",
       "  u'is',\n",
       "  u'a',\n",
       "  u'copper',\n",
       "  u'statue',\n",
       "  u'of',\n",
       "  u'christ',\n",
       "  u'with',\n",
       "  u'arms',\n",
       "  u'upraised',\n",
       "  u'with',\n",
       "  u'the',\n",
       "  u'legend',\n",
       "  u'``',\n",
       "  u'venite',\n",
       "  u'ad',\n",
       "  u'me',\n",
       "  u'omnes',\n",
       "  u\"''\",\n",
       "  u'.',\n",
       "  u'next',\n",
       "  u'to',\n",
       "  u'the',\n",
       "  u'main',\n",
       "  u'building',\n",
       "  u'is',\n",
       "  u'the',\n",
       "  u'basilica',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'sacred',\n",
       "  u'heart',\n",
       "  u'.',\n",
       "  u'immediately',\n",
       "  u'behind',\n",
       "  u'the',\n",
       "  u'basilica',\n",
       "  u'is',\n",
       "  u'the',\n",
       "  u'grotto',\n",
       "  u',',\n",
       "  u'a',\n",
       "  u'marian',\n",
       "  u'place',\n",
       "  u'of',\n",
       "  u'prayer',\n",
       "  u'and',\n",
       "  u'reflection',\n",
       "  u'.',\n",
       "  u'it',\n",
       "  u'is',\n",
       "  u'a',\n",
       "  u'replica',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'grotto',\n",
       "  u'at',\n",
       "  u'lourdes',\n",
       "  u',',\n",
       "  u'france',\n",
       "  u'where',\n",
       "  u'the',\n",
       "  u'virgin',\n",
       "  u'mary',\n",
       "  u'reputedly',\n",
       "  u'appeared',\n",
       "  u'to',\n",
       "  u'saint',\n",
       "  u'bernadette',\n",
       "  u'soubirous',\n",
       "  u'in',\n",
       "  u'1858',\n",
       "  u'.',\n",
       "  u'at',\n",
       "  u'the',\n",
       "  u'end',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'main',\n",
       "  u'drive',\n",
       "  u'(',\n",
       "  u'and',\n",
       "  u'in',\n",
       "  u'a',\n",
       "  u'direct',\n",
       "  u'line',\n",
       "  u'that',\n",
       "  u'connects',\n",
       "  u'through',\n",
       "  u'3',\n",
       "  u'statues',\n",
       "  u'and',\n",
       "  u'the',\n",
       "  u'gold',\n",
       "  u'dome',\n",
       "  u')',\n",
       "  u',',\n",
       "  u'is',\n",
       "  u'a',\n",
       "  u'simple',\n",
       "  u',',\n",
       "  u'modern',\n",
       "  u'stone',\n",
       "  u'statue',\n",
       "  u'of',\n",
       "  u'mary',\n",
       "  u'.']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_bin_feats(sample):\n",
    "    q, x = sample\n",
    "    qset = set(q)\n",
    "    return [w in qset for w in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [d[1:] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_feats = map(make_bin_feats, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/train_bin_feats.pkl', 'w') as f:\n",
    "    pickle.dump(bin_feats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev = [d[1:3] for d in data_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_bin_feats = map(make_bin_feats, data_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev_bin_feats.pkl', 'w') as f:\n",
    "    pickle.dump(dev_bin_feats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn words into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_to_w = dict(enumerate(words))\n",
    "w_to_i = {v:k for (k,v) in i_to_w.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_on_dot(s):\n",
    "    res = [[]]\n",
    "    for w in s:\n",
    "        res[-1].append(w)\n",
    "        if w == u'.':\n",
    "            res.append([])\n",
    "    return res if res[-1] else res[:-1]\n",
    "\n",
    "def words_to_num(s):\n",
    "    return map(lambda x: w_to_i.get(x, w_to_i['<unk>']), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(data)):\n",
    "    data[i][2] = split_on_dot(data[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(515, [u'saint', u'bernadette', u'soubirous'])],\n",
       " [u'to',\n",
       "  u'whom',\n",
       "  u'did',\n",
       "  u'the',\n",
       "  u'virgin',\n",
       "  u'mary',\n",
       "  u'allegedly',\n",
       "  u'appear',\n",
       "  u'in',\n",
       "  u'1858',\n",
       "  u'in',\n",
       "  u'lourdes',\n",
       "  u'france',\n",
       "  u'?'],\n",
       " [[u'architecturally',\n",
       "   u',',\n",
       "   u'the',\n",
       "   u'school',\n",
       "   u'has',\n",
       "   u'a',\n",
       "   u'catholic',\n",
       "   u'character',\n",
       "   u'.'],\n",
       "  [u'atop',\n",
       "   u'the',\n",
       "   u'main',\n",
       "   u'building',\n",
       "   u\"'s\",\n",
       "   u'gold',\n",
       "   u'dome',\n",
       "   u'is',\n",
       "   u'a',\n",
       "   u'golden',\n",
       "   u'statue',\n",
       "   u'of',\n",
       "   u'the',\n",
       "   u'virgin',\n",
       "   u'mary',\n",
       "   u'.'],\n",
       "  [u'immediately',\n",
       "   u'in',\n",
       "   u'front',\n",
       "   u'of',\n",
       "   u'the',\n",
       "   u'main',\n",
       "   u'building',\n",
       "   u'and',\n",
       "   u'facing',\n",
       "   u'it',\n",
       "   u',',\n",
       "   u'is',\n",
       "   u'a',\n",
       "   u'copper',\n",
       "   u'statue',\n",
       "   u'of',\n",
       "   u'christ',\n",
       "   u'with',\n",
       "   u'arms',\n",
       "   u'upraised',\n",
       "   u'with',\n",
       "   u'the',\n",
       "   u'legend',\n",
       "   u'``',\n",
       "   u'venite',\n",
       "   u'ad',\n",
       "   u'me',\n",
       "   u'omnes',\n",
       "   u\"''\",\n",
       "   u'.'],\n",
       "  [u'next',\n",
       "   u'to',\n",
       "   u'the',\n",
       "   u'main',\n",
       "   u'building',\n",
       "   u'is',\n",
       "   u'the',\n",
       "   u'basilica',\n",
       "   u'of',\n",
       "   u'the',\n",
       "   u'sacred',\n",
       "   u'heart',\n",
       "   u'.'],\n",
       "  [u'immediately',\n",
       "   u'behind',\n",
       "   u'the',\n",
       "   u'basilica',\n",
       "   u'is',\n",
       "   u'the',\n",
       "   u'grotto',\n",
       "   u',',\n",
       "   u'a',\n",
       "   u'marian',\n",
       "   u'place',\n",
       "   u'of',\n",
       "   u'prayer',\n",
       "   u'and',\n",
       "   u'reflection',\n",
       "   u'.'],\n",
       "  [u'it',\n",
       "   u'is',\n",
       "   u'a',\n",
       "   u'replica',\n",
       "   u'of',\n",
       "   u'the',\n",
       "   u'grotto',\n",
       "   u'at',\n",
       "   u'lourdes',\n",
       "   u',',\n",
       "   u'france',\n",
       "   u'where',\n",
       "   u'the',\n",
       "   u'virgin',\n",
       "   u'mary',\n",
       "   u'reputedly',\n",
       "   u'appeared',\n",
       "   u'to',\n",
       "   u'saint',\n",
       "   u'bernadette',\n",
       "   u'soubirous',\n",
       "   u'in',\n",
       "   u'1858',\n",
       "   u'.'],\n",
       "  [u'at',\n",
       "   u'the',\n",
       "   u'end',\n",
       "   u'of',\n",
       "   u'the',\n",
       "   u'main',\n",
       "   u'drive',\n",
       "   u'(',\n",
       "   u'and',\n",
       "   u'in',\n",
       "   u'a',\n",
       "   u'direct',\n",
       "   u'line',\n",
       "   u'that',\n",
       "   u'connects',\n",
       "   u'through',\n",
       "   u'3',\n",
       "   u'statues',\n",
       "   u'and',\n",
       "   u'the',\n",
       "   u'gold',\n",
       "   u'dome',\n",
       "   u')',\n",
       "   u',',\n",
       "   u'is',\n",
       "   u'a',\n",
       "   u'simple',\n",
       "   u',',\n",
       "   u'modern',\n",
       "   u'stone',\n",
       "   u'statue',\n",
       "   u'of',\n",
       "   u'mary',\n",
       "   u'.']]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = []\n",
    "\n",
    "for a, q, c in data:\n",
    "    answers = []\n",
    "    for ans in a:\n",
    "        answers.append((ans[0], words_to_num(ans[1])))        \n",
    "    data_num.append([answers, words_to_num(q), map(words_to_num, c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(515, [1871, 11246, 70514])],\n",
       " [78116,\n",
       "  49985,\n",
       "  45432,\n",
       "  67467,\n",
       "  86834,\n",
       "  71625,\n",
       "  58400,\n",
       "  29461,\n",
       "  93998,\n",
       "  55583,\n",
       "  93998,\n",
       "  72050,\n",
       "  96784,\n",
       "  82775],\n",
       " [[100416, 44790, 67467, 60466, 604, 43144, 88878, 32471, 488],\n",
       "  [40539,\n",
       "   67467,\n",
       "   45649,\n",
       "   54124,\n",
       "   55580,\n",
       "   78499,\n",
       "   78216,\n",
       "   19464,\n",
       "   43144,\n",
       "   51144,\n",
       "   10775,\n",
       "   23654,\n",
       "   67467,\n",
       "   86834,\n",
       "   71625,\n",
       "   488],\n",
       "  [82731,\n",
       "   93998,\n",
       "   95282,\n",
       "   23654,\n",
       "   67467,\n",
       "   45649,\n",
       "   54124,\n",
       "   49507,\n",
       "   33330,\n",
       "   19467,\n",
       "   44790,\n",
       "   19464,\n",
       "   43144,\n",
       "   84794,\n",
       "   10775,\n",
       "   23654,\n",
       "   99211,\n",
       "   1477,\n",
       "   95970,\n",
       "   37322,\n",
       "   1477,\n",
       "   67467,\n",
       "   78193,\n",
       "   64615,\n",
       "   100635,\n",
       "   14055,\n",
       "   32694,\n",
       "   66309,\n",
       "   77274,\n",
       "   488],\n",
       "  [19356,\n",
       "   78116,\n",
       "   67467,\n",
       "   45649,\n",
       "   54124,\n",
       "   19464,\n",
       "   67467,\n",
       "   32617,\n",
       "   23654,\n",
       "   67467,\n",
       "   7958,\n",
       "   80613,\n",
       "   488],\n",
       "  [82731,\n",
       "   71360,\n",
       "   67467,\n",
       "   32617,\n",
       "   19464,\n",
       "   67467,\n",
       "   16841,\n",
       "   44790,\n",
       "   43144,\n",
       "   50115,\n",
       "   87195,\n",
       "   23654,\n",
       "   54900,\n",
       "   49507,\n",
       "   18731,\n",
       "   488],\n",
       "  [19467,\n",
       "   19464,\n",
       "   43144,\n",
       "   41368,\n",
       "   23654,\n",
       "   67467,\n",
       "   16841,\n",
       "   14071,\n",
       "   72050,\n",
       "   44790,\n",
       "   96784,\n",
       "   39501,\n",
       "   67467,\n",
       "   86834,\n",
       "   71625,\n",
       "   52528,\n",
       "   15497,\n",
       "   78116,\n",
       "   1871,\n",
       "   11246,\n",
       "   70514,\n",
       "   93998,\n",
       "   55583,\n",
       "   488],\n",
       "  [14071,\n",
       "   67467,\n",
       "   22337,\n",
       "   23654,\n",
       "   67467,\n",
       "   45649,\n",
       "   58033,\n",
       "   86998,\n",
       "   49507,\n",
       "   93998,\n",
       "   43144,\n",
       "   9435,\n",
       "   66033,\n",
       "   94208,\n",
       "   44589,\n",
       "   59063,\n",
       "   42652,\n",
       "   9052,\n",
       "   49507,\n",
       "   67467,\n",
       "   78499,\n",
       "   78216,\n",
       "   60198,\n",
       "   44790,\n",
       "   19464,\n",
       "   43144,\n",
       "   85018,\n",
       "   44790,\n",
       "   83286,\n",
       "   90833,\n",
       "   10775,\n",
       "   23654,\n",
       "   71625,\n",
       "   488]]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_num[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num = [[l[0], [l[1]] + l[2]] for l in data_num]\n",
    "data_num = [[[t[1] for t in l[0]], l[1]] for l in data_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1871, 11246, 70514]],\n",
       " [[78116,\n",
       "   49985,\n",
       "   45432,\n",
       "   67467,\n",
       "   86834,\n",
       "   71625,\n",
       "   58400,\n",
       "   29461,\n",
       "   93998,\n",
       "   55583,\n",
       "   93998,\n",
       "   72050,\n",
       "   96784,\n",
       "   82775],\n",
       "  [100416, 44790, 67467, 60466, 604, 43144, 88878, 32471, 488],\n",
       "  [40539,\n",
       "   67467,\n",
       "   45649,\n",
       "   54124,\n",
       "   55580,\n",
       "   78499,\n",
       "   78216,\n",
       "   19464,\n",
       "   43144,\n",
       "   51144,\n",
       "   10775,\n",
       "   23654,\n",
       "   67467,\n",
       "   86834,\n",
       "   71625,\n",
       "   488],\n",
       "  [82731,\n",
       "   93998,\n",
       "   95282,\n",
       "   23654,\n",
       "   67467,\n",
       "   45649,\n",
       "   54124,\n",
       "   49507,\n",
       "   33330,\n",
       "   19467,\n",
       "   44790,\n",
       "   19464,\n",
       "   43144,\n",
       "   84794,\n",
       "   10775,\n",
       "   23654,\n",
       "   99211,\n",
       "   1477,\n",
       "   95970,\n",
       "   37322,\n",
       "   1477,\n",
       "   67467,\n",
       "   78193,\n",
       "   64615,\n",
       "   100635,\n",
       "   14055,\n",
       "   32694,\n",
       "   66309,\n",
       "   77274,\n",
       "   488],\n",
       "  [19356,\n",
       "   78116,\n",
       "   67467,\n",
       "   45649,\n",
       "   54124,\n",
       "   19464,\n",
       "   67467,\n",
       "   32617,\n",
       "   23654,\n",
       "   67467,\n",
       "   7958,\n",
       "   80613,\n",
       "   488],\n",
       "  [82731,\n",
       "   71360,\n",
       "   67467,\n",
       "   32617,\n",
       "   19464,\n",
       "   67467,\n",
       "   16841,\n",
       "   44790,\n",
       "   43144,\n",
       "   50115,\n",
       "   87195,\n",
       "   23654,\n",
       "   54900,\n",
       "   49507,\n",
       "   18731,\n",
       "   488],\n",
       "  [19467,\n",
       "   19464,\n",
       "   43144,\n",
       "   41368,\n",
       "   23654,\n",
       "   67467,\n",
       "   16841,\n",
       "   14071,\n",
       "   72050,\n",
       "   44790,\n",
       "   96784,\n",
       "   39501,\n",
       "   67467,\n",
       "   86834,\n",
       "   71625,\n",
       "   52528,\n",
       "   15497,\n",
       "   78116,\n",
       "   1871,\n",
       "   11246,\n",
       "   70514,\n",
       "   93998,\n",
       "   55583,\n",
       "   488],\n",
       "  [14071,\n",
       "   67467,\n",
       "   22337,\n",
       "   23654,\n",
       "   67467,\n",
       "   45649,\n",
       "   58033,\n",
       "   86998,\n",
       "   49507,\n",
       "   93998,\n",
       "   43144,\n",
       "   9435,\n",
       "   66033,\n",
       "   94208,\n",
       "   44589,\n",
       "   59063,\n",
       "   42652,\n",
       "   9052,\n",
       "   49507,\n",
       "   67467,\n",
       "   78499,\n",
       "   78216,\n",
       "   60198,\n",
       "   44790,\n",
       "   19464,\n",
       "   43144,\n",
       "   85018,\n",
       "   44790,\n",
       "   83286,\n",
       "   90833,\n",
       "   10775,\n",
       "   23654,\n",
       "   71625,\n",
       "   488]]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_num[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1028"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are some broken answers, because of the tokenizer (I count words instead of characters)\n",
    "\n",
    "k = 0\n",
    "for a, q in data_num:\n",
    "    for w in a[0]:\n",
    "        if w not in list(chain(*q[1:])):\n",
    "            k += 1\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1871, 11246, 70514]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a,q = data_num[0]\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find answer indices on words, not characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inds = []\n",
    "\n",
    "for a, q in data_num:\n",
    "    ans = []\n",
    "    tot_q = list(chain(*q[1:]))\n",
    "    for x in a:\n",
    "        for i in xrange(len(tot_q)):\n",
    "            if x == tot_q[i:i+len(x)]:\n",
    "                ans.append(list(xrange(i, i + len(x))))\n",
    "                break\n",
    "    inds.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in xrange(len(data_num)):\n",
    "    data_num[i][0] = inds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(515, [u'saint', u'bernadette', u'soubirous'])],\n",
       " [u'to',\n",
       "  u'whom',\n",
       "  u'did',\n",
       "  u'the',\n",
       "  u'virgin',\n",
       "  u'mary',\n",
       "  u'allegedly',\n",
       "  u'appear',\n",
       "  u'in',\n",
       "  u'1858',\n",
       "  u'in',\n",
       "  u'lourdes',\n",
       "  u'france',\n",
       "  u'?'],\n",
       " [u'architecturally',\n",
       "  u',',\n",
       "  u'the',\n",
       "  u'school',\n",
       "  u'has',\n",
       "  u'a',\n",
       "  u'catholic',\n",
       "  u'character',\n",
       "  u'.',\n",
       "  u'atop',\n",
       "  u'the',\n",
       "  u'main',\n",
       "  u'building',\n",
       "  u\"'s\",\n",
       "  u'gold',\n",
       "  u'dome',\n",
       "  u'is',\n",
       "  u'a',\n",
       "  u'golden',\n",
       "  u'statue',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'virgin',\n",
       "  u'mary',\n",
       "  u'.',\n",
       "  u'immediately',\n",
       "  u'in',\n",
       "  u'front',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'main',\n",
       "  u'building',\n",
       "  u'and',\n",
       "  u'facing',\n",
       "  u'it',\n",
       "  u',',\n",
       "  u'is',\n",
       "  u'a',\n",
       "  u'copper',\n",
       "  u'statue',\n",
       "  u'of',\n",
       "  u'christ',\n",
       "  u'with',\n",
       "  u'arms',\n",
       "  u'upraised',\n",
       "  u'with',\n",
       "  u'the',\n",
       "  u'legend',\n",
       "  u'``',\n",
       "  u'venite',\n",
       "  u'ad',\n",
       "  u'me',\n",
       "  u'omnes',\n",
       "  u\"''\",\n",
       "  u'.',\n",
       "  u'next',\n",
       "  u'to',\n",
       "  u'the',\n",
       "  u'main',\n",
       "  u'building',\n",
       "  u'is',\n",
       "  u'the',\n",
       "  u'basilica',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'sacred',\n",
       "  u'heart',\n",
       "  u'.',\n",
       "  u'immediately',\n",
       "  u'behind',\n",
       "  u'the',\n",
       "  u'basilica',\n",
       "  u'is',\n",
       "  u'the',\n",
       "  u'grotto',\n",
       "  u',',\n",
       "  u'a',\n",
       "  u'marian',\n",
       "  u'place',\n",
       "  u'of',\n",
       "  u'prayer',\n",
       "  u'and',\n",
       "  u'reflection',\n",
       "  u'.',\n",
       "  u'it',\n",
       "  u'is',\n",
       "  u'a',\n",
       "  u'replica',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'grotto',\n",
       "  u'at',\n",
       "  u'lourdes',\n",
       "  u',',\n",
       "  u'france',\n",
       "  u'where',\n",
       "  u'the',\n",
       "  u'virgin',\n",
       "  u'mary',\n",
       "  u'reputedly',\n",
       "  u'appeared',\n",
       "  u'to',\n",
       "  u'saint',\n",
       "  u'bernadette',\n",
       "  u'soubirous',\n",
       "  u'in',\n",
       "  u'1858',\n",
       "  u'.',\n",
       "  u'at',\n",
       "  u'the',\n",
       "  u'end',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'main',\n",
       "  u'drive',\n",
       "  u'(',\n",
       "  u'and',\n",
       "  u'in',\n",
       "  u'a',\n",
       "  u'direct',\n",
       "  u'line',\n",
       "  u'that',\n",
       "  u'connects',\n",
       "  u'through',\n",
       "  u'3',\n",
       "  u'statues',\n",
       "  u'and',\n",
       "  u'the',\n",
       "  u'gold',\n",
       "  u'dome',\n",
       "  u')',\n",
       "  u',',\n",
       "  u'is',\n",
       "  u'a',\n",
       "  u'simple',\n",
       "  u',',\n",
       "  u'modern',\n",
       "  u'stone',\n",
       "  u'statue',\n",
       "  u'of',\n",
       "  u'mary',\n",
       "  u'.']]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.load('evaluate/glove_vocab/pred_glove_premadeBin_charemb_all_fixed_ep3.npz')['arr_0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open('evaluate/glove_vocab/pred_glove_premadeBin_charemb_all_fixed_ep3.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(u'{')\n",
    "    for i in xrange(len(data_dev)):\n",
    "        ans = ' '.join(data_dev[i][2][preds[i][0]:preds[i][1] + 1])\n",
    "        Id = data_dev[i][3]\n",
    "        f.write(u'\"{}\": \"{}\"'.format(Id, ans))\n",
    "        if i < len(data_dev) - 1:\n",
    "            f.write(u', ')\n",
    "    f.write(u'}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_dev = set()\n",
    "data_dev = []\n",
    "lower = lambda x: x.lower()\n",
    "\n",
    "for par in dev['data']:\n",
    "    title = par['title']\n",
    "    \n",
    "    for con in par['paragraphs']:\n",
    "        context = con['context']\n",
    "        context_tok = map(lower, nltk.word_tokenize(context))\n",
    "        words_dev |= set(context_tok)\n",
    "        \n",
    "        for q in con['qas']:\n",
    "            question = q['question']\n",
    "            question_tok = map(lower, nltk.word_tokenize(question))\n",
    "            words_dev |= set(question_tok)\n",
    "            \n",
    "            Id = q['id']\n",
    "            \n",
    "            answers = []\n",
    "            \n",
    "            for ans in q['answers']:\n",
    "                text = ans['text']\n",
    "                text_tok = map(lower, nltk.word_tokenize(text))\n",
    "                ans_start = ans['answer_start']\n",
    "                \n",
    "                answers.append((ans_start, text_tok))\n",
    "                \n",
    "            data_dev.append([answers, question_tok, context_tok, Id])\n",
    "            \n",
    "words_dev.add('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(81, [u'90']), (81, [u'90', u'%']), (81, [u'90', u'%'])],\n",
       " [u'what',\n",
       "  u'percentage',\n",
       "  u'of',\n",
       "  u'electricity',\n",
       "  u'was',\n",
       "  u'made',\n",
       "  u'by',\n",
       "  u'steam',\n",
       "  u'turbine',\n",
       "  u'in',\n",
       "  u'the',\n",
       "  u'1990s',\n",
       "  u'?'],\n",
       " [u'the',\n",
       "  u'main',\n",
       "  u'use',\n",
       "  u'for',\n",
       "  u'steam',\n",
       "  u'turbines',\n",
       "  u'is',\n",
       "  u'in',\n",
       "  u'electricity',\n",
       "  u'generation',\n",
       "  u'(',\n",
       "  u'in',\n",
       "  u'the',\n",
       "  u'1990s',\n",
       "  u'about',\n",
       "  u'90',\n",
       "  u'%',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'world',\n",
       "  u\"'s\",\n",
       "  u'electric',\n",
       "  u'production',\n",
       "  u'was',\n",
       "  u'by',\n",
       "  u'use',\n",
       "  u'of',\n",
       "  u'steam',\n",
       "  u'turbines',\n",
       "  u')',\n",
       "  u'however',\n",
       "  u'the',\n",
       "  u'recent',\n",
       "  u'widespread',\n",
       "  u'application',\n",
       "  u'of',\n",
       "  u'large',\n",
       "  u'gas',\n",
       "  u'turbine',\n",
       "  u'units',\n",
       "  u'and',\n",
       "  u'typical',\n",
       "  u'combined',\n",
       "  u'cycle',\n",
       "  u'power',\n",
       "  u'plants',\n",
       "  u'has',\n",
       "  u'resulted',\n",
       "  u'in',\n",
       "  u'reduction',\n",
       "  u'of',\n",
       "  u'this',\n",
       "  u'percentage',\n",
       "  u'to',\n",
       "  u'the',\n",
       "  u'80',\n",
       "  u'%',\n",
       "  u'regime',\n",
       "  u'for',\n",
       "  u'steam',\n",
       "  u'turbines',\n",
       "  u'.',\n",
       "  u'in',\n",
       "  u'electricity',\n",
       "  u'production',\n",
       "  u',',\n",
       "  u'the',\n",
       "  u'high',\n",
       "  u'speed',\n",
       "  u'of',\n",
       "  u'turbine',\n",
       "  u'rotation',\n",
       "  u'matches',\n",
       "  u'well',\n",
       "  u'with',\n",
       "  u'the',\n",
       "  u'speed',\n",
       "  u'of',\n",
       "  u'modern',\n",
       "  u'electric',\n",
       "  u'generators',\n",
       "  u',',\n",
       "  u'which',\n",
       "  u'are',\n",
       "  u'typically',\n",
       "  u'direct',\n",
       "  u'connected',\n",
       "  u'to',\n",
       "  u'their',\n",
       "  u'driving',\n",
       "  u'turbines',\n",
       "  u'.',\n",
       "  u'in',\n",
       "  u'marine',\n",
       "  u'service',\n",
       "  u',',\n",
       "  u'(',\n",
       "  u'pioneered',\n",
       "  u'on',\n",
       "  u'the',\n",
       "  u'turbinia',\n",
       "  u')',\n",
       "  u',',\n",
       "  u'steam',\n",
       "  u'turbines',\n",
       "  u'with',\n",
       "  u'reduction',\n",
       "  u'gearing',\n",
       "  u'(',\n",
       "  u'although',\n",
       "  u'the',\n",
       "  u'turbinia',\n",
       "  u'has',\n",
       "  u'direct',\n",
       "  u'turbines',\n",
       "  u'to',\n",
       "  u'propellers',\n",
       "  u'with',\n",
       "  u'no',\n",
       "  u'reduction',\n",
       "  u'gearbox',\n",
       "  u')',\n",
       "  u'dominated',\n",
       "  u'large',\n",
       "  u'ship',\n",
       "  u'propulsion',\n",
       "  u'throughout',\n",
       "  u'the',\n",
       "  u'late',\n",
       "  u'20th',\n",
       "  u'century',\n",
       "  u',',\n",
       "  u'being',\n",
       "  u'more',\n",
       "  u'efficient',\n",
       "  u'(',\n",
       "  u'and',\n",
       "  u'requiring',\n",
       "  u'far',\n",
       "  u'less',\n",
       "  u'maintenance',\n",
       "  u')',\n",
       "  u'than',\n",
       "  u'reciprocating',\n",
       "  u'steam',\n",
       "  u'engines',\n",
       "  u'.',\n",
       "  u'in',\n",
       "  u'recent',\n",
       "  u'decades',\n",
       "  u',',\n",
       "  u'reciprocating',\n",
       "  u'diesel',\n",
       "  u'engines',\n",
       "  u',',\n",
       "  u'and',\n",
       "  u'gas',\n",
       "  u'turbines',\n",
       "  u',',\n",
       "  u'have',\n",
       "  u'almost',\n",
       "  u'entirely',\n",
       "  u'supplanted',\n",
       "  u'steam',\n",
       "  u'propulsion',\n",
       "  u'for',\n",
       "  u'marine',\n",
       "  u'applications',\n",
       "  u'.'],\n",
       " u'571154c72419e31400955587']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dev[3353]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev.pkl', 'w') as f:\n",
    "    pickle.dump(data_dev, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dev = np.load('/pio/data/data/squad/dev.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in xrange(len(data_dev)):\n",
    "    data_dev[i][2] = [w if w in words else '<unk>' for w in data_dev[i][2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in xrange(len(data_dev)):\n",
    "    data_dev[i][2] = split_on_dot(data_dev[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words_to_num(s):\n",
    "    return map(lambda x: glove_w_to_i.get(x, 0), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_num_dev = []\n",
    "\n",
    "for a, q, c, _ in data_dev:\n",
    "    answers = []\n",
    "    for ans in a:\n",
    "        answers.append((ans[0], words_to_num(ans[1])))        \n",
    "    data_num_dev.append([answers, words_to_num(q), map(words_to_num, c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_num_dev = [[l[0], [l[1]] + l[2]] for l in data_num_dev]\n",
    "data_num_dev = [[[t[1] for t in l[0]], l[1]] for l in data_num_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds = []\n",
    "\n",
    "for a, q in data_num_dev:\n",
    "    ans = []\n",
    "    tot_q = list(chain(*q[1:]))\n",
    "    for x in a:\n",
    "        for i in xrange(len(tot_q)):\n",
    "            if x == tot_q[i:i+len(x)]:\n",
    "                ans.append(list(xrange(i, i + len(x))))\n",
    "                break\n",
    "    inds.append(ans)\n",
    "    \n",
    "for i in xrange(len(data_num_dev)):\n",
    "    data_num_dev[i][0] = inds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_num_dev = [[d[0]] + map(words_to_num, d[1:]) for d in data_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev_with_glove_vocab.pkl', 'w') as f:\n",
    "    pickle.dump(data_num_dev, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get Glove vectors for words in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vec = np.load('/pio/data/data/glove_vec/6B/glove/glove.6B.300d.npy')\n",
    "\n",
    "glove_words = []\n",
    "\n",
    "with io.open('/pio/data/data/glove_vec/6B/glove/glove.6B.wordlist.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        glove_words.append(line.split()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_i_to_w = glove_words\n",
    "glove_w_to_i = {v:k for (k,v) in list(enumerate(glove_words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(102802, 300)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs = np.zeros((len(w_to_i), 300), dtype=np.float32)\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73351"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_inds = [i for i in xrange(len(w_to_i)) if i_to_w[i] in glove_w_to_i]\n",
    "len(known_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_known = set(known_inds)\n",
    "unknown_inds = [i for i in xrange(len(w_to_i)) if i not in s_known]\n",
    "s_unknown = set(unknown_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_num = np.load('/pio/data/data/squad/dev_with_training_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_num = np.load('/pio/data/data/squad/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_dev = words - words_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4445"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_to_i['<unk>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for di in xrange(len(train_num)):\n",
    "    for si in xrange(len(train_num[di][1])):\n",
    "        for wi in xrange(len(train_num[di][1][si])):\n",
    "            w = train_num[di][1][si][wi]\n",
    "            if i_to_w[w] in not_in_dev:\n",
    "                train_num[di][1][si][wi] = 4445"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/train_with_unks.pkl', 'w') as f:\n",
    "    pickle.dump(train_num, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embs[known_inds] = glove_vec[[glove_w_to_i[i_to_w[i]] for i in known_inds]]\n",
    "embs[unknown_inds] = L.init.Normal()((len(unknown_inds), 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1569040"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([w for d in data_dev for w in list(chain(*d[1:3])) if w in w_to_i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03201188947819429"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of <unk> in dev\n",
    "51889. / 1620929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9679881105218057"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of dev set in train vocabulary\n",
    "1569040. / 1620929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9874362171322741"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of dev set in glove\n",
    "1600564. / 1620929"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.021761058288548987"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# <unk> in dev questions\n",
    "2632. / 120950"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08194192478236054"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no-devs in train\n",
    "1070257. / 13061165"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01261426526653633"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no-gloves in train\n",
    "164757. / 13061165"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_words = map(lambda x: x[0], sorted(w_to_i.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with io.open('/pio/data/data/squad/wordlist.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in sorted_words:\n",
    "        f.write(unicode(w + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file has a lot of redundant parts, context is repeated for each question.\n",
    "# It only slows down the initial loading.\n",
    "\n",
    "with open('/pio/data/data/squad/train.pkl', 'w') as f:\n",
    "    pickle.dump(data_num, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = np.load('/pio/data/data/squad/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_to_i = {}\n",
    "idx = 0\n",
    "\n",
    "with io.open('/pio/data/data/squad/train_wordlist.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        w_to_i[line[:-1]] = idx\n",
    "        idx += 1\n",
    "        \n",
    "i_to_w = {v:k for (k,v) in w_to_i.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lens = np.array(map(lambda x: len(x[1]), data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(idx):\n",
    "    for s in data[idx][1]:\n",
    "        print ' '.join([i_to_w[x] for x in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is the largest hottest continuously large area worldwide ?\n",
      "the sky is usually clear above the desert and the sunshine duration is extremely high everywhere in the sahara .\n",
      "most of the desert enjoys more than 3,600 h of bright sunshine annually or over 82 % of the time and a wide area in the eastern part experiences in excess of 4,000 h of bright sunshine a year or over 91 % of the time , and the highest values are very close to the theoretical maximum value .\n",
      "a value of 4,300 h or 98 % of the time would be recorded in upper egypt ( aswan , luxor ) and in the nubian desert ( wadi halfa ) .\n",
      "the annual average direct solar irradiation is around 2,800 kwh/ ( m2 year ) in the great desert .\n",
      "the sahara has a huge potential for solar energy production .\n",
      "the constantly high position of the sun , the extremely low relative humidity , the lack of vegetation and rainfall make the great desert the hottest continuously large area worldwide and certainly the hottest place on earth during summertime in some spots .\n",
      "the average high temperature exceeds 38 c ( 100.4 f ) - 40 c ( 104 f ) during the hottest month nearly everywhere in the desert except at very high mountainous areas .\n",
      "the highest officially recorded average high temperature was 47 c ( 116.6 f ) in a remote desert town in the algerian desert called bou bernous with an elevation of 378 meters above sea level .\n",
      "it 's the world 's highest recorded average high temperature and only death valley , california rivals it .\n",
      "other hot spots in algeria such as adrar , timimoun , in salah , ouallene , aoulef , reggane with an elevation between 200 and 400 meters above sea level get slightly lower summer average highs around 46 c ( 114.8 f ) during the hottest months of the year .\n",
      "salah , well known in algeria for its extreme heat , has an average high temperature of 43.8 c ( 110.8 f ) , 46.4 c ( 115.5 f ) , 45.5 ( 113.9 f ) .\n",
      "furthermore , 41.9 c ( 107.4 f ) in june , july , august and september .\n",
      "in fact , there are even hotter spots in the sahara , but they are located in extremely remote areas , especially in the azalai , lying in northern mali .\n",
      "the major part of the desert experiences around 3  5 months when the average high strictly exceeds 40 c ( 104 f ) .\n",
      "the southern central part of the desert experiences up to 6  7 months when the average high temperature strictly exceeds 40 c ( 104 f ) which shows the constancy and the length of the really hot season in the sahara .\n",
      "some examples of this are bilma , niger and faya-largeau , chad .\n",
      "the annual average daily temperature exceeds 20 c ( 68 f ) everywhere and can approach 30 c ( 86 f ) in the hottest regions year-round .\n",
      "however , most of the desert has a value in excess of 25 c ( 77 f ) .\n",
      "the sand and ground temperatures are even more extreme .\n",
      "during daytime , the sand temperature is extremely high as it can easily reach 80 c ( 176 f ) or more .\n",
      "a sand temperature of 83.5 c ( 182.3 f ) has been recorded in port sudan .\n",
      "ground temperatures of 72 c ( 161.6 f ) have been recorded in the adrar of mauritania and a value of 75 c ( 167 f ) has been measured in borkou , northern chad .\n",
      "due to lack of cloud cover and very low humidity , the desert usually features high diurnal temperature variations between days and nights .\n",
      "however , it 's a myth that the nights are cold after extremely hot days in the sahara .\n",
      "the average diurnal temperature range is typically between 13 c ( 55.4 f ) and 20 c ( 68 f ) .\n",
      "the lowest values are found along the coastal regions due to high humidity and are often even lower than 10 c ( 50 f ) , while the highest values are found in inland desert areas where the humidity is the lowest , mainly in the southern sahara .\n",
      "still , it 's true that winter nights can be cold as it can drop to the freezing point and even below , especially in high-elevation areas .\n"
     ]
    }
   ],
   "source": [
    "show_data(60023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([    0,     0,  1665,  6015, 12958, 18663, 16891, 12176,  7727,\n",
       "        4709,  2711,  1677,   859,   636,   352,   223,   166,    60,\n",
       "          39,    24,     9,     0,     5,    19,     5,     0,     0,\n",
       "           0,    10])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print max(lens)\n",
    "np.bincount(lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun with characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# i_to_c = list(glove_chars)\n",
    "# c_to_i = {v:k for (k,v) in i_to_c.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data powinno by bezporednio po wykonaniu okienka, w ktrym jest inicjowane words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chars = {c for d in data for s in d[1:] for w in s for c in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chars_dev = {c for d in data_dev for s in d[1:3] for w in s for c in w}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chars.add('<unk>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - unk\n",
    "# 1 - start\n",
    "# 2 - end\n",
    "# 3 - not_a_word char (added later, in wikipedia negative samplesF)\n",
    "# there are no 1s or 2s in data, so these are safe\n",
    "\n",
    "chars = [unichr(i) for i in xrange(128)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i_to_c = chars\n",
    "c_to_i = {v:k for (k,v) in list(enumerate(chars))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_char = []\n",
    "\n",
    "for _, q, x in data:\n",
    "    q_char = [[1] + [c_to_i.get(c, 0) for c in w] + [2] for w in q]\n",
    "    x_char = [[1] + [c_to_i.get(c, 0) for c in w] + [2] for w in x]\n",
    "    data_char.append([q_char, x_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dev_char = []\n",
    "\n",
    "for _, q, x, _ in data_dev:\n",
    "    q_char = [[1] + [c_to_i.get(c, 0) for c in w] + [2] for w in q]\n",
    "    x_char = [[1] + [c_to_i.get(c, 0) for c in w] + [2] for w in x]\n",
    "    data_dev_char.append([q_char, x_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[u'which',\n",
       "  u'nfl',\n",
       "  u'team',\n",
       "  u'represented',\n",
       "  u'the',\n",
       "  u'afc',\n",
       "  u'at',\n",
       "  u'super',\n",
       "  u'bowl',\n",
       "  u'50',\n",
       "  u'?'],\n",
       " [u'super',\n",
       "  u'bowl',\n",
       "  u'50',\n",
       "  u'was',\n",
       "  u'an',\n",
       "  u'american',\n",
       "  u'football',\n",
       "  u'game',\n",
       "  u'to',\n",
       "  u'determine',\n",
       "  u'the',\n",
       "  u'champion',\n",
       "  u'of',\n",
       "  u'the',\n",
       "  u'national',\n",
       "  u'football',\n",
       "  u'league',\n",
       "  u'(',\n",
       "  u'nfl',\n",
       "  u')',\n",
       "  u'for',\n",
       "  u'the',\n",
       "  u'2015',\n",
       "  u'season',\n",
       "  u'.',\n",
       "  u'the',\n",
       "  u'american',\n",
       "  u'football',\n",
       "  u'conference',\n",
       "  u'(',\n",
       "  u'afc',\n",
       "  u')',\n",
       "  u'champion',\n",
       "  u'denver',\n",
       "  u'broncos',\n",
       "  u'defeated',\n",
       "  u'the',\n",
       "  u'national',\n",
       "  u'football',\n",
       "  u'conference',\n",
       "  u'(',\n",
       "  u'nfc',\n",
       "  u')',\n",
       "  u'champion',\n",
       "  u'carolina',\n",
       "  u'panthers',\n",
       "  u'24\\u201310',\n",
       "  u'to',\n",
       "  u'earn',\n",
       "  u'their',\n",
       "  u'third',\n",
       "  u'super',\n",
       "  u'bowl',\n",
       "  u'title',\n",
       "  u'.',\n",
       "  u'the',\n",
       "  u'game',\n",
       "  u'was',\n",
       "  u'played',\n",
       "  u'on',\n",
       "  u'february',\n",
       "  u'7',\n",
       "  u',',\n",
       "  u'2016',\n",
       "  u',',\n",
       "  u'at',\n",
       "  u'levi',\n",
       "  u\"'s\",\n",
       "  u'stadium',\n",
       "  u'in',\n",
       "  u'the',\n",
       "  u'san',\n",
       "  u'francisco',\n",
       "  u'bay',\n",
       "  u'area',\n",
       "  u'at',\n",
       "  u'santa',\n",
       "  u'clara',\n",
       "  u',',\n",
       "  u'california',\n",
       "  u'.',\n",
       "  u'as',\n",
       "  u'this',\n",
       "  u'was',\n",
       "  u'the',\n",
       "  u'50th',\n",
       "  u'super',\n",
       "  u'bowl',\n",
       "  u',',\n",
       "  u'the',\n",
       "  u'league',\n",
       "  u'emphasized',\n",
       "  u'the',\n",
       "  u'``',\n",
       "  u'golden',\n",
       "  u'anniversary',\n",
       "  u\"''\",\n",
       "  u'with',\n",
       "  u'various',\n",
       "  u'gold-themed',\n",
       "  u'initiatives',\n",
       "  u',',\n",
       "  u'as',\n",
       "  u'well',\n",
       "  u'as',\n",
       "  u'temporarily',\n",
       "  u'suspending',\n",
       "  u'the',\n",
       "  u'tradition',\n",
       "  u'of',\n",
       "  u'naming',\n",
       "  u'each',\n",
       "  u'super',\n",
       "  u'bowl',\n",
       "  u'game',\n",
       "  u'with',\n",
       "  u'roman',\n",
       "  u'numerals',\n",
       "  u'(',\n",
       "  u'under',\n",
       "  u'which',\n",
       "  u'the',\n",
       "  u'game',\n",
       "  u'would',\n",
       "  u'have',\n",
       "  u'been',\n",
       "  u'known',\n",
       "  u'as',\n",
       "  u'``',\n",
       "  u'super',\n",
       "  u'bowl',\n",
       "  u'l',\n",
       "  u\"''\",\n",
       "  u')',\n",
       "  u',',\n",
       "  u'so',\n",
       "  u'that',\n",
       "  u'the',\n",
       "  u'logo',\n",
       "  u'could',\n",
       "  u'prominently',\n",
       "  u'feature',\n",
       "  u'the',\n",
       "  u'arabic',\n",
       "  u'numerals',\n",
       "  u'50',\n",
       "  u'.']]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dev[0][1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 119, 104, 105, 99, 104, 2],\n",
       "  [1, 110, 102, 108, 2],\n",
       "  [1, 116, 101, 97, 109, 2],\n",
       "  [1, 114, 101, 112, 114, 101, 115, 101, 110, 116, 101, 100, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 97, 102, 99, 2],\n",
       "  [1, 97, 116, 2],\n",
       "  [1, 115, 117, 112, 101, 114, 2],\n",
       "  [1, 98, 111, 119, 108, 2],\n",
       "  [1, 53, 48, 2],\n",
       "  [1, 63, 2]],\n",
       " [[1, 115, 117, 112, 101, 114, 2],\n",
       "  [1, 98, 111, 119, 108, 2],\n",
       "  [1, 53, 48, 2],\n",
       "  [1, 119, 97, 115, 2],\n",
       "  [1, 97, 110, 2],\n",
       "  [1, 97, 109, 101, 114, 105, 99, 97, 110, 2],\n",
       "  [1, 102, 111, 111, 116, 98, 97, 108, 108, 2],\n",
       "  [1, 103, 97, 109, 101, 2],\n",
       "  [1, 116, 111, 2],\n",
       "  [1, 100, 101, 116, 101, 114, 109, 105, 110, 101, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 99, 104, 97, 109, 112, 105, 111, 110, 2],\n",
       "  [1, 111, 102, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 110, 97, 116, 105, 111, 110, 97, 108, 2],\n",
       "  [1, 102, 111, 111, 116, 98, 97, 108, 108, 2],\n",
       "  [1, 108, 101, 97, 103, 117, 101, 2],\n",
       "  [1, 40, 2],\n",
       "  [1, 110, 102, 108, 2],\n",
       "  [1, 41, 2],\n",
       "  [1, 102, 111, 114, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 50, 48, 49, 53, 2],\n",
       "  [1, 115, 101, 97, 115, 111, 110, 2],\n",
       "  [1, 46, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 97, 109, 101, 114, 105, 99, 97, 110, 2],\n",
       "  [1, 102, 111, 111, 116, 98, 97, 108, 108, 2],\n",
       "  [1, 99, 111, 110, 102, 101, 114, 101, 110, 99, 101, 2],\n",
       "  [1, 40, 2],\n",
       "  [1, 97, 102, 99, 2],\n",
       "  [1, 41, 2],\n",
       "  [1, 99, 104, 97, 109, 112, 105, 111, 110, 2],\n",
       "  [1, 100, 101, 110, 118, 101, 114, 2],\n",
       "  [1, 98, 114, 111, 110, 99, 111, 115, 2],\n",
       "  [1, 100, 101, 102, 101, 97, 116, 101, 100, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 110, 97, 116, 105, 111, 110, 97, 108, 2],\n",
       "  [1, 102, 111, 111, 116, 98, 97, 108, 108, 2],\n",
       "  [1, 99, 111, 110, 102, 101, 114, 101, 110, 99, 101, 2],\n",
       "  [1, 40, 2],\n",
       "  [1, 110, 102, 99, 2],\n",
       "  [1, 41, 2],\n",
       "  [1, 99, 104, 97, 109, 112, 105, 111, 110, 2],\n",
       "  [1, 99, 97, 114, 111, 108, 105, 110, 97, 2],\n",
       "  [1, 112, 97, 110, 116, 104, 101, 114, 115, 2],\n",
       "  [1, 50, 52, 0, 49, 48, 2],\n",
       "  [1, 116, 111, 2],\n",
       "  [1, 101, 97, 114, 110, 2],\n",
       "  [1, 116, 104, 101, 105, 114, 2],\n",
       "  [1, 116, 104, 105, 114, 100, 2],\n",
       "  [1, 115, 117, 112, 101, 114, 2],\n",
       "  [1, 98, 111, 119, 108, 2],\n",
       "  [1, 116, 105, 116, 108, 101, 2],\n",
       "  [1, 46, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 103, 97, 109, 101, 2],\n",
       "  [1, 119, 97, 115, 2],\n",
       "  [1, 112, 108, 97, 121, 101, 100, 2],\n",
       "  [1, 111, 110, 2],\n",
       "  [1, 102, 101, 98, 114, 117, 97, 114, 121, 2],\n",
       "  [1, 55, 2],\n",
       "  [1, 44, 2],\n",
       "  [1, 50, 48, 49, 54, 2],\n",
       "  [1, 44, 2],\n",
       "  [1, 97, 116, 2],\n",
       "  [1, 108, 101, 118, 105, 2],\n",
       "  [1, 39, 115, 2],\n",
       "  [1, 115, 116, 97, 100, 105, 117, 109, 2],\n",
       "  [1, 105, 110, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 115, 97, 110, 2],\n",
       "  [1, 102, 114, 97, 110, 99, 105, 115, 99, 111, 2],\n",
       "  [1, 98, 97, 121, 2],\n",
       "  [1, 97, 114, 101, 97, 2],\n",
       "  [1, 97, 116, 2],\n",
       "  [1, 115, 97, 110, 116, 97, 2],\n",
       "  [1, 99, 108, 97, 114, 97, 2],\n",
       "  [1, 44, 2],\n",
       "  [1, 99, 97, 108, 105, 102, 111, 114, 110, 105, 97, 2],\n",
       "  [1, 46, 2],\n",
       "  [1, 97, 115, 2],\n",
       "  [1, 116, 104, 105, 115, 2],\n",
       "  [1, 119, 97, 115, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 53, 48, 116, 104, 2],\n",
       "  [1, 115, 117, 112, 101, 114, 2],\n",
       "  [1, 98, 111, 119, 108, 2],\n",
       "  [1, 44, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 108, 101, 97, 103, 117, 101, 2],\n",
       "  [1, 101, 109, 112, 104, 97, 115, 105, 122, 101, 100, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 96, 96, 2],\n",
       "  [1, 103, 111, 108, 100, 101, 110, 2],\n",
       "  [1, 97, 110, 110, 105, 118, 101, 114, 115, 97, 114, 121, 2],\n",
       "  [1, 39, 39, 2],\n",
       "  [1, 119, 105, 116, 104, 2],\n",
       "  [1, 118, 97, 114, 105, 111, 117, 115, 2],\n",
       "  [1, 103, 111, 108, 100, 45, 116, 104, 101, 109, 101, 100, 2],\n",
       "  [1, 105, 110, 105, 116, 105, 97, 116, 105, 118, 101, 115, 2],\n",
       "  [1, 44, 2],\n",
       "  [1, 97, 115, 2],\n",
       "  [1, 119, 101, 108, 108, 2],\n",
       "  [1, 97, 115, 2],\n",
       "  [1, 116, 101, 109, 112, 111, 114, 97, 114, 105, 108, 121, 2],\n",
       "  [1, 115, 117, 115, 112, 101, 110, 100, 105, 110, 103, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 116, 114, 97, 100, 105, 116, 105, 111, 110, 2],\n",
       "  [1, 111, 102, 2],\n",
       "  [1, 110, 97, 109, 105, 110, 103, 2],\n",
       "  [1, 101, 97, 99, 104, 2],\n",
       "  [1, 115, 117, 112, 101, 114, 2],\n",
       "  [1, 98, 111, 119, 108, 2],\n",
       "  [1, 103, 97, 109, 101, 2],\n",
       "  [1, 119, 105, 116, 104, 2],\n",
       "  [1, 114, 111, 109, 97, 110, 2],\n",
       "  [1, 110, 117, 109, 101, 114, 97, 108, 115, 2],\n",
       "  [1, 40, 2],\n",
       "  [1, 117, 110, 100, 101, 114, 2],\n",
       "  [1, 119, 104, 105, 99, 104, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 103, 97, 109, 101, 2],\n",
       "  [1, 119, 111, 117, 108, 100, 2],\n",
       "  [1, 104, 97, 118, 101, 2],\n",
       "  [1, 98, 101, 101, 110, 2],\n",
       "  [1, 107, 110, 111, 119, 110, 2],\n",
       "  [1, 97, 115, 2],\n",
       "  [1, 96, 96, 2],\n",
       "  [1, 115, 117, 112, 101, 114, 2],\n",
       "  [1, 98, 111, 119, 108, 2],\n",
       "  [1, 108, 2],\n",
       "  [1, 39, 39, 2],\n",
       "  [1, 41, 2],\n",
       "  [1, 44, 2],\n",
       "  [1, 115, 111, 2],\n",
       "  [1, 116, 104, 97, 116, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 108, 111, 103, 111, 2],\n",
       "  [1, 99, 111, 117, 108, 100, 2],\n",
       "  [1, 112, 114, 111, 109, 105, 110, 101, 110, 116, 108, 121, 2],\n",
       "  [1, 102, 101, 97, 116, 117, 114, 101, 2],\n",
       "  [1, 116, 104, 101, 2],\n",
       "  [1, 97, 114, 97, 98, 105, 99, 2],\n",
       "  [1, 110, 117, 109, 101, 114, 97, 108, 115, 2],\n",
       "  [1, 53, 48, 2],\n",
       "  [1, 46, 2]]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dev_char[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_chars = map(lambda x: x[0], sorted(c_to_i.items(), key=lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open('/pio/data/data/squad/train_charlist.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in sorted_chars:\n",
    "        f.write(unicode(w + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/train_char_ascii.pkl', 'w') as f:\n",
    "    pickle.dump(data_char, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev_char_ascii.pkl', 'w') as f:\n",
    "    pickle.dump(data_dev_char, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD data with glove dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add unk to glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_vec = np.load('/pio/data/data/glove_vec/6B/glove.6B.300d.npy')\n",
    "\n",
    "glove_words = []\n",
    "\n",
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.wordlist.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        glove_words.append(line.split()[0])\n",
    "        \n",
    "glove_words.insert(0, '<unk>')\n",
    "glove_vec = np.vstack([glove_vec.mean(axis=0), glove_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_i_to_w = glove_words\n",
    "glove_w_to_i = {v:k for (k,v) in list(enumerate(glove_words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/pio/data/data/glove_vec/6B/glove.6B.300d', glove_vec)\n",
    "\n",
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.wordlist.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in glove_words:\n",
    "        f.write(unicode(w + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make train and dev set with glove dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.load('/pio/data/data/squad/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally contexts are split into sentences, this reverses that.\n",
    "for i in xrange(len(train_set)):\n",
    "    train_set[i].append(list(chain(*train_set[i][1][1:])))\n",
    "    train_set[i][1] = train_set[i][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "for di in xrange(len(train_set)):\n",
    "    for si in xrange(len(train_set[di][1:])):\n",
    "        for ii in xrange(len(train_set[di][1:][si])):\n",
    "            i = train_set[di][1:][si][ii]\n",
    "            train_set[di][1:][si][ii] = glove_w_to_i.get(i_to_w[i], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/train_with_glove_vocab.pkl', 'w') as f:\n",
    "    pickle.dump(train_set, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zrobione wyej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = np.load('/pio/data/data/squad/train_with_glove_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_set = np.load('/pio/data/data/squad/dev_with_glove_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Originally contexts are split into sentences, this reverses that.\n",
    "for i in xrange(len(dev_set)):\n",
    "    dev_set[i].append(list(chain(*dev_set[i][1][1:])))\n",
    "    dev_set[i][1] = dev_set[i][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev_with_glove_vocab.pkl', 'w') as f:\n",
    "    pickle.dump(dev_set, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data powinno by bezporednio po wykonaniu okienka, w ktrym jest inicjowane words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_chars = sorted({c for w in glove_w_to_i for c in w})\n",
    "glove_chars.insert(0, '<unk_char>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_i_to_c = glove_chars\n",
    "glove_c_to_i = {v:k for (k,v) in list(enumerate(glove_chars))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.charlist.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in glove_chars:\n",
    "        f.write(unicode(w + '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_char = []\n",
    "\n",
    "for _, q, x in data:\n",
    "    q_char = [[glove_c_to_i.get(c, 0) for c in w] for w in q]\n",
    "    x_char = [[glove_c_to_i.get(c, 0) for c in w] for w in x]\n",
    "    data_char.append([q_char, x_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dev_char = []\n",
    "\n",
    "for _, q, x, _ in data_dev:\n",
    "    q_char = [[glove_c_to_i.get(c, 0) for c in w] for w in q]\n",
    "    x_char = [[glove_c_to_i.get(c, 0) for c in w] for w in x]\n",
    "    data_dev_char.append([q_char, x_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/train_char_with_glove_alphabet.pkl', 'w') as f:\n",
    "    pickle.dump(data_char, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev_char_with_glove_alphabet.pkl', 'w') as f:\n",
    "    pickle.dump(data_dev_char, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
