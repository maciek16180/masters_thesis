{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, nltk, io, pickle\n",
    "import numpy as np\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/train-v1.1.json') as f:\n",
    "    train = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev-v1.1.json') as f:\n",
    "    dev = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_ver = '840B'\n",
    "glove_words_fname = 'glove.' + glove_ver + '.wordlist.pkl'\n",
    "glove_words_path  = '/pio/data/data/glove_vec/' + glove_ver + '/glove/' + glove_words_fname\n",
    "glove_words = np.load(glove_words_path)\n",
    "\n",
    "i_to_w = glove_words\n",
    "w_to_i = {v:k for (k,v) in i_to_w.items()}\n",
    "\n",
    "def words_to_num(s):\n",
    "    return map(lambda x: w_to_i.get(x, w_to_i['<unk>']), s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_bin_feats(sample):\n",
    "    q, x = sample\n",
    "    qset = set(q)\n",
    "    return [w in qset for w in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab all the question-answer pairs and create a wordlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "lower = lambda x: x.lower()\n",
    "\n",
    "for par in train['data']:\n",
    "    title = par['title']\n",
    "    \n",
    "    for con in par['paragraphs']:\n",
    "        context = con['context']\n",
    "        context_tok = map(lower, nltk.word_tokenize(context))\n",
    "        \n",
    "        for q in con['qas']:\n",
    "            question = q['question']\n",
    "            question_tok = map(lower, nltk.word_tokenize(question))\n",
    "            \n",
    "            Id = q['id']\n",
    "            \n",
    "            answers = []\n",
    "            \n",
    "            for ans in q['answers']:\n",
    "                text = ans['text']\n",
    "                text_tok = map(lower, nltk.word_tokenize(text))\n",
    "                ans_start = ans['answer_start']\n",
    "                \n",
    "                answers.append((ans_start, text_tok))\n",
    "                \n",
    "            data.append([answers, question_tok, context_tok])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/train.pkl', 'w') as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bin_feats = map(make_bin_feats, [d[1:] for d in data])\n",
    "\n",
    "with open('/pio/data/data/squad/train_bin_feats.pkl', 'w') as f:\n",
    "    pickle.dump(bin_feats, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn words into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_on_dot(s):\n",
    "    res = [[]]\n",
    "    for w in s:\n",
    "        res[-1].append(w)\n",
    "        if w == u'.':\n",
    "            res.append([])\n",
    "    return res if res[-1] else res[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(data)):\n",
    "    data[i][2] = split_on_dot(data[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_num = []\n",
    "\n",
    "for a, q, c in data:\n",
    "    answers = []\n",
    "    for ans in a:\n",
    "        answers.append((ans[0], words_to_num(ans[1])))        \n",
    "    data_num.append([answers, words_to_num(q), map(words_to_num, c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_num = [[l[0], [l[1]] + l[2]] for l in data_num]\n",
    "data_num = [[[t[1] for t in l[0]], l[1]] for l in data_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1028"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are some broken answers, because of the tokenizer (I count words instead of characters)\n",
    "\n",
    "k = 0\n",
    "for a, q in data_num:\n",
    "    for w in a[0]:\n",
    "        if w not in list(chain(*q[1:])):\n",
    "            k += 1\n",
    "k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find answer indices on words, not characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inds = []\n",
    "\n",
    "for a, q in data_num:\n",
    "    ans = []\n",
    "    tot_q = list(chain(*q[1:]))\n",
    "    for x in a:\n",
    "        for i in xrange(len(tot_q)):\n",
    "            if x == tot_q[i:i+len(x)]:\n",
    "                ans.append(list(xrange(i, i + len(x))))\n",
    "                break\n",
    "    inds.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in xrange(len(data_num)):\n",
    "    data_num[i][0] = inds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This file has a lot of redundant parts, context is repeated for each question.\n",
    "# It only slows down the initial loading.\n",
    "\n",
    "with open('/pio/data/data/squad/train.pkl', 'w') as f:\n",
    "    pickle.dump(data_num, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dev = []\n",
    "lower = lambda x: x.lower()\n",
    "\n",
    "for par in dev['data']:\n",
    "    title = par['title']\n",
    "    \n",
    "    for con in par['paragraphs']:\n",
    "        context = con['context']\n",
    "        context_tok = map(lower, nltk.word_tokenize(context))\n",
    "        \n",
    "        for q in con['qas']:\n",
    "            question = q['question']\n",
    "            question_tok = map(lower, nltk.word_tokenize(question))\n",
    "            \n",
    "            Id = q['id']\n",
    "            \n",
    "            answers = []\n",
    "            \n",
    "            for ans in q['answers']:\n",
    "                text = ans['text']\n",
    "                text_tok = map(lower, nltk.word_tokenize(text))\n",
    "                ans_start = ans['answer_start']\n",
    "                \n",
    "                answers.append((ans_start, text_tok))\n",
    "                \n",
    "            data_dev.append([answers, question_tok, context_tok, Id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev.pkl', 'w') as f:\n",
    "    pickle.dump(data_dev, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_bin_feats = map(make_bin_feats, [d[1:3] for d in data_dev])\n",
    "\n",
    "with open('/pio/data/data/squad/dev_bin_feats.pkl', 'w') as f:\n",
    "    pickle.dump(dev_bin_feats, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(data_dev)):\n",
    "    data_dev[i][2] = split_on_dot(data_dev[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_num_dev = []\n",
    "\n",
    "for a, q, c, _ in data_dev:\n",
    "    answers = []\n",
    "    for ans in a:\n",
    "        answers.append((ans[0], words_to_num(ans[1])))        \n",
    "    data_num_dev.append([answers, words_to_num(q), map(words_to_num, c)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_num_dev = [[l[0], [l[1]] + l[2]] for l in data_num_dev]\n",
    "data_num_dev = [[[t[1] for t in l[0]], l[1]] for l in data_num_dev]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inds = []\n",
    "\n",
    "for a, q in data_num_dev:\n",
    "    ans = []\n",
    "    tot_q = list(chain(*q[1:]))\n",
    "    for x in a:\n",
    "        for i in xrange(len(tot_q)):\n",
    "            if x == tot_q[i:i+len(x)]:\n",
    "                ans.append(list(xrange(i, i + len(x))))\n",
    "                break\n",
    "    inds.append(ans)\n",
    "    \n",
    "for i in xrange(len(data_num_dev)):\n",
    "    data_num_dev[i][0] = inds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev_with_glove_vocab.pkl', 'w') as f:\n",
    "    pickle.dump(data_num_dev, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun with characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 0 - unk\n",
    "# 1 - start\n",
    "# 2 - end\n",
    "# 3 - not_a_word char (added later, in wikipedia negative samplesF)\n",
    "# there are no 1s or 2s in data, so these are safe\n",
    "\n",
    "chars = [unichr(i) for i in xrange(128)]\n",
    "c_to_i = {v:k for (k,v) in list(enumerate(chars))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_char = []\n",
    "\n",
    "for _, q, x in data:\n",
    "    q_char = [[1] + [c_to_i.get(c, 0) for c in w] + [2] for w in q]\n",
    "    x_char = [[1] + [c_to_i.get(c, 0) for c in w] + [2] for w in x]\n",
    "    data_char.append([q_char, x_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/train_char_ascii.pkl', 'w') as f:\n",
    "    pickle.dump(data_char, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dev_char = []\n",
    "\n",
    "for _, q, x, _ in data_dev:\n",
    "    q_char = [[1] + [c_to_i.get(c, 0) for c in w] + [2] for w in q]\n",
    "    x_char = [[1] + [c_to_i.get(c, 0) for c in w] + [2] for w in x]\n",
    "    data_dev_char.append([q_char, x_char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev_char_ascii.pkl', 'w') as f:\n",
    "    pickle.dump(data_dev_char, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQuAD data with glove dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add unk to glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_vec = np.load('/pio/data/data/glove_vec/6B/glove.6B.300d.npy')\n",
    "\n",
    "glove_words = []\n",
    "\n",
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.wordlist.txt', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        glove_words.append(line.split()[0])\n",
    "        \n",
    "glove_words.insert(0, '<unk>')\n",
    "glove_vec = np.vstack([glove_vec.mean(axis=0), glove_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove_i_to_w = glove_words\n",
    "glove_w_to_i = {v:k for (k,v) in list(enumerate(glove_words))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('/pio/data/data/glove_vec/6B/glove.6B.300d', glove_vec)\n",
    "\n",
    "with io.open('/pio/data/data/glove_vec/6B/glove.6B.wordlist.txt', 'w', encoding='utf-8') as f:\n",
    "    for w in glove_words:\n",
    "        f.write(unicode(w + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make train and dev set with glove dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_set = np.load('/pio/data/data/squad/train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Originally contexts are split into sentences, this reverses that.\n",
    "for i in xrange(len(train_set)):\n",
    "    train_set[i].append(list(chain(*train_set[i][1][1:])))\n",
    "    train_set[i][1] = train_set[i][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for di in xrange(len(train_set)):\n",
    "    for si in xrange(len(train_set[di][1:])):\n",
    "        for ii in xrange(len(train_set[di][1:][si])):\n",
    "            i = train_set[di][1:][si][ii]\n",
    "            train_set[di][1:][si][ii] = glove_w_to_i.get(i_to_w[i], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/train_with_glove_vocab.pkl', 'w') as f:\n",
    "    pickle.dump(train_set, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# zrobione wy≈ºej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dev_set = np.load('/pio/data/data/squad/dev_with_glove_vocab.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Originally contexts are split into sentences, this reverses that.\n",
    "for i in xrange(len(dev_set)):\n",
    "    dev_set[i].append(list(chain(*dev_set[i][1][1:])))\n",
    "    dev_set[i][1] = dev_set[i][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('/pio/data/data/squad/dev_with_glove_vocab.pkl', 'w') as f:\n",
    "    pickle.dump(dev_set, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
