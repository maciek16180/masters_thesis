{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "import numpy as np\n",
    "import lasagne as L\n",
    "from squad_load import get_glove_train_embs, get_squad_train_voc, load_squad_train, get_squad_train_chars\n",
    "from itertools import chain\n",
    "\n",
    "%aimport QANet\n",
    "%aimport HRED_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples total: 87599\n",
      "Working examples: 86474\n",
      "CPU times: user 58.3 s, sys: 1.21 s, total: 59.5 s\n",
      "Wall time: 59.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "squad_path = '/pio/data/data/squad/'\n",
    "glove_path = '/pio/data/data/glove_vec/6B/'\n",
    "\n",
    "data = load_squad_train(squad_path, with_chars=True)\n",
    "i_to_w, w_to_i, voc_size = get_squad_train_voc(squad_path)\n",
    "i_to_c, c_to_i, alphabet_size = get_squad_train_chars(squad_path)\n",
    "glove_embs = get_glove_train_embs(squad_path, glove_path)\n",
    "\n",
    "def get_w(idx):\n",
    "    return i_to_w[idx]\n",
    "\n",
    "# Some answers get broken in the process of tokenization, because some answer words are not properly split.\n",
    "def filter_broken_answers(data):\n",
    "    return zip(*[d for d in zip(*data) if d[0][0]])\n",
    "\n",
    "print 'Examples total:', len(data[0])\n",
    "\n",
    "data = filter_broken_answers(data)\n",
    "\n",
    "print 'Working examples:', len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = [map(list, data[0]), map(list, data[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Originally contexts are split into sentences, this reverses that.\n",
    "for i in xrange(len(data[0])):\n",
    "    data[0][i].append(list(chain(*data[0][i][1][1:])))\n",
    "    data[0][i][1] = data[0][i][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimmed examples: 86355\n"
     ]
    }
   ],
   "source": [
    "trim = 300\n",
    "data = zip(*[(d0[:2] + [d0[2][:trim]], [d1[0], d1[1][:trim]]) for d0, d1 in zip(*data) if max(d0[0][0]) < trim])\n",
    "print 'Trimmed examples:', len(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [map(list, data[0]), map(list, data[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# debug section below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = QANet.iterate_minibatches(data, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = a.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions, contexts, questions_char, contexts_char, bin_feats, question_mask, context_mask, \\\n",
    "                    question_char_mask, context_char_mask, answer_inds =b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1014,  678, 1240,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [ 665,  303,  765,  918,  385,  918,  753,  303,   -1],\n",
       "        [1087,  222,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [1014,  678, 1240,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [ 765,  303,  753,  540, 1240, 1004,   -1,   -1,   -1],\n",
       "        [ 678, 1240,  303,  540, 1014,   -1,   -1,   -1,   -1],\n",
       "        [ 303, 1014,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [  96, 1087, 1014,  540, 1240,   -1,   -1,   -1,   -1],\n",
       "        [1004,  303,  611, 1240,   -1,   -1,   -1,   -1,   -1],\n",
       "        [ 918,  765,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [ 665, 1240,  765,  918, 1004, 1240,   -1,   -1,   -1],\n",
       "        [1014, 1087,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [ 469,  678,  918,  753,  678,   -1,   -1,   -1,   -1],\n",
       "        [ 765, 1014,  540, 1247,  753, 1014, 1247,  540, 1240],\n",
       "        [  76,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1]],\n",
       "\n",
       "       [[ 469,  678,  303, 1014,   -1,   -1,   -1,   -1,   -1],\n",
       "        [ 918,  765,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [1014,  678, 1240,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [ 459,  540, 1087, 1014, 1014, 1087,   -1,   -1,   -1],\n",
       "        [ 303, 1014,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [  96, 1087, 1014,  540, 1240,   -1,   -1,   -1,   -1],\n",
       "        [1004,  303,  611, 1240,   -1,   -1,   -1,   -1,   -1],\n",
       "        [  76,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1],\n",
       "        [  -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1,   -1]]], dtype=int32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1014, 678, 1240],\n",
       "  [665, 303, 765, 918, 385, 918, 753, 303],\n",
       "  [1087, 222],\n",
       "  [1014, 678, 1240],\n",
       "  [765, 303, 753, 540, 1240, 1004],\n",
       "  [678, 1240, 303, 540, 1014],\n",
       "  [303, 1014],\n",
       "  [96, 1087, 1014, 540, 1240],\n",
       "  [1004, 303, 611, 1240],\n",
       "  [918, 765],\n",
       "  [665, 1240, 765, 918, 1004, 1240],\n",
       "  [1014, 1087],\n",
       "  [469, 678, 918, 753, 678],\n",
       "  [765, 1014, 540, 1247, 753, 1014, 1247, 540, 1240],\n",
       "  [76]],\n",
       " [[303,\n",
       "   540,\n",
       "   753,\n",
       "   678,\n",
       "   918,\n",
       "   1014,\n",
       "   1240,\n",
       "   753,\n",
       "   1014,\n",
       "   1247,\n",
       "   540,\n",
       "   303,\n",
       "   385,\n",
       "   385,\n",
       "   930],\n",
       "  [343],\n",
       "  [1014, 678, 1240],\n",
       "  [765, 753, 678, 1087, 1087, 385],\n",
       "  [678, 303, 765],\n",
       "  [303],\n",
       "  [753, 303, 1014, 678, 1087, 385, 918, 753],\n",
       "  [753, 678, 303, 540, 303, 753, 1014, 1240, 540],\n",
       "  [800],\n",
       "  [303, 1014, 1087, 75],\n",
       "  [1014, 678, 1240],\n",
       "  [611, 303, 918, 96],\n",
       "  [665, 1247, 918, 385, 1004, 918, 96, 459],\n",
       "  [419, 765],\n",
       "  [459, 1087, 385, 1004],\n",
       "  [1004, 1087, 611, 1240],\n",
       "  [918, 765],\n",
       "  [303],\n",
       "  [459, 1087, 385, 1004, 1240, 96],\n",
       "  [765, 1014, 303, 1014, 1247, 1240],\n",
       "  [1087, 222],\n",
       "  [1014, 678, 1240],\n",
       "  [230, 918, 540, 459, 918, 96],\n",
       "  [611, 303, 540, 930],\n",
       "  [800],\n",
       "  [918, 611, 611, 1240, 1004, 918, 303, 1014, 1240, 385, 930],\n",
       "  [918, 96],\n",
       "  [222, 540, 1087, 96, 1014],\n",
       "  [1087, 222],\n",
       "  [1014, 678, 1240],\n",
       "  [611, 303, 918, 96],\n",
       "  [665, 1247, 918, 385, 1004, 918, 96, 459],\n",
       "  [303, 96, 1004],\n",
       "  [222, 303, 753, 918, 96, 459],\n",
       "  [918, 1014],\n",
       "  [343],\n",
       "  [918, 765],\n",
       "  [303],\n",
       "  [753, 1087, 75, 75, 1240, 540],\n",
       "  [765, 1014, 303, 1014, 1247, 1240],\n",
       "  [1087, 222],\n",
       "  [753, 678, 540, 918, 765, 1014],\n",
       "  [469, 918, 1014, 678],\n",
       "  [303, 540, 611, 765],\n",
       "  [1247, 75, 540, 303, 918, 765, 1240, 1004],\n",
       "  [469, 918, 1014, 678],\n",
       "  [1014, 678, 1240],\n",
       "  [385, 1240, 459, 1240, 96, 1004],\n",
       "  [64, 64],\n",
       "  [230, 1240, 96, 918, 1014, 1240],\n",
       "  [303, 1004],\n",
       "  [611, 1240],\n",
       "  [1087, 611, 96, 1240, 765],\n",
       "  [419, 419],\n",
       "  [800],\n",
       "  [96, 1240, 686, 1014],\n",
       "  [1014, 1087],\n",
       "  [1014, 678, 1240],\n",
       "  [611, 303, 918, 96],\n",
       "  [665, 1247, 918, 385, 1004, 918, 96, 459],\n",
       "  [918, 765],\n",
       "  [1014, 678, 1240],\n",
       "  [665, 303, 765, 918, 385, 918, 753, 303],\n",
       "  [1087, 222],\n",
       "  [1014, 678, 1240],\n",
       "  [765, 303, 753, 540, 1240, 1004],\n",
       "  [678, 1240, 303, 540, 1014],\n",
       "  [800],\n",
       "  [918, 611, 611, 1240, 1004, 918, 303, 1014, 1240, 385, 930],\n",
       "  [665, 1240, 678, 918, 96, 1004],\n",
       "  [1014, 678, 1240],\n",
       "  [665, 303, 765, 918, 385, 918, 753, 303],\n",
       "  [918, 765],\n",
       "  [1014, 678, 1240],\n",
       "  [459, 540, 1087, 1014, 1014, 1087],\n",
       "  [343],\n",
       "  [303],\n",
       "  [611, 303, 540, 918, 303, 96],\n",
       "  [75, 385, 303, 753, 1240],\n",
       "  [1087, 222],\n",
       "  [75, 540, 303, 930, 1240, 540],\n",
       "  [303, 96, 1004],\n",
       "  [540, 1240, 222, 385, 1240, 753, 1014, 918, 1087, 96],\n",
       "  [800],\n",
       "  [918, 1014],\n",
       "  [918, 765],\n",
       "  [303],\n",
       "  [540, 1240, 75, 385, 918, 753, 303],\n",
       "  [1087, 222],\n",
       "  [1014, 678, 1240],\n",
       "  [459, 540, 1087, 1014, 1014, 1087],\n",
       "  [303, 1014],\n",
       "  [385, 1087, 1247, 540, 1004, 1240, 765],\n",
       "  [343],\n",
       "  [222, 540, 303, 96, 753, 1240],\n",
       "  [469, 678, 1240, 540, 1240],\n",
       "  [1014, 678, 1240],\n",
       "  [230, 918, 540, 459, 918, 96],\n",
       "  [611, 303, 540, 930],\n",
       "  [540, 1240, 75, 1247, 1014, 1240, 1004, 385, 930],\n",
       "  [303, 75, 75, 1240, 303, 540, 1240, 1004],\n",
       "  [1014, 1087],\n",
       "  [765, 303, 918, 96, 1014],\n",
       "  [665, 1240, 540, 96, 303, 1004, 1240, 1014, 1014, 1240],\n",
       "  [765, 1087, 1247, 665, 918, 540, 1087, 1247, 765],\n",
       "  [918, 96],\n",
       "  [269, 651, 1208, 651],\n",
       "  [800],\n",
       "  [303, 1014],\n",
       "  [1014, 678, 1240],\n",
       "  [1240, 96, 1004],\n",
       "  [1087, 222],\n",
       "  [1014, 678, 1240],\n",
       "  [611, 303, 918, 96],\n",
       "  [1004, 540, 918, 230, 1240],\n",
       "  [640],\n",
       "  [303, 96, 1004],\n",
       "  [918, 96],\n",
       "  [303],\n",
       "  [1004, 918, 540, 1240, 753, 1014],\n",
       "  [385, 918, 96, 1240],\n",
       "  [1014, 678, 303, 1014],\n",
       "  [753, 1087, 96, 96, 1240, 753, 1014, 765],\n",
       "  [1014, 678, 540, 1087, 1247, 459, 678],\n",
       "  [723],\n",
       "  [765, 1014, 303, 1014, 1247, 1240, 765],\n",
       "  [303, 96, 1004],\n",
       "  [1014, 678, 1240],\n",
       "  [459, 1087, 385, 1004],\n",
       "  [1004, 1087, 611, 1240],\n",
       "  [875],\n",
       "  [343],\n",
       "  [918, 765],\n",
       "  [303],\n",
       "  [765, 918, 611, 75, 385, 1240],\n",
       "  [343],\n",
       "  [611, 1087, 1004, 1240, 540, 96],\n",
       "  [765, 1014, 1087, 96, 1240],\n",
       "  [765, 1014, 303, 1014, 1247, 1240],\n",
       "  [1087, 222],\n",
       "  [611, 303, 540, 930],\n",
       "  [800]]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[76, 77, 78, 79, 80, 81, 82]],\n",
       " [88056, 19554, 67711, 16921, 14138, 87486, 82100, 83077],\n",
       " [100780,\n",
       "  44968,\n",
       "  67711,\n",
       "  60695,\n",
       "  608,\n",
       "  43315,\n",
       "  89195,\n",
       "  32610,\n",
       "  492,\n",
       "  40701,\n",
       "  67711,\n",
       "  45830,\n",
       "  54332,\n",
       "  55792,\n",
       "  78791,\n",
       "  78506,\n",
       "  19554,\n",
       "  43315,\n",
       "  51341,\n",
       "  10820,\n",
       "  23764,\n",
       "  67711,\n",
       "  87146,\n",
       "  71884,\n",
       "  492,\n",
       "  83032,\n",
       "  94338,\n",
       "  95628,\n",
       "  23764,\n",
       "  67711,\n",
       "  45830,\n",
       "  54332,\n",
       "  49698,\n",
       "  33470,\n",
       "  19557,\n",
       "  44968,\n",
       "  19554,\n",
       "  43315,\n",
       "  85100,\n",
       "  10820,\n",
       "  23764,\n",
       "  99569,\n",
       "  1485,\n",
       "  96317,\n",
       "  37478,\n",
       "  1485,\n",
       "  67711,\n",
       "  78483,\n",
       "  64851,\n",
       "  101002,\n",
       "  14122,\n",
       "  32833,\n",
       "  66547,\n",
       "  77561,\n",
       "  492,\n",
       "  19445,\n",
       "  78406,\n",
       "  67711,\n",
       "  45830,\n",
       "  54332,\n",
       "  19554,\n",
       "  67711,\n",
       "  32756,\n",
       "  23764,\n",
       "  67711,\n",
       "  7991,\n",
       "  80913,\n",
       "  492,\n",
       "  83032,\n",
       "  71615,\n",
       "  67711,\n",
       "  32756,\n",
       "  19554,\n",
       "  67711,\n",
       "  16921,\n",
       "  44968,\n",
       "  43315,\n",
       "  50307,\n",
       "  87507,\n",
       "  23764,\n",
       "  55111,\n",
       "  49698,\n",
       "  18814,\n",
       "  492,\n",
       "  19557,\n",
       "  19554,\n",
       "  43315,\n",
       "  41529,\n",
       "  23764,\n",
       "  67711,\n",
       "  16921,\n",
       "  14138,\n",
       "  72312,\n",
       "  44968,\n",
       "  97133,\n",
       "  39657,\n",
       "  67711,\n",
       "  87146,\n",
       "  71884,\n",
       "  52733,\n",
       "  15575,\n",
       "  78406,\n",
       "  1880,\n",
       "  11294,\n",
       "  70764,\n",
       "  94338,\n",
       "  55795,\n",
       "  492,\n",
       "  14138,\n",
       "  67711,\n",
       "  22438,\n",
       "  23764,\n",
       "  67711,\n",
       "  45830,\n",
       "  58251,\n",
       "  87309,\n",
       "  49698,\n",
       "  94338,\n",
       "  43315,\n",
       "  9478,\n",
       "  66270,\n",
       "  94548,\n",
       "  44767,\n",
       "  59287,\n",
       "  42821,\n",
       "  9093,\n",
       "  49698,\n",
       "  67711,\n",
       "  78791,\n",
       "  78506,\n",
       "  60426,\n",
       "  44968,\n",
       "  19554,\n",
       "  43315,\n",
       "  85325,\n",
       "  44968,\n",
       "  83589,\n",
       "  91160,\n",
       "  10820,\n",
       "  23764,\n",
       "  71884,\n",
       "  492]]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[88056, 19554, 67711, 16921, 14138, 87486, 82100, 83077],\n",
       " [100780,\n",
       "  44968,\n",
       "  67711,\n",
       "  60695,\n",
       "  608,\n",
       "  43315,\n",
       "  89195,\n",
       "  32610,\n",
       "  492,\n",
       "  40701,\n",
       "  67711,\n",
       "  45830,\n",
       "  54332,\n",
       "  55792,\n",
       "  78791,\n",
       "  78506,\n",
       "  19554,\n",
       "  43315,\n",
       "  51341,\n",
       "  10820,\n",
       "  23764,\n",
       "  67711,\n",
       "  87146,\n",
       "  71884,\n",
       "  492,\n",
       "  83032,\n",
       "  94338,\n",
       "  95628,\n",
       "  23764,\n",
       "  67711,\n",
       "  45830,\n",
       "  54332,\n",
       "  49698,\n",
       "  33470,\n",
       "  19557,\n",
       "  44968,\n",
       "  19554,\n",
       "  43315,\n",
       "  85100,\n",
       "  10820,\n",
       "  23764,\n",
       "  99569,\n",
       "  1485,\n",
       "  96317,\n",
       "  37478,\n",
       "  1485,\n",
       "  67711,\n",
       "  78483,\n",
       "  64851,\n",
       "  101002,\n",
       "  14122,\n",
       "  32833,\n",
       "  66547,\n",
       "  77561,\n",
       "  492,\n",
       "  19445,\n",
       "  78406,\n",
       "  67711,\n",
       "  45830,\n",
       "  54332,\n",
       "  19554,\n",
       "  67711,\n",
       "  32756,\n",
       "  23764,\n",
       "  67711,\n",
       "  7991,\n",
       "  80913,\n",
       "  492,\n",
       "  83032,\n",
       "  71615,\n",
       "  67711,\n",
       "  32756,\n",
       "  19554,\n",
       "  67711,\n",
       "  16921,\n",
       "  44968,\n",
       "  43315,\n",
       "  50307,\n",
       "  87507,\n",
       "  23764,\n",
       "  55111,\n",
       "  49698,\n",
       "  18814,\n",
       "  492,\n",
       "  19557,\n",
       "  19554,\n",
       "  43315,\n",
       "  41529,\n",
       "  23764,\n",
       "  67711,\n",
       "  16921,\n",
       "  14138,\n",
       "  72312,\n",
       "  44968,\n",
       "  97133,\n",
       "  39657,\n",
       "  67711,\n",
       "  87146,\n",
       "  71884,\n",
       "  52733,\n",
       "  15575,\n",
       "  78406,\n",
       "  1880,\n",
       "  11294,\n",
       "  70764,\n",
       "  94338,\n",
       "  55795,\n",
       "  492,\n",
       "  14138,\n",
       "  67711,\n",
       "  22438,\n",
       "  23764,\n",
       "  67711,\n",
       "  45830,\n",
       "  58251,\n",
       "  87309,\n",
       "  49698,\n",
       "  94338,\n",
       "  43315,\n",
       "  9478,\n",
       "  66270,\n",
       "  94548,\n",
       "  44767,\n",
       "  59287,\n",
       "  42821,\n",
       "  9093,\n",
       "  49698,\n",
       "  67711,\n",
       "  78791,\n",
       "  78506,\n",
       "  60426,\n",
       "  44968,\n",
       "  19554,\n",
       "  43315,\n",
       "  85325,\n",
       "  44968,\n",
       "  83589,\n",
       "  91160,\n",
       "  10820,\n",
       "  23764,\n",
       "  71884,\n",
       "  492]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][3][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# debug end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.03 s, sys: 15.5 ms, total: 8.05 s\n",
      "Wall time: 8.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_dev = np.load(squad_path + 'dev_with_training_vocab.pkl')\n",
    "data_dev_char = np.load(squad_path + 'dev_char_with_training_charlist.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Originally contexts are split into sentences, this reverses that.\n",
    "for i in xrange(len(data_dev)):\n",
    "    data_dev[i].append(list(chain(*data_dev[i][1][1:])))\n",
    "    data_dev[i][1] = data_dev[i][1][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Using custom update_fn.\n",
      "Compiling theano functions:\n",
      "    train_fn...\n",
      "    get_start_probs_fn...\n",
      "    get_end_probs_fn...\n",
      "Done\n",
      "CPU times: user 1min 10s, sys: 3.27 s, total: 1min 13s\n",
      "Wall time: 2min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "update_fn = lambda l, p: L.updates.adam(l, p)\n",
    "\n",
    "qa_net = QANet.QANet(voc_size=voc_size,\n",
    "                     alphabet_size=alphabet_size,\n",
    "                     emb_size=300,\n",
    "                     emb_char_size=100,\n",
    "                     num_emb_char_filters=200,\n",
    "                     rec_size=300,\n",
    "                     emb_init=glove_embs,\n",
    "                     skip_pred_fns=False,\n",
    "                     update_fn=update_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 5 batches in 6.24s\ttraining loss:\t2.588375\n",
      "Done 10 batches in 11.14s\ttraining loss:\t2.387342\n",
      "Done 15 batches in 15.62s\ttraining loss:\t2.310226\n",
      "Done 20 batches in 19.91s\ttraining loss:\t2.356700\n",
      "Done 25 batches in 24.69s\ttraining loss:\t2.240458\n",
      "Done 30 batches in 28.94s\ttraining loss:\t2.164079\n",
      "Done 35 batches in 33.36s\ttraining loss:\t2.182367\n",
      "Done 40 batches in 37.80s\ttraining loss:\t2.167793\n",
      "Done 45 batches in 42.20s\ttraining loss:\t2.231234\n",
      "Done 50 batches in 46.61s\ttraining loss:\t2.238087\n",
      "Done 55 batches in 50.64s\ttraining loss:\t2.258154\n",
      "Done 60 batches in 55.49s\ttraining loss:\t2.284326\n",
      "Done 65 batches in 59.61s\ttraining loss:\t2.314155\n",
      "Done 70 batches in 63.79s\ttraining loss:\t2.293630\n",
      "Done 75 batches in 68.54s\ttraining loss:\t2.286849\n",
      "Done 80 batches in 72.75s\ttraining loss:\t2.306152\n",
      "Done 85 batches in 77.14s\ttraining loss:\t2.293647\n",
      "Done 90 batches in 81.65s\ttraining loss:\t2.306131\n",
      "Done 95 batches in 86.04s\ttraining loss:\t2.300315\n",
      "Done 100 batches in 90.72s\ttraining loss:\t2.314458\n",
      "Done 105 batches in 95.10s\ttraining loss:\t2.322870\n",
      "Done 110 batches in 99.17s\ttraining loss:\t2.327924\n",
      "Done 115 batches in 103.72s\ttraining loss:\t2.337555\n",
      "Done 120 batches in 108.21s\ttraining loss:\t2.349244\n",
      "Done 125 batches in 112.95s\ttraining loss:\t2.323282\n",
      "Done 130 batches in 117.29s\ttraining loss:\t2.327586\n",
      "Done 135 batches in 121.66s\ttraining loss:\t2.326445\n",
      "Done 140 batches in 125.86s\ttraining loss:\t2.330983\n",
      "Done 145 batches in 130.12s\ttraining loss:\t2.328203\n",
      "Done 150 batches in 134.89s\ttraining loss:\t2.311860\n",
      "Done 155 batches in 139.51s\ttraining loss:\t2.306285\n",
      "Done 160 batches in 144.46s\ttraining loss:\t2.299755\n",
      "Done 165 batches in 148.44s\ttraining loss:\t2.302923\n",
      "Done 170 batches in 153.33s\ttraining loss:\t2.304281\n",
      "Done 175 batches in 157.75s\ttraining loss:\t2.326354\n",
      "Done 180 batches in 162.00s\ttraining loss:\t2.326453\n",
      "Done 185 batches in 166.66s\ttraining loss:\t2.333798\n",
      "Done 190 batches in 171.35s\ttraining loss:\t2.344538\n",
      "Done 195 batches in 175.88s\ttraining loss:\t2.357089\n",
      "Done 200 batches in 180.54s\ttraining loss:\t2.360444\n",
      "Done 205 batches in 185.12s\ttraining loss:\t2.362592\n",
      "Done 210 batches in 189.41s\ttraining loss:\t2.367987\n",
      "Done 215 batches in 193.61s\ttraining loss:\t2.367402\n",
      "Done 220 batches in 198.12s\ttraining loss:\t2.374731\n",
      "Done 225 batches in 202.48s\ttraining loss:\t2.383867\n",
      "Done 230 batches in 206.78s\ttraining loss:\t2.381457\n",
      "Done 235 batches in 210.53s\ttraining loss:\t2.379231\n",
      "Done 240 batches in 214.59s\ttraining loss:\t2.387079\n",
      "Done 245 batches in 219.61s\ttraining loss:\t2.380192\n",
      "Done 250 batches in 223.83s\ttraining loss:\t2.379625\n",
      "Done 255 batches in 228.45s\ttraining loss:\t2.388235\n",
      "Done 260 batches in 232.76s\ttraining loss:\t2.388956\n",
      "Done 265 batches in 237.44s\ttraining loss:\t2.388020\n",
      "Done 270 batches in 241.71s\ttraining loss:\t2.385939\n",
      "Done 275 batches in 245.79s\ttraining loss:\t2.390401\n",
      "Done 280 batches in 249.84s\ttraining loss:\t2.393557\n",
      "Done 285 batches in 254.26s\ttraining loss:\t2.397109\n",
      "Done 290 batches in 258.68s\ttraining loss:\t2.402484\n",
      "Done 295 batches in 263.24s\ttraining loss:\t2.405615\n",
      "Done 300 batches in 267.59s\ttraining loss:\t2.406581\n",
      "Done 305 batches in 271.89s\ttraining loss:\t2.405243\n",
      "Done 310 batches in 276.02s\ttraining loss:\t2.400319\n",
      "Done 315 batches in 280.24s\ttraining loss:\t2.396159\n",
      "Done 320 batches in 284.51s\ttraining loss:\t2.391980\n",
      "Done 325 batches in 288.51s\ttraining loss:\t2.392265\n",
      "Done 330 batches in 293.19s\ttraining loss:\t2.390132\n",
      "Done 335 batches in 298.21s\ttraining loss:\t2.400047\n",
      "Done 340 batches in 302.94s\ttraining loss:\t2.390984\n",
      "Done 345 batches in 307.71s\ttraining loss:\t2.393478\n",
      "Done 350 batches in 311.88s\ttraining loss:\t2.393763\n",
      "Done 355 batches in 315.79s\ttraining loss:\t2.391106\n",
      "Done 360 batches in 319.88s\ttraining loss:\t2.394957\n",
      "Done 365 batches in 324.41s\ttraining loss:\t2.391294\n",
      "Done 370 batches in 328.64s\ttraining loss:\t2.392618\n",
      "Done 375 batches in 333.55s\ttraining loss:\t2.398547\n",
      "Done 380 batches in 338.07s\ttraining loss:\t2.403613\n",
      "Done 385 batches in 342.75s\ttraining loss:\t2.408642\n",
      "Done 390 batches in 347.37s\ttraining loss:\t2.409917\n",
      "Done 395 batches in 352.10s\ttraining loss:\t2.407065\n",
      "Done 400 batches in 356.66s\ttraining loss:\t2.409899\n",
      "Done 405 batches in 361.55s\ttraining loss:\t2.410237\n",
      "Done 410 batches in 366.39s\ttraining loss:\t2.410498\n",
      "Done 415 batches in 370.81s\ttraining loss:\t2.410755\n",
      "Done 420 batches in 375.39s\ttraining loss:\t2.407205\n",
      "Done 425 batches in 379.98s\ttraining loss:\t2.404935\n",
      "Done 430 batches in 384.82s\ttraining loss:\t2.404549\n",
      "Done 435 batches in 389.61s\ttraining loss:\t2.407798\n",
      "Done 440 batches in 394.33s\ttraining loss:\t2.410748\n",
      "Done 445 batches in 398.56s\ttraining loss:\t2.410876\n",
      "Done 450 batches in 403.24s\ttraining loss:\t2.410309\n",
      "Done 455 batches in 407.63s\ttraining loss:\t2.406068\n",
      "Done 460 batches in 411.81s\ttraining loss:\t2.405424\n",
      "Done 465 batches in 416.41s\ttraining loss:\t2.406142\n",
      "Done 470 batches in 420.76s\ttraining loss:\t2.405399\n",
      "Done 475 batches in 425.19s\ttraining loss:\t2.403639\n",
      "Done 480 batches in 429.92s\ttraining loss:\t2.409558\n",
      "Done 485 batches in 434.25s\ttraining loss:\t2.408485\n",
      "Done 490 batches in 438.55s\ttraining loss:\t2.407362\n",
      "Done 495 batches in 443.11s\ttraining loss:\t2.406554\n",
      "Done 500 batches in 447.94s\ttraining loss:\t2.404423\n",
      "Done 505 batches in 451.99s\ttraining loss:\t2.400057\n",
      "Done 510 batches in 456.40s\ttraining loss:\t2.397761\n",
      "Done 515 batches in 461.16s\ttraining loss:\t2.395949\n",
      "Done 520 batches in 465.83s\ttraining loss:\t2.400867\n",
      "Done 525 batches in 470.84s\ttraining loss:\t2.403747\n",
      "Done 530 batches in 475.13s\ttraining loss:\t2.400386\n",
      "Done 535 batches in 480.02s\ttraining loss:\t2.402734\n",
      "Done 540 batches in 483.95s\ttraining loss:\t2.401516\n",
      "Done 545 batches in 487.97s\ttraining loss:\t2.396764\n",
      "Done 550 batches in 492.15s\ttraining loss:\t2.399907\n",
      "Done 555 batches in 496.56s\ttraining loss:\t2.401154\n",
      "Done 560 batches in 500.49s\ttraining loss:\t2.399126\n",
      "Done 565 batches in 505.12s\ttraining loss:\t2.399184\n",
      "Done 570 batches in 509.44s\ttraining loss:\t2.397989\n",
      "Done 575 batches in 514.17s\ttraining loss:\t2.393735\n",
      "Done 580 batches in 518.65s\ttraining loss:\t2.395320\n",
      "Done 585 batches in 522.79s\ttraining loss:\t2.393984\n",
      "Done 590 batches in 527.39s\ttraining loss:\t2.392793\n",
      "Done 595 batches in 532.07s\ttraining loss:\t2.393603\n",
      "Done 600 batches in 536.68s\ttraining loss:\t2.395312\n",
      "Done 605 batches in 541.18s\ttraining loss:\t2.396445\n",
      "Done 610 batches in 545.32s\ttraining loss:\t2.394175\n",
      "Done 615 batches in 549.99s\ttraining loss:\t2.394717\n",
      "Done 620 batches in 554.71s\ttraining loss:\t2.393343\n",
      "Done 625 batches in 558.83s\ttraining loss:\t2.388998\n",
      "Done 630 batches in 563.28s\ttraining loss:\t2.388586\n",
      "Done 635 batches in 567.95s\ttraining loss:\t2.389727\n",
      "Done 640 batches in 572.30s\ttraining loss:\t2.388306\n",
      "Done 645 batches in 576.93s\ttraining loss:\t2.390296\n",
      "Done 650 batches in 582.16s\ttraining loss:\t2.390452\n",
      "Done 655 batches in 586.40s\ttraining loss:\t2.392358\n",
      "Done 660 batches in 590.82s\ttraining loss:\t2.390095\n",
      "Done 665 batches in 594.94s\ttraining loss:\t2.390025\n",
      "Done 670 batches in 599.24s\ttraining loss:\t2.392081\n",
      "Done 675 batches in 603.49s\ttraining loss:\t2.391798\n",
      "Done 680 batches in 607.59s\ttraining loss:\t2.391822\n",
      "Done 685 batches in 612.08s\ttraining loss:\t2.396573\n",
      "Done 690 batches in 617.01s\ttraining loss:\t2.397427\n",
      "Done 695 batches in 621.09s\ttraining loss:\t2.398247\n",
      "Done 700 batches in 625.01s\ttraining loss:\t2.394822\n",
      "Done 705 batches in 629.16s\ttraining loss:\t2.393866\n",
      "Done 710 batches in 634.33s\ttraining loss:\t2.389869\n",
      "Done 715 batches in 638.77s\ttraining loss:\t2.391729\n",
      "Done 720 batches in 643.32s\ttraining loss:\t2.392625\n",
      "Done 725 batches in 647.78s\ttraining loss:\t2.394482\n",
      "Done 730 batches in 651.76s\ttraining loss:\t2.395732\n",
      "Done 735 batches in 656.39s\ttraining loss:\t2.395508\n",
      "Done 740 batches in 661.29s\ttraining loss:\t2.394557\n",
      "Done 745 batches in 666.04s\ttraining loss:\t2.395858\n",
      "Done 750 batches in 670.42s\ttraining loss:\t2.396984\n",
      "Done 755 batches in 675.17s\ttraining loss:\t2.396913\n",
      "Done 760 batches in 679.69s\ttraining loss:\t2.397200\n",
      "Done 765 batches in 683.54s\ttraining loss:\t2.397105\n",
      "Done 770 batches in 687.98s\ttraining loss:\t2.395925\n",
      "Done 775 batches in 692.08s\ttraining loss:\t2.397708\n",
      "Done 780 batches in 696.97s\ttraining loss:\t2.396471\n",
      "Done 785 batches in 700.88s\ttraining loss:\t2.396824\n",
      "Done 790 batches in 705.63s\ttraining loss:\t2.396289\n",
      "Done 795 batches in 710.02s\ttraining loss:\t2.395017\n",
      "Done 800 batches in 714.27s\ttraining loss:\t2.392081\n",
      "Done 805 batches in 718.07s\ttraining loss:\t2.394807\n",
      "Done 810 batches in 723.02s\ttraining loss:\t2.396075\n",
      "Done 815 batches in 727.48s\ttraining loss:\t2.394595\n",
      "Done 820 batches in 732.06s\ttraining loss:\t2.397048\n",
      "Done 825 batches in 736.54s\ttraining loss:\t2.397902\n",
      "Done 830 batches in 741.12s\ttraining loss:\t2.396184\n",
      "Done 835 batches in 745.66s\ttraining loss:\t2.394233\n",
      "Done 840 batches in 750.60s\ttraining loss:\t2.396008\n",
      "Done 845 batches in 755.36s\ttraining loss:\t2.394342\n",
      "Done 850 batches in 760.00s\ttraining loss:\t2.394086\n",
      "Done 855 batches in 764.46s\ttraining loss:\t2.393612\n",
      "Done 860 batches in 768.56s\ttraining loss:\t2.393648\n",
      "Done 865 batches in 773.04s\ttraining loss:\t2.392027\n",
      "Done 870 batches in 777.87s\ttraining loss:\t2.392485\n",
      "Done 875 batches in 782.31s\ttraining loss:\t2.393921\n",
      "Done 880 batches in 787.01s\ttraining loss:\t2.393484\n",
      "Done 885 batches in 791.17s\ttraining loss:\t2.391275\n",
      "Done 890 batches in 795.86s\ttraining loss:\t2.389176\n",
      "Done 895 batches in 799.99s\ttraining loss:\t2.389965\n",
      "Done 900 batches in 804.03s\ttraining loss:\t2.390877\n",
      "Done 905 batches in 808.21s\ttraining loss:\t2.392125\n",
      "Done 910 batches in 812.71s\ttraining loss:\t2.393391\n",
      "Done 915 batches in 816.77s\ttraining loss:\t2.394310\n",
      "Done 920 batches in 821.25s\ttraining loss:\t2.394937\n",
      "Done 925 batches in 825.68s\ttraining loss:\t2.394785\n",
      "Done 930 batches in 830.29s\ttraining loss:\t2.395041\n",
      "Done 935 batches in 834.95s\ttraining loss:\t2.393890\n",
      "Done 940 batches in 839.20s\ttraining loss:\t2.393505\n",
      "Done 945 batches in 843.88s\ttraining loss:\t2.391828\n",
      "Done 950 batches in 848.56s\ttraining loss:\t2.391194\n",
      "Done 955 batches in 853.01s\ttraining loss:\t2.387950\n",
      "Done 960 batches in 857.19s\ttraining loss:\t2.389852\n",
      "Done 965 batches in 862.03s\ttraining loss:\t2.389174\n",
      "Done 970 batches in 866.68s\ttraining loss:\t2.387847\n",
      "Done 975 batches in 871.72s\ttraining loss:\t2.387835\n",
      "Done 980 batches in 876.41s\ttraining loss:\t2.388988\n",
      "Done 985 batches in 880.82s\ttraining loss:\t2.387574\n",
      "Done 990 batches in 885.36s\ttraining loss:\t2.387316\n",
      "Done 995 batches in 890.13s\ttraining loss:\t2.386918\n",
      "Done 1000 batches in 894.76s\ttraining loss:\t2.385958\n",
      "Done 1005 batches in 899.22s\ttraining loss:\t2.385048\n",
      "Done 1010 batches in 903.59s\ttraining loss:\t2.387714\n",
      "Done 1015 batches in 908.41s\ttraining loss:\t2.385424\n",
      "Done 1020 batches in 912.87s\ttraining loss:\t2.385140\n",
      "Done 1025 batches in 916.85s\ttraining loss:\t2.385730\n",
      "Done 1030 batches in 921.21s\ttraining loss:\t2.383877\n",
      "Done 1035 batches in 925.68s\ttraining loss:\t2.384701\n",
      "Done 1040 batches in 930.14s\ttraining loss:\t2.384883\n",
      "Done 1045 batches in 934.62s\ttraining loss:\t2.385161\n",
      "Done 1050 batches in 939.10s\ttraining loss:\t2.386903\n",
      "Done 1055 batches in 942.95s\ttraining loss:\t2.386454\n",
      "Done 1060 batches in 947.08s\ttraining loss:\t2.386686\n",
      "Done 1065 batches in 952.15s\ttraining loss:\t2.388510\n",
      "Done 1070 batches in 956.96s\ttraining loss:\t2.388481\n",
      "Done 1075 batches in 961.64s\ttraining loss:\t2.387249\n",
      "Done 1080 batches in 965.68s\ttraining loss:\t2.387638\n",
      "Done 1085 batches in 970.41s\ttraining loss:\t2.386554\n",
      "Done 1090 batches in 975.15s\ttraining loss:\t2.384657\n",
      "Done 1095 batches in 979.46s\ttraining loss:\t2.386585\n",
      "Done 1100 batches in 983.62s\ttraining loss:\t2.386312\n",
      "Done 1105 batches in 987.81s\ttraining loss:\t2.386332\n",
      "Done 1110 batches in 992.10s\ttraining loss:\t2.386190\n",
      "Done 1115 batches in 995.94s\ttraining loss:\t2.387770\n",
      "Done 1120 batches in 1000.74s\ttraining loss:\t2.388314\n",
      "Done 1125 batches in 1005.31s\ttraining loss:\t2.386365\n",
      "Done 1130 batches in 1010.04s\ttraining loss:\t2.388598\n",
      "Done 1135 batches in 1014.40s\ttraining loss:\t2.388832\n",
      "Done 1140 batches in 1019.17s\ttraining loss:\t2.388641\n",
      "Done 1145 batches in 1022.95s\ttraining loss:\t2.387787\n",
      "Done 1150 batches in 1027.62s\ttraining loss:\t2.387365\n",
      "Done 1155 batches in 1032.37s\ttraining loss:\t2.387874\n",
      "Done 1160 batches in 1036.46s\ttraining loss:\t2.388610\n",
      "Done 1165 batches in 1041.06s\ttraining loss:\t2.389131\n",
      "Done 1170 batches in 1045.62s\ttraining loss:\t2.387249\n",
      "Done 1175 batches in 1050.12s\ttraining loss:\t2.388373\n",
      "Done 1180 batches in 1053.92s\ttraining loss:\t2.387406\n",
      "Done 1185 batches in 1057.85s\ttraining loss:\t2.388340\n",
      "Done 1190 batches in 1062.71s\ttraining loss:\t2.389012\n",
      "Done 1195 batches in 1067.31s\ttraining loss:\t2.387026\n",
      "Done 1200 batches in 1071.59s\ttraining loss:\t2.385449\n",
      "Done 1205 batches in 1076.31s\ttraining loss:\t2.386504\n",
      "Done 1210 batches in 1080.12s\ttraining loss:\t2.385625\n",
      "Done 1215 batches in 1084.40s\ttraining loss:\t2.386823\n",
      "Done 1220 batches in 1088.32s\ttraining loss:\t2.386601\n",
      "Done 1225 batches in 1092.49s\ttraining loss:\t2.388077\n",
      "Done 1230 batches in 1097.00s\ttraining loss:\t2.388060\n",
      "Done 1235 batches in 1101.64s\ttraining loss:\t2.388800\n",
      "Done 1240 batches in 1105.76s\ttraining loss:\t2.390711\n",
      "Done 1245 batches in 1110.17s\ttraining loss:\t2.390163\n",
      "Done 1250 batches in 1114.25s\ttraining loss:\t2.388529\n",
      "Done 1255 batches in 1118.98s\ttraining loss:\t2.386962\n",
      "Done 1260 batches in 1124.06s\ttraining loss:\t2.385724\n",
      "Done 1265 batches in 1128.62s\ttraining loss:\t2.387472\n",
      "Done 1270 batches in 1133.51s\ttraining loss:\t2.385864\n",
      "Done 1275 batches in 1137.45s\ttraining loss:\t2.384929\n",
      "Done 1280 batches in 1141.67s\ttraining loss:\t2.385332\n",
      "Done 1285 batches in 1146.28s\ttraining loss:\t2.384698\n",
      "Done 1290 batches in 1150.45s\ttraining loss:\t2.385830\n",
      "Done 1295 batches in 1154.98s\ttraining loss:\t2.385121\n",
      "Done 1300 batches in 1159.06s\ttraining loss:\t2.385464\n",
      "Done 1305 batches in 1163.47s\ttraining loss:\t2.386380\n",
      "Done 1310 batches in 1167.96s\ttraining loss:\t2.386923\n",
      "Done 1315 batches in 1172.85s\ttraining loss:\t2.386654\n",
      "Done 1320 batches in 1177.43s\ttraining loss:\t2.389492\n",
      "Done 1325 batches in 1182.22s\ttraining loss:\t2.388217\n",
      "Done 1330 batches in 1186.50s\ttraining loss:\t2.390342\n",
      "Done 1335 batches in 1190.84s\ttraining loss:\t2.390829\n",
      "Done 1340 batches in 1195.02s\ttraining loss:\t2.390376\n",
      "Done 1345 batches in 1199.66s\ttraining loss:\t2.389838\n",
      "Done 1350 batches in 1204.55s\ttraining loss:\t2.389252\n",
      "Done 1355 batches in 1209.08s\ttraining loss:\t2.390683\n",
      "Done 1360 batches in 1213.62s\ttraining loss:\t2.389829\n",
      "Done 1365 batches in 1218.38s\ttraining loss:\t2.390658\n",
      "Done 1370 batches in 1223.25s\ttraining loss:\t2.392040\n",
      "Done 1375 batches in 1227.61s\ttraining loss:\t2.391788\n",
      "Done 1380 batches in 1236.50s\ttraining loss:\t2.392162\n",
      "Done 1385 batches in 1241.58s\ttraining loss:\t2.392206\n",
      "Done 1390 batches in 1246.01s\ttraining loss:\t2.392649\n",
      "Done 1395 batches in 1250.93s\ttraining loss:\t2.392618\n",
      "Done 1400 batches in 1255.60s\ttraining loss:\t2.391891\n",
      "Done 1405 batches in 1260.45s\ttraining loss:\t2.393664\n",
      "Done 1410 batches in 1265.26s\ttraining loss:\t2.394202\n",
      "Done 1415 batches in 1269.93s\ttraining loss:\t2.394662\n",
      "Done 1420 batches in 1274.52s\ttraining loss:\t2.394506\n",
      "Done 1425 batches in 1278.87s\ttraining loss:\t2.394893\n",
      "Done 1430 batches in 1283.12s\ttraining loss:\t2.395083\n",
      "Done 1435 batches in 1288.15s\ttraining loss:\t2.395869\n",
      "Done 1440 batches in 1292.57s\ttraining loss:\t2.397693\n",
      "Done 1445 batches in 1296.73s\ttraining loss:\t2.398636\n",
      "Done 1450 batches in 1301.33s\ttraining loss:\t2.399695\n",
      "Done 1455 batches in 1305.73s\ttraining loss:\t2.399442\n",
      "Done 1460 batches in 1310.40s\ttraining loss:\t2.398935\n",
      "Done 1465 batches in 1314.75s\ttraining loss:\t2.400786\n",
      "Done 1470 batches in 1319.09s\ttraining loss:\t2.401527\n",
      "Done 1475 batches in 1323.46s\ttraining loss:\t2.400627\n",
      "Done 1480 batches in 1327.97s\ttraining loss:\t2.401149\n",
      "Done 1485 batches in 1331.96s\ttraining loss:\t2.400395\n",
      "Done 1490 batches in 1336.19s\ttraining loss:\t2.401089\n",
      "Done 1495 batches in 1340.12s\ttraining loss:\t2.400722\n",
      "Done 1500 batches in 1344.17s\ttraining loss:\t2.399331\n",
      "Done 1505 batches in 1348.41s\ttraining loss:\t2.398848\n",
      "Done 1510 batches in 1352.66s\ttraining loss:\t2.398581\n",
      "Done 1515 batches in 1357.25s\ttraining loss:\t2.397667\n",
      "Done 1520 batches in 1361.96s\ttraining loss:\t2.398568\n",
      "Done 1525 batches in 1366.15s\ttraining loss:\t2.398704\n",
      "Done 1530 batches in 1370.98s\ttraining loss:\t2.398904\n",
      "Done 1535 batches in 1375.14s\ttraining loss:\t2.399816\n",
      "Done 1540 batches in 1379.81s\ttraining loss:\t2.399612\n",
      "Done 1545 batches in 1383.96s\ttraining loss:\t2.400395\n",
      "Done 1550 batches in 1388.60s\ttraining loss:\t2.400021\n",
      "Done 1555 batches in 1392.93s\ttraining loss:\t2.400337\n",
      "Done 1560 batches in 1397.79s\ttraining loss:\t2.400827\n",
      "Done 1565 batches in 1402.34s\ttraining loss:\t2.400564\n",
      "Done 1570 batches in 1407.00s\ttraining loss:\t2.399568\n",
      "Done 1575 batches in 1411.50s\ttraining loss:\t2.398618\n",
      "Done 1580 batches in 1416.19s\ttraining loss:\t2.399215\n",
      "Done 1585 batches in 1420.21s\ttraining loss:\t2.398620\n",
      "Done 1590 batches in 1424.57s\ttraining loss:\t2.399334\n",
      "Done 1595 batches in 1429.41s\ttraining loss:\t2.400118\n",
      "Done 1600 batches in 1433.80s\ttraining loss:\t2.402228\n",
      "Done 1605 batches in 1438.65s\ttraining loss:\t2.403553\n",
      "Done 1610 batches in 1443.28s\ttraining loss:\t2.402941\n",
      "Done 1615 batches in 1447.54s\ttraining loss:\t2.403457\n",
      "Done 1620 batches in 1451.70s\ttraining loss:\t2.402345\n",
      "Done 1625 batches in 1455.73s\ttraining loss:\t2.402365\n",
      "Done 1630 batches in 1460.11s\ttraining loss:\t2.402223\n",
      "Done 1635 batches in 1464.50s\ttraining loss:\t2.401754\n",
      "Done 1640 batches in 1469.13s\ttraining loss:\t2.401727\n",
      "Done 1645 batches in 1474.12s\ttraining loss:\t2.401605\n",
      "Done 1650 batches in 1478.42s\ttraining loss:\t2.400989\n",
      "Done 1655 batches in 1482.68s\ttraining loss:\t2.400872\n",
      "Done 1660 batches in 1487.08s\ttraining loss:\t2.401715\n",
      "Done 1665 batches in 1491.17s\ttraining loss:\t2.401721\n",
      "Done 1670 batches in 1495.51s\ttraining loss:\t2.402175\n",
      "Done 1675 batches in 1500.31s\ttraining loss:\t2.402910\n",
      "Done 1680 batches in 1505.07s\ttraining loss:\t2.402311\n",
      "Done 1685 batches in 1509.34s\ttraining loss:\t2.402922\n",
      "Done 1690 batches in 1513.47s\ttraining loss:\t2.402515\n",
      "Done 1695 batches in 1517.97s\ttraining loss:\t2.403272\n",
      "Done 1700 batches in 1522.75s\ttraining loss:\t2.402373\n",
      "Done 1705 batches in 1527.04s\ttraining loss:\t2.402695\n",
      "Done 1710 batches in 1531.56s\ttraining loss:\t2.403827\n",
      "Done 1715 batches in 1535.88s\ttraining loss:\t2.403084\n",
      "Done 1720 batches in 1540.42s\ttraining loss:\t2.403187\n",
      "Done 1725 batches in 1544.74s\ttraining loss:\t2.402097\n",
      "Done 1730 batches in 1549.01s\ttraining loss:\t2.402471\n",
      "Done 1735 batches in 1553.70s\ttraining loss:\t2.402405\n",
      "Done 1740 batches in 1558.42s\ttraining loss:\t2.403203\n",
      "Done 1745 batches in 1562.93s\ttraining loss:\t2.404779\n",
      "Done 1750 batches in 1567.64s\ttraining loss:\t2.404854\n",
      "Done 1755 batches in 1572.07s\ttraining loss:\t2.405072\n",
      "Done 1760 batches in 1576.72s\ttraining loss:\t2.404478\n",
      "Done 1765 batches in 1580.69s\ttraining loss:\t2.404523\n",
      "Done 1770 batches in 1585.13s\ttraining loss:\t2.403928\n",
      "Done 1775 batches in 1589.43s\ttraining loss:\t2.404058\n",
      "Done 1780 batches in 1593.39s\ttraining loss:\t2.405524\n",
      "Done 1785 batches in 1598.21s\ttraining loss:\t2.405382\n",
      "Done 1790 batches in 1602.69s\ttraining loss:\t2.406953\n",
      "Done 1795 batches in 1606.75s\ttraining loss:\t2.406865\n",
      "Done 1800 batches in 1610.77s\ttraining loss:\t2.408454\n",
      "Done 1805 batches in 1615.14s\ttraining loss:\t2.408160\n",
      "Done 1810 batches in 1619.66s\ttraining loss:\t2.407809\n",
      "Done 1815 batches in 1623.77s\ttraining loss:\t2.408683\n",
      "Done 1820 batches in 1628.46s\ttraining loss:\t2.409143\n",
      "Done 1825 batches in 1632.88s\ttraining loss:\t2.409908\n",
      "Done 1830 batches in 1637.42s\ttraining loss:\t2.409938\n",
      "Done 1835 batches in 1641.87s\ttraining loss:\t2.409375\n",
      "Done 1840 batches in 1646.57s\ttraining loss:\t2.409720\n",
      "Done 1845 batches in 1650.97s\ttraining loss:\t2.409650\n",
      "Done 1850 batches in 1655.04s\ttraining loss:\t2.410402\n",
      "Done 1855 batches in 1659.71s\ttraining loss:\t2.410942\n",
      "Done 1860 batches in 1663.74s\ttraining loss:\t2.412039\n",
      "Done 1865 batches in 1668.22s\ttraining loss:\t2.412206\n",
      "Done 1870 batches in 1672.25s\ttraining loss:\t2.412554\n",
      "Done 1875 batches in 1676.78s\ttraining loss:\t2.412752\n",
      "Done 1880 batches in 1680.98s\ttraining loss:\t2.413281\n",
      "Done 1885 batches in 1685.66s\ttraining loss:\t2.413768\n",
      "Done 1890 batches in 1690.15s\ttraining loss:\t2.414405\n",
      "Done 1895 batches in 1694.35s\ttraining loss:\t2.415101\n",
      "Done 1900 batches in 1698.71s\ttraining loss:\t2.415286\n",
      "Done 1905 batches in 1702.98s\ttraining loss:\t2.415572\n",
      "Done 1910 batches in 1707.49s\ttraining loss:\t2.415423\n",
      "Done 1915 batches in 1711.79s\ttraining loss:\t2.415193\n",
      "Done 1920 batches in 1716.26s\ttraining loss:\t2.415771\n",
      "Done 1925 batches in 1721.00s\ttraining loss:\t2.415653\n",
      "Done 1930 batches in 1725.80s\ttraining loss:\t2.416090\n",
      "Done 1935 batches in 1729.93s\ttraining loss:\t2.414562\n",
      "Done 1940 batches in 1734.89s\ttraining loss:\t2.414769\n",
      "Done 1945 batches in 1739.21s\ttraining loss:\t2.414435\n",
      "Done 1950 batches in 1743.50s\ttraining loss:\t2.414555\n",
      "Done 1955 batches in 1747.98s\ttraining loss:\t2.414900\n",
      "Done 1960 batches in 1752.23s\ttraining loss:\t2.414723\n",
      "Done 1965 batches in 1757.30s\ttraining loss:\t2.415117\n",
      "Done 1970 batches in 1761.64s\ttraining loss:\t2.414308\n",
      "Done 1975 batches in 1765.82s\ttraining loss:\t2.413762\n",
      "Done 1980 batches in 1770.43s\ttraining loss:\t2.414115\n",
      "Done 1985 batches in 1775.14s\ttraining loss:\t2.413634\n",
      "Done 1990 batches in 1779.47s\ttraining loss:\t2.412937\n",
      "Done 1995 batches in 1783.40s\ttraining loss:\t2.413505\n",
      "Done 2000 batches in 1788.09s\ttraining loss:\t2.414899\n",
      "Done 2005 batches in 1792.56s\ttraining loss:\t2.414615\n",
      "Done 2010 batches in 1796.35s\ttraining loss:\t2.414093\n",
      "Done 2015 batches in 1800.79s\ttraining loss:\t2.414843\n",
      "Done 2020 batches in 1805.36s\ttraining loss:\t2.414477\n",
      "Done 2025 batches in 1809.76s\ttraining loss:\t2.414300\n",
      "Done 2030 batches in 1814.47s\ttraining loss:\t2.414203\n",
      "Done 2035 batches in 1818.92s\ttraining loss:\t2.415226\n",
      "Done 2040 batches in 1822.77s\ttraining loss:\t2.414337\n",
      "Done 2045 batches in 1827.11s\ttraining loss:\t2.414340\n",
      "Done 2050 batches in 1832.07s\ttraining loss:\t2.413532\n",
      "Done 2055 batches in 1836.57s\ttraining loss:\t2.413423\n",
      "Done 2060 batches in 1841.18s\ttraining loss:\t2.412413\n",
      "Done 2065 batches in 1845.56s\ttraining loss:\t2.412084\n",
      "Done 2070 batches in 1849.50s\ttraining loss:\t2.411803\n",
      "Done 2075 batches in 1854.03s\ttraining loss:\t2.412268\n",
      "Done 2080 batches in 1858.77s\ttraining loss:\t2.414119\n",
      "Done 2085 batches in 1863.21s\ttraining loss:\t2.413570\n",
      "Done 2090 batches in 1867.49s\ttraining loss:\t2.414490\n",
      "Done 2095 batches in 1871.94s\ttraining loss:\t2.415013\n",
      "Done 2100 batches in 1876.68s\ttraining loss:\t2.414313\n",
      "Done 2105 batches in 1880.78s\ttraining loss:\t2.413371\n",
      "Done 2110 batches in 1885.52s\ttraining loss:\t2.412107\n",
      "Done 2115 batches in 1890.09s\ttraining loss:\t2.413568\n",
      "Done 2120 batches in 1894.21s\ttraining loss:\t2.414360\n",
      "Done 2125 batches in 1898.78s\ttraining loss:\t2.414086\n",
      "Done 2130 batches in 1903.27s\ttraining loss:\t2.414411\n",
      "Done 2135 batches in 1908.26s\ttraining loss:\t2.413504\n",
      "Done 2140 batches in 1912.91s\ttraining loss:\t2.413300\n",
      "Done 2145 batches in 1917.29s\ttraining loss:\t2.413933\n",
      "Done 2150 batches in 1921.43s\ttraining loss:\t2.413997\n",
      "Done 2155 batches in 1925.81s\ttraining loss:\t2.414751\n",
      "Done 2160 batches in 1930.41s\ttraining loss:\t2.414229\n",
      "Done 2165 batches in 1934.49s\ttraining loss:\t2.414054\n",
      "Done 2170 batches in 1939.07s\ttraining loss:\t2.413840\n",
      "Done 2175 batches in 1943.57s\ttraining loss:\t2.413924\n",
      "Done 2180 batches in 1947.88s\ttraining loss:\t2.415372\n",
      "Done 2185 batches in 1952.41s\ttraining loss:\t2.415536\n",
      "Done 2190 batches in 1957.04s\ttraining loss:\t2.416179\n",
      "Done 2195 batches in 1961.01s\ttraining loss:\t2.416662\n",
      "Done 2200 batches in 1965.36s\ttraining loss:\t2.416295\n",
      "Done 2205 batches in 1969.55s\ttraining loss:\t2.415848\n",
      "Done 2210 batches in 1973.97s\ttraining loss:\t2.415670\n",
      "Done 2215 batches in 1978.81s\ttraining loss:\t2.416647\n",
      "Done 2220 batches in 1983.38s\ttraining loss:\t2.416237\n",
      "Done 2225 batches in 1987.65s\ttraining loss:\t2.417610\n",
      "Done 2230 batches in 1992.35s\ttraining loss:\t2.417099\n",
      "Done 2235 batches in 1996.92s\ttraining loss:\t2.417711\n",
      "Done 2240 batches in 2001.44s\ttraining loss:\t2.417151\n",
      "Done 2245 batches in 2005.88s\ttraining loss:\t2.417184\n",
      "Done 2250 batches in 2009.94s\ttraining loss:\t2.416983\n",
      "Done 2255 batches in 2014.38s\ttraining loss:\t2.415695\n",
      "Done 2260 batches in 2019.07s\ttraining loss:\t2.416209\n",
      "Done 2265 batches in 2023.84s\ttraining loss:\t2.417649\n",
      "Done 2270 batches in 2028.32s\ttraining loss:\t2.417736\n",
      "Done 2275 batches in 2032.64s\ttraining loss:\t2.417459\n",
      "Done 2280 batches in 2036.94s\ttraining loss:\t2.417565\n",
      "Done 2285 batches in 2041.35s\ttraining loss:\t2.417602\n",
      "Done 2290 batches in 2045.21s\ttraining loss:\t2.418124\n",
      "Done 2295 batches in 2049.47s\ttraining loss:\t2.417259\n",
      "Done 2300 batches in 2054.05s\ttraining loss:\t2.416861\n",
      "Done 2305 batches in 2058.58s\ttraining loss:\t2.418415\n",
      "Done 2310 batches in 2063.24s\ttraining loss:\t2.418405\n",
      "Done 2315 batches in 2067.43s\ttraining loss:\t2.418814\n",
      "Done 2320 batches in 2071.92s\ttraining loss:\t2.419260\n",
      "Done 2325 batches in 2076.54s\ttraining loss:\t2.419577\n",
      "Done 2330 batches in 2080.87s\ttraining loss:\t2.418862\n",
      "Done 2335 batches in 2085.47s\ttraining loss:\t2.418912\n",
      "Done 2340 batches in 2089.70s\ttraining loss:\t2.419053\n",
      "Done 2345 batches in 2094.29s\ttraining loss:\t2.419180\n",
      "Done 2350 batches in 2098.44s\ttraining loss:\t2.420147\n",
      "Done 2355 batches in 2102.77s\ttraining loss:\t2.419108\n",
      "Done 2360 batches in 2107.23s\ttraining loss:\t2.418960\n",
      "Done 2365 batches in 2111.72s\ttraining loss:\t2.418759\n",
      "Done 2370 batches in 2116.22s\ttraining loss:\t2.418849\n",
      "Done 2375 batches in 2120.65s\ttraining loss:\t2.418446\n",
      "Done 2380 batches in 2125.42s\ttraining loss:\t2.417851\n",
      "Done 2385 batches in 2130.07s\ttraining loss:\t2.417976\n",
      "Done 2390 batches in 2134.58s\ttraining loss:\t2.417497\n",
      "Done 2395 batches in 2138.61s\ttraining loss:\t2.417485\n",
      "Done 2400 batches in 2142.93s\ttraining loss:\t2.417321\n",
      "Done 2405 batches in 2147.76s\ttraining loss:\t2.417939\n",
      "Done 2410 batches in 2152.48s\ttraining loss:\t2.417561\n",
      "Done 2415 batches in 2156.92s\ttraining loss:\t2.417924\n",
      "Done 2420 batches in 2161.15s\ttraining loss:\t2.418149\n",
      "Done 2425 batches in 2165.82s\ttraining loss:\t2.417839\n",
      "Done 2430 batches in 2170.04s\ttraining loss:\t2.418170\n",
      "Done 2435 batches in 2174.44s\ttraining loss:\t2.418305\n",
      "Done 2440 batches in 2178.67s\ttraining loss:\t2.420080\n",
      "Done 2445 batches in 2183.08s\ttraining loss:\t2.420518\n",
      "Done 2450 batches in 2187.49s\ttraining loss:\t2.419450\n",
      "Done 2455 batches in 2192.20s\ttraining loss:\t2.418969\n",
      "Done 2460 batches in 2196.54s\ttraining loss:\t2.418842\n",
      "Done 2465 batches in 2201.34s\ttraining loss:\t2.418235\n",
      "Done 2470 batches in 2206.03s\ttraining loss:\t2.417196\n",
      "Done 2475 batches in 2210.75s\ttraining loss:\t2.417980\n",
      "Done 2480 batches in 2215.38s\ttraining loss:\t2.418072\n",
      "Done 2485 batches in 2219.51s\ttraining loss:\t2.418545\n",
      "Done 2490 batches in 2223.98s\ttraining loss:\t2.418922\n",
      "Done 2495 batches in 2228.29s\ttraining loss:\t2.418972\n",
      "Done 2500 batches in 2232.60s\ttraining loss:\t2.418330\n",
      "Done 2505 batches in 2236.71s\ttraining loss:\t2.417717\n",
      "Done 2510 batches in 2241.02s\ttraining loss:\t2.417056\n",
      "Done 2515 batches in 2245.32s\ttraining loss:\t2.417122\n",
      "Done 2520 batches in 2249.41s\ttraining loss:\t2.417345\n",
      "Done 2525 batches in 2253.68s\ttraining loss:\t2.416945\n",
      "Done 2530 batches in 2258.20s\ttraining loss:\t2.417011\n",
      "Done 2535 batches in 2262.30s\ttraining loss:\t2.416520\n",
      "Done 2540 batches in 2267.33s\ttraining loss:\t2.416123\n",
      "Done 2545 batches in 2272.04s\ttraining loss:\t2.416467\n",
      "Done 2550 batches in 2276.89s\ttraining loss:\t2.417085\n",
      "Done 2555 batches in 2281.39s\ttraining loss:\t2.416549\n",
      "Done 2560 batches in 2285.58s\ttraining loss:\t2.416586\n",
      "Done 2565 batches in 2290.22s\ttraining loss:\t2.416260\n",
      "Done 2570 batches in 2295.09s\ttraining loss:\t2.416013\n",
      "Done 2575 batches in 2298.94s\ttraining loss:\t2.415976\n",
      "Done 2580 batches in 2303.30s\ttraining loss:\t2.416417\n",
      "Done 2585 batches in 2308.19s\ttraining loss:\t2.416174\n",
      "Done 2590 batches in 2313.16s\ttraining loss:\t2.416611\n",
      "Done 2595 batches in 2317.98s\ttraining loss:\t2.416423\n",
      "Done 2600 batches in 2322.92s\ttraining loss:\t2.417028\n",
      "Done 2605 batches in 2327.32s\ttraining loss:\t2.416713\n",
      "Done 2610 batches in 2331.80s\ttraining loss:\t2.417333\n",
      "Done 2615 batches in 2336.17s\ttraining loss:\t2.416646\n",
      "Done 2620 batches in 2340.51s\ttraining loss:\t2.415922\n",
      "Done 2625 batches in 2345.15s\ttraining loss:\t2.416770\n",
      "Done 2630 batches in 2349.12s\ttraining loss:\t2.417348\n",
      "Done 2635 batches in 2353.98s\ttraining loss:\t2.416972\n",
      "Done 2640 batches in 2358.84s\ttraining loss:\t2.417090\n",
      "Done 2645 batches in 2363.26s\ttraining loss:\t2.416968\n",
      "Done 2650 batches in 2367.56s\ttraining loss:\t2.417688\n",
      "Done 2655 batches in 2371.70s\ttraining loss:\t2.417953\n",
      "Done 2660 batches in 2376.10s\ttraining loss:\t2.418562\n",
      "Done 2665 batches in 2380.17s\ttraining loss:\t2.418497\n",
      "Done 2670 batches in 2384.73s\ttraining loss:\t2.418052\n",
      "Done 2675 batches in 2389.21s\ttraining loss:\t2.418116\n",
      "Done 2680 batches in 2394.07s\ttraining loss:\t2.418440\n",
      "Done 2685 batches in 2399.04s\ttraining loss:\t2.418096\n",
      "Done 2690 batches in 2403.46s\ttraining loss:\t2.418620\n",
      "Done 2695 batches in 2408.16s\ttraining loss:\t2.418953\n",
      "Done 2700 batches in 2412.75s\ttraining loss:\t2.419582\n",
      "Done 2705 batches in 2417.05s\ttraining loss:\t2.418887\n",
      "Done 2710 batches in 2422.03s\ttraining loss:\t2.418868\n",
      "Done 2715 batches in 2426.48s\ttraining loss:\t2.417609\n",
      "Done 2720 batches in 2430.70s\ttraining loss:\t2.418697\n",
      "Done 2725 batches in 2435.05s\ttraining loss:\t2.418552\n",
      "Done 2730 batches in 2439.21s\ttraining loss:\t2.418236\n",
      "Done 2735 batches in 2443.90s\ttraining loss:\t2.417780\n",
      "Done 2740 batches in 2448.05s\ttraining loss:\t2.418049\n",
      "Done 2745 batches in 2452.46s\ttraining loss:\t2.417799\n",
      "Done 2750 batches in 2456.66s\ttraining loss:\t2.417652\n",
      "Done 2755 batches in 2460.89s\ttraining loss:\t2.417553\n",
      "Done 2760 batches in 2464.68s\ttraining loss:\t2.417278\n",
      "Done 2765 batches in 2469.55s\ttraining loss:\t2.417082\n",
      "Done 2770 batches in 2474.08s\ttraining loss:\t2.416848\n",
      "Done 2775 batches in 2478.00s\ttraining loss:\t2.417426\n",
      "Done 2780 batches in 2482.16s\ttraining loss:\t2.416560\n",
      "Done 2785 batches in 2486.43s\ttraining loss:\t2.416421\n",
      "Done 2790 batches in 2491.03s\ttraining loss:\t2.416242\n",
      "Done 2795 batches in 2495.32s\ttraining loss:\t2.416133\n",
      "Done 2800 batches in 2499.85s\ttraining loss:\t2.416456\n",
      "Done 2805 batches in 2504.27s\ttraining loss:\t2.416615\n",
      "Done 2810 batches in 2508.73s\ttraining loss:\t2.416359\n",
      "Done 2815 batches in 2513.17s\ttraining loss:\t2.415884\n",
      "Done 2820 batches in 2517.55s\ttraining loss:\t2.415891\n",
      "Done 2825 batches in 2522.26s\ttraining loss:\t2.416436\n",
      "Done 2830 batches in 2527.00s\ttraining loss:\t2.417108\n",
      "Done 2835 batches in 2531.58s\ttraining loss:\t2.417385\n",
      "Done 2840 batches in 2535.62s\ttraining loss:\t2.417442\n",
      "Done 2845 batches in 2539.86s\ttraining loss:\t2.416867\n",
      "Done 2850 batches in 2544.57s\ttraining loss:\t2.417083\n",
      "Done 2855 batches in 2548.81s\ttraining loss:\t2.417027\n",
      "Done 2860 batches in 2553.16s\ttraining loss:\t2.417194\n",
      "Done 2865 batches in 2557.47s\ttraining loss:\t2.417259\n",
      "Done 2870 batches in 2561.42s\ttraining loss:\t2.416520\n",
      "Done 2875 batches in 2565.61s\ttraining loss:\t2.416805\n",
      "Done 2880 batches in 2570.12s\ttraining loss:\t2.417418\n",
      "Done 2885 batches in 2574.27s\ttraining loss:\t2.418430\n",
      "Done 2890 batches in 2578.36s\ttraining loss:\t2.418622\n",
      "Done 2895 batches in 2582.82s\ttraining loss:\t2.418814\n",
      "Done 2900 batches in 2586.46s\ttraining loss:\t2.419162\n",
      "Done 2905 batches in 2590.89s\ttraining loss:\t2.419441\n",
      "Done 2910 batches in 2595.33s\ttraining loss:\t2.419199\n",
      "Done 2915 batches in 2599.85s\ttraining loss:\t2.419585\n",
      "Done 2920 batches in 2604.18s\ttraining loss:\t2.419377\n",
      "Done 2925 batches in 2608.71s\ttraining loss:\t2.419143\n",
      "Done 2930 batches in 2613.19s\ttraining loss:\t2.420004\n",
      "Done 2935 batches in 2617.67s\ttraining loss:\t2.421291\n",
      "Done 2940 batches in 2622.09s\ttraining loss:\t2.421774\n",
      "Done 2945 batches in 2626.77s\ttraining loss:\t2.421737\n",
      "Done 2950 batches in 2631.42s\ttraining loss:\t2.421837\n",
      "Done 2955 batches in 2636.22s\ttraining loss:\t2.422233\n",
      "Done 2960 batches in 2640.84s\ttraining loss:\t2.422689\n",
      "Done 2965 batches in 2645.49s\ttraining loss:\t2.424151\n",
      "Done 2970 batches in 2650.03s\ttraining loss:\t2.423520\n",
      "Done 2975 batches in 2653.68s\ttraining loss:\t2.423555\n",
      "Done 2980 batches in 2657.85s\ttraining loss:\t2.423958\n",
      "Done 2985 batches in 2662.80s\ttraining loss:\t2.424113\n",
      "Done 2990 batches in 2667.28s\ttraining loss:\t2.423423\n",
      "Done 2995 batches in 2671.60s\ttraining loss:\t2.423214\n",
      "Done 3000 batches in 2676.15s\ttraining loss:\t2.422945\n",
      "Done 3005 batches in 2680.91s\ttraining loss:\t2.423564\n",
      "Done 3010 batches in 2685.38s\ttraining loss:\t2.423240\n",
      "Done 3015 batches in 2689.19s\ttraining loss:\t2.423151\n",
      "Done 3020 batches in 2693.89s\ttraining loss:\t2.423366\n",
      "Done 3025 batches in 2698.18s\ttraining loss:\t2.423948\n",
      "Done 3030 batches in 2702.63s\ttraining loss:\t2.423913\n",
      "Done 3035 batches in 2706.87s\ttraining loss:\t2.424606\n",
      "Done 3040 batches in 2711.69s\ttraining loss:\t2.425203\n",
      "Done 3045 batches in 2715.76s\ttraining loss:\t2.424989\n",
      "Done 3050 batches in 2720.21s\ttraining loss:\t2.425192\n",
      "Done 3055 batches in 2724.70s\ttraining loss:\t2.424498\n",
      "Done 3060 batches in 2729.38s\ttraining loss:\t2.424789\n",
      "Done 3065 batches in 2734.12s\ttraining loss:\t2.425652\n",
      "Done 3070 batches in 2739.00s\ttraining loss:\t2.425259\n",
      "Done 3075 batches in 2743.37s\ttraining loss:\t2.424705\n",
      "Done 3080 batches in 2747.98s\ttraining loss:\t2.424883\n",
      "Done 3085 batches in 2752.36s\ttraining loss:\t2.424737\n",
      "Done 3090 batches in 2756.97s\ttraining loss:\t2.424037\n",
      "Done 3095 batches in 2761.50s\ttraining loss:\t2.424417\n",
      "Done 3100 batches in 2765.96s\ttraining loss:\t2.425540\n",
      "Done 3105 batches in 2770.57s\ttraining loss:\t2.426233\n",
      "Done 3110 batches in 2774.40s\ttraining loss:\t2.426299\n",
      "Done 3115 batches in 2778.60s\ttraining loss:\t2.426568\n",
      "Done 3120 batches in 2783.28s\ttraining loss:\t2.426454\n",
      "Done 3125 batches in 2787.80s\ttraining loss:\t2.425887\n",
      "Done 3130 batches in 2792.36s\ttraining loss:\t2.426092\n",
      "Done 3135 batches in 2796.90s\ttraining loss:\t2.425518\n",
      "Done 3140 batches in 2801.21s\ttraining loss:\t2.425385\n",
      "Done 3145 batches in 2805.66s\ttraining loss:\t2.424713\n",
      "Done 3150 batches in 2810.13s\ttraining loss:\t2.425048\n",
      "Done 3155 batches in 2814.93s\ttraining loss:\t2.424751\n",
      "Done 3160 batches in 2818.75s\ttraining loss:\t2.424307\n",
      "Done 3165 batches in 2823.21s\ttraining loss:\t2.423787\n",
      "Done 3170 batches in 2827.63s\ttraining loss:\t2.423913\n",
      "Done 3175 batches in 2832.14s\ttraining loss:\t2.423832\n",
      "Done 3180 batches in 2836.38s\ttraining loss:\t2.423953\n",
      "Done 3185 batches in 2840.76s\ttraining loss:\t2.424117\n",
      "Done 3190 batches in 2845.57s\ttraining loss:\t2.424655\n",
      "Done 3195 batches in 2850.08s\ttraining loss:\t2.424334\n",
      "Done 3200 batches in 2854.44s\ttraining loss:\t2.423759\n",
      "Done 3205 batches in 2858.73s\ttraining loss:\t2.423728\n",
      "Done 3210 batches in 2863.13s\ttraining loss:\t2.423517\n",
      "Done 3215 batches in 2867.34s\ttraining loss:\t2.423578\n",
      "Done 3220 batches in 2872.14s\ttraining loss:\t2.422837\n",
      "Done 3225 batches in 2876.84s\ttraining loss:\t2.422471\n",
      "Done 3230 batches in 2880.99s\ttraining loss:\t2.422369\n",
      "Done 3235 batches in 2885.49s\ttraining loss:\t2.422526\n",
      "Done 3240 batches in 2889.95s\ttraining loss:\t2.422876\n",
      "Done 3245 batches in 2894.57s\ttraining loss:\t2.423702\n",
      "Done 3250 batches in 2898.53s\ttraining loss:\t2.424156\n",
      "Done 3255 batches in 2902.90s\ttraining loss:\t2.424453\n",
      "Done 3260 batches in 2906.58s\ttraining loss:\t2.425021\n",
      "Done 3265 batches in 2910.75s\ttraining loss:\t2.425008\n",
      "Done 3270 batches in 2914.88s\ttraining loss:\t2.425605\n",
      "Done 3275 batches in 2919.72s\ttraining loss:\t2.426129\n",
      "Done 3280 batches in 2924.29s\ttraining loss:\t2.426340\n",
      "Done 3285 batches in 2928.72s\ttraining loss:\t2.426339\n",
      "Done 3290 batches in 2933.24s\ttraining loss:\t2.426085\n",
      "Done 3295 batches in 2937.65s\ttraining loss:\t2.425465\n",
      "Done 3300 batches in 2941.87s\ttraining loss:\t2.425932\n",
      "Done 3305 batches in 2946.66s\ttraining loss:\t2.426133\n",
      "Done 3310 batches in 2950.89s\ttraining loss:\t2.425870\n",
      "Done 3315 batches in 2955.61s\ttraining loss:\t2.425296\n",
      "Done 3320 batches in 2960.29s\ttraining loss:\t2.424522\n",
      "Done 3325 batches in 2964.70s\ttraining loss:\t2.424419\n",
      "Done 3330 batches in 2968.80s\ttraining loss:\t2.424322\n",
      "Done 3335 batches in 2973.70s\ttraining loss:\t2.424091\n",
      "Done 3340 batches in 2977.70s\ttraining loss:\t2.424395\n",
      "Done 3345 batches in 2982.04s\ttraining loss:\t2.424112\n",
      "Done 3350 batches in 2986.67s\ttraining loss:\t2.424746\n",
      "Done 3355 batches in 2991.37s\ttraining loss:\t2.424310\n",
      "Done 3360 batches in 2995.87s\ttraining loss:\t2.423884\n",
      "Done 3365 batches in 3000.46s\ttraining loss:\t2.423834\n",
      "Done 3370 batches in 3004.39s\ttraining loss:\t2.423997\n",
      "Done 3375 batches in 3009.04s\ttraining loss:\t2.423625\n",
      "Done 3380 batches in 3013.45s\ttraining loss:\t2.423702\n",
      "Done 3385 batches in 3017.95s\ttraining loss:\t2.423792\n",
      "Done 3390 batches in 3022.43s\ttraining loss:\t2.423309\n",
      "Done 3395 batches in 3026.85s\ttraining loss:\t2.422460\n",
      "Done 3400 batches in 3031.69s\ttraining loss:\t2.422128\n",
      "Done 3405 batches in 3036.04s\ttraining loss:\t2.422614\n",
      "Done 3410 batches in 3040.71s\ttraining loss:\t2.422312\n",
      "Done 3415 batches in 3045.10s\ttraining loss:\t2.422768\n",
      "Done 3420 batches in 3049.08s\ttraining loss:\t2.422186\n",
      "Done 3425 batches in 3053.40s\ttraining loss:\t2.422685\n",
      "Done 3430 batches in 3057.84s\ttraining loss:\t2.422352\n",
      "Done 3435 batches in 3061.98s\ttraining loss:\t2.421898\n",
      "Done 3440 batches in 3066.23s\ttraining loss:\t2.421799\n",
      "Done 3445 batches in 3070.92s\ttraining loss:\t2.422306\n",
      "Done 3450 batches in 3075.67s\ttraining loss:\t2.422037\n",
      "Done 3455 batches in 3079.93s\ttraining loss:\t2.421983\n",
      "Done 3460 batches in 3084.35s\ttraining loss:\t2.422316\n",
      "Done 3465 batches in 3088.60s\ttraining loss:\t2.422058\n",
      "Done 3470 batches in 3093.26s\ttraining loss:\t2.422072\n",
      "Done 3475 batches in 3097.83s\ttraining loss:\t2.421899\n",
      "Done 3480 batches in 3102.62s\ttraining loss:\t2.421939\n",
      "Done 3485 batches in 3107.13s\ttraining loss:\t2.421678\n",
      "Done 3490 batches in 3111.38s\ttraining loss:\t2.422221\n",
      "Done 3495 batches in 3115.75s\ttraining loss:\t2.422329\n",
      "Done 3500 batches in 3119.88s\ttraining loss:\t2.422269\n",
      "Done 3505 batches in 3124.01s\ttraining loss:\t2.421702\n",
      "Done 3510 batches in 3128.43s\ttraining loss:\t2.422264\n",
      "Done 3515 batches in 3133.14s\ttraining loss:\t2.421839\n",
      "Done 3520 batches in 3137.52s\ttraining loss:\t2.421961\n",
      "Done 3525 batches in 3141.89s\ttraining loss:\t2.421513\n",
      "Done 3530 batches in 3146.03s\ttraining loss:\t2.421381\n",
      "Done 3535 batches in 3150.54s\ttraining loss:\t2.421165\n",
      "Done 3540 batches in 3155.14s\ttraining loss:\t2.420927\n",
      "Done 3545 batches in 3159.85s\ttraining loss:\t2.421184\n",
      "Done 3550 batches in 3163.90s\ttraining loss:\t2.422018\n",
      "Done 3555 batches in 3168.04s\ttraining loss:\t2.422413\n",
      "Done 3560 batches in 3172.04s\ttraining loss:\t2.422284\n",
      "Done 3565 batches in 3175.89s\ttraining loss:\t2.421962\n",
      "Done 3570 batches in 3179.88s\ttraining loss:\t2.422184\n",
      "Done 3575 batches in 3184.08s\ttraining loss:\t2.422459\n",
      "Done 3580 batches in 3188.89s\ttraining loss:\t2.422424\n",
      "Done 3585 batches in 3192.98s\ttraining loss:\t2.422929\n",
      "Done 3590 batches in 3197.83s\ttraining loss:\t2.423394\n",
      "Done 3595 batches in 3202.44s\ttraining loss:\t2.423136\n",
      "Done 3600 batches in 3207.24s\ttraining loss:\t2.422429\n",
      "Done 3605 batches in 3212.03s\ttraining loss:\t2.421975\n",
      "Done 3610 batches in 3216.83s\ttraining loss:\t2.421633\n",
      "Done 3615 batches in 3221.50s\ttraining loss:\t2.421415\n",
      "Done 3620 batches in 3226.54s\ttraining loss:\t2.420659\n",
      "Done 3625 batches in 3230.82s\ttraining loss:\t2.421232\n",
      "Done 3630 batches in 3235.25s\ttraining loss:\t2.421430\n",
      "Done 3635 batches in 3239.63s\ttraining loss:\t2.420965\n",
      "Done 3640 batches in 3244.57s\ttraining loss:\t2.421138\n",
      "Done 3645 batches in 3248.53s\ttraining loss:\t2.421231\n",
      "Done 3650 batches in 3253.07s\ttraining loss:\t2.421318\n",
      "Done 3655 batches in 3256.99s\ttraining loss:\t2.421969\n",
      "Done 3660 batches in 3261.24s\ttraining loss:\t2.422016\n",
      "Done 3665 batches in 3265.96s\ttraining loss:\t2.422085\n",
      "Done 3670 batches in 3270.08s\ttraining loss:\t2.421225\n",
      "Done 3675 batches in 3274.58s\ttraining loss:\t2.421365\n",
      "Done 3680 batches in 3279.43s\ttraining loss:\t2.421456\n",
      "Done 3685 batches in 3284.53s\ttraining loss:\t2.421934\n",
      "Done 3690 batches in 3289.01s\ttraining loss:\t2.422151\n",
      "Done 3695 batches in 3293.41s\ttraining loss:\t2.421312\n",
      "Done 3700 batches in 3298.13s\ttraining loss:\t2.421607\n",
      "Done 3705 batches in 3302.85s\ttraining loss:\t2.421209\n",
      "Done 3710 batches in 3307.43s\ttraining loss:\t2.420385\n",
      "Done 3715 batches in 3311.72s\ttraining loss:\t2.420969\n",
      "Done 3720 batches in 3316.32s\ttraining loss:\t2.420978\n",
      "Done 3725 batches in 3321.01s\ttraining loss:\t2.421581\n",
      "Done 3730 batches in 3325.17s\ttraining loss:\t2.421600\n",
      "Done 3735 batches in 3329.97s\ttraining loss:\t2.421894\n",
      "Done 3740 batches in 3334.63s\ttraining loss:\t2.421646\n",
      "Done 3745 batches in 3338.30s\ttraining loss:\t2.421936\n",
      "Done 3750 batches in 3343.07s\ttraining loss:\t2.422291\n",
      "Done 3755 batches in 3347.49s\ttraining loss:\t2.422139\n",
      "Done 3760 batches in 3351.65s\ttraining loss:\t2.421988\n",
      "Done 3765 batches in 3356.24s\ttraining loss:\t2.422263\n",
      "Done 3770 batches in 3360.69s\ttraining loss:\t2.422289\n",
      "Done 3775 batches in 3364.55s\ttraining loss:\t2.422711\n",
      "Done 3780 batches in 3369.20s\ttraining loss:\t2.422452\n",
      "Done 3785 batches in 3373.46s\ttraining loss:\t2.422910\n",
      "Done 3790 batches in 3377.41s\ttraining loss:\t2.423178\n",
      "Done 3795 batches in 3381.70s\ttraining loss:\t2.423142\n",
      "Done 3800 batches in 3385.69s\ttraining loss:\t2.423270\n",
      "Done 3805 batches in 3390.50s\ttraining loss:\t2.423008\n",
      "Done 3810 batches in 3394.78s\ttraining loss:\t2.422576\n",
      "Done 3815 batches in 3398.77s\ttraining loss:\t2.422514\n",
      "Done 3820 batches in 3403.12s\ttraining loss:\t2.422429\n",
      "Done 3825 batches in 3408.04s\ttraining loss:\t2.422834\n",
      "Done 3830 batches in 3412.18s\ttraining loss:\t2.422824\n",
      "Done 3835 batches in 3416.36s\ttraining loss:\t2.423040\n",
      "Done 3840 batches in 3420.42s\ttraining loss:\t2.423018\n",
      "Done 3845 batches in 3424.55s\ttraining loss:\t2.422899\n",
      "Done 3850 batches in 3428.68s\ttraining loss:\t2.423079\n",
      "Done 3855 batches in 3433.23s\ttraining loss:\t2.423059\n",
      "Done 3860 batches in 3438.24s\ttraining loss:\t2.422735\n",
      "Done 3865 batches in 3442.29s\ttraining loss:\t2.423664\n",
      "Done 3870 batches in 3446.75s\ttraining loss:\t2.423757\n",
      "Done 3875 batches in 3451.00s\ttraining loss:\t2.423573\n",
      "Done 3880 batches in 3455.39s\ttraining loss:\t2.423061\n",
      "Done 3885 batches in 3459.98s\ttraining loss:\t2.422868\n",
      "Done 3890 batches in 3463.91s\ttraining loss:\t2.423396\n",
      "Done 3895 batches in 3468.23s\ttraining loss:\t2.423054\n",
      "Done 3900 batches in 3472.69s\ttraining loss:\t2.423000\n",
      "Done 3905 batches in 3477.09s\ttraining loss:\t2.423449\n",
      "Done 3910 batches in 3481.42s\ttraining loss:\t2.423943\n",
      "Done 3915 batches in 3486.11s\ttraining loss:\t2.423257\n",
      "Done 3920 batches in 3489.81s\ttraining loss:\t2.423117\n",
      "Done 3925 batches in 3493.74s\ttraining loss:\t2.423162\n",
      "Done 3930 batches in 3497.73s\ttraining loss:\t2.423485\n",
      "Done 3935 batches in 3502.47s\ttraining loss:\t2.423907\n",
      "Done 3940 batches in 3506.99s\ttraining loss:\t2.423493\n",
      "Done 3945 batches in 3511.54s\ttraining loss:\t2.422887\n",
      "Done 3950 batches in 3516.31s\ttraining loss:\t2.423180\n",
      "Done 3955 batches in 3520.84s\ttraining loss:\t2.423616\n",
      "Done 3960 batches in 3525.00s\ttraining loss:\t2.423806\n",
      "Done 3965 batches in 3529.87s\ttraining loss:\t2.424275\n",
      "Done 3970 batches in 3534.09s\ttraining loss:\t2.423601\n",
      "Done 3975 batches in 3538.57s\ttraining loss:\t2.423851\n",
      "Done 3980 batches in 3543.36s\ttraining loss:\t2.424207\n",
      "Done 3985 batches in 3548.33s\ttraining loss:\t2.424486\n",
      "Done 3990 batches in 3552.94s\ttraining loss:\t2.424077\n",
      "Done 3995 batches in 3557.43s\ttraining loss:\t2.423653\n",
      "Done 4000 batches in 3561.83s\ttraining loss:\t2.423966\n",
      "Done 4005 batches in 3566.48s\ttraining loss:\t2.423892\n",
      "Done 4010 batches in 3570.71s\ttraining loss:\t2.423737\n",
      "Done 4015 batches in 3575.28s\ttraining loss:\t2.423826\n",
      "Done 4020 batches in 3579.59s\ttraining loss:\t2.423608\n",
      "Done 4025 batches in 3584.08s\ttraining loss:\t2.423393\n",
      "Done 4030 batches in 3588.83s\ttraining loss:\t2.423304\n",
      "Done 4035 batches in 3593.42s\ttraining loss:\t2.423424\n",
      "Done 4040 batches in 3597.64s\ttraining loss:\t2.424134\n",
      "Done 4045 batches in 3602.24s\ttraining loss:\t2.424065\n",
      "Done 4050 batches in 3606.90s\ttraining loss:\t2.424404\n",
      "Done 4055 batches in 3611.38s\ttraining loss:\t2.424209\n",
      "Done 4060 batches in 3616.28s\ttraining loss:\t2.424843\n",
      "Done 4065 batches in 3621.12s\ttraining loss:\t2.424945\n",
      "Done 4070 batches in 3625.53s\ttraining loss:\t2.425034\n",
      "Done 4075 batches in 3629.71s\ttraining loss:\t2.425154\n",
      "Done 4080 batches in 3634.20s\ttraining loss:\t2.424702\n",
      "Done 4085 batches in 3638.77s\ttraining loss:\t2.424513\n",
      "Done 4090 batches in 3643.23s\ttraining loss:\t2.424410\n",
      "Done 4095 batches in 3647.53s\ttraining loss:\t2.424342\n",
      "Done 4100 batches in 3651.73s\ttraining loss:\t2.424548\n",
      "Done 4105 batches in 3656.36s\ttraining loss:\t2.424007\n",
      "Done 4110 batches in 3660.62s\ttraining loss:\t2.424014\n",
      "Done 4115 batches in 3664.88s\ttraining loss:\t2.424225\n",
      "Done 4120 batches in 3669.21s\ttraining loss:\t2.424306\n",
      "Done 4125 batches in 3673.44s\ttraining loss:\t2.424034\n",
      "Done 4130 batches in 3677.88s\ttraining loss:\t2.424215\n",
      "Done 4135 batches in 3682.11s\ttraining loss:\t2.423672\n",
      "Done 4140 batches in 3686.37s\ttraining loss:\t2.423640\n",
      "Done 4145 batches in 3690.95s\ttraining loss:\t2.423803\n",
      "Done 4150 batches in 3695.37s\ttraining loss:\t2.423848\n",
      "Done 4155 batches in 3699.39s\ttraining loss:\t2.424535\n",
      "Done 4160 batches in 3703.94s\ttraining loss:\t2.424785\n",
      "Done 4165 batches in 3707.91s\ttraining loss:\t2.424777\n",
      "Done 4170 batches in 3712.18s\ttraining loss:\t2.424747\n",
      "Done 4175 batches in 3716.87s\ttraining loss:\t2.424503\n",
      "Done 4180 batches in 3720.83s\ttraining loss:\t2.424495\n",
      "Done 4185 batches in 3724.84s\ttraining loss:\t2.424475\n",
      "Done 4190 batches in 3729.19s\ttraining loss:\t2.424694\n",
      "Done 4195 batches in 3733.49s\ttraining loss:\t2.425352\n",
      "Done 4200 batches in 3737.69s\ttraining loss:\t2.425602\n",
      "Done 4205 batches in 3742.54s\ttraining loss:\t2.425630\n",
      "Done 4210 batches in 3746.21s\ttraining loss:\t2.425255\n",
      "Done 4215 batches in 3750.47s\ttraining loss:\t2.425620\n",
      "Done 4220 batches in 3754.85s\ttraining loss:\t2.425259\n",
      "Done 4225 batches in 3758.70s\ttraining loss:\t2.424893\n",
      "Done 4230 batches in 3763.00s\ttraining loss:\t2.424490\n",
      "Done 4235 batches in 3767.72s\ttraining loss:\t2.424619\n",
      "Done 4240 batches in 3772.05s\ttraining loss:\t2.424379\n",
      "Done 4245 batches in 3776.55s\ttraining loss:\t2.424416\n",
      "Done 4250 batches in 3781.37s\ttraining loss:\t2.424566\n",
      "Done 4255 batches in 3785.67s\ttraining loss:\t2.424143\n",
      "Done 4260 batches in 3790.08s\ttraining loss:\t2.424197\n",
      "Done 4265 batches in 3794.61s\ttraining loss:\t2.423830\n",
      "Done 4270 batches in 3799.26s\ttraining loss:\t2.423677\n",
      "Done 4275 batches in 3803.80s\ttraining loss:\t2.424033\n",
      "Done 4280 batches in 3808.80s\ttraining loss:\t2.424067\n",
      "Done 4285 batches in 3813.36s\ttraining loss:\t2.423821\n",
      "Done 4290 batches in 3817.54s\ttraining loss:\t2.423987\n",
      "Done 4295 batches in 3822.39s\ttraining loss:\t2.424126\n",
      "Done 4300 batches in 3826.85s\ttraining loss:\t2.423676\n",
      "Done 4305 batches in 3831.44s\ttraining loss:\t2.423818\n",
      "Done 4310 batches in 3835.82s\ttraining loss:\t2.424262\n",
      "Done 4315 batches in 3839.84s\ttraining loss:\t2.424342\n",
      "Done 4320 batches in 3844.70s\ttraining loss:\t2.425616\n",
      "Done 4325 batches in 3849.13s\ttraining loss:\t2.426796\n",
      "Done 4330 batches in 3853.58s\ttraining loss:\t2.427580\n",
      "Done 4335 batches in 3857.93s\ttraining loss:\t2.428363\n",
      "Done 4340 batches in 3862.41s\ttraining loss:\t2.428657\n",
      "Done 4345 batches in 3866.43s\ttraining loss:\t2.429314\n",
      "Done 4350 batches in 3871.29s\ttraining loss:\t2.430489\n",
      "Done 4355 batches in 3875.56s\ttraining loss:\t2.430673\n",
      "Done 4360 batches in 3880.33s\ttraining loss:\t2.430887\n",
      "Done 4365 batches in 3884.79s\ttraining loss:\t2.431304\n",
      "Done 4370 batches in 3889.24s\ttraining loss:\t2.431966\n",
      "Done 4375 batches in 3893.81s\ttraining loss:\t2.432753\n",
      "Done 4380 batches in 3898.20s\ttraining loss:\t2.433089\n",
      "Done 4385 batches in 3902.66s\ttraining loss:\t2.433493\n",
      "Done 4390 batches in 3907.16s\ttraining loss:\t2.433651\n",
      "Done 4395 batches in 3911.35s\ttraining loss:\t2.434860\n",
      "Done 4400 batches in 3915.58s\ttraining loss:\t2.435054\n",
      "Done 4405 batches in 3919.89s\ttraining loss:\t2.435332\n",
      "Done 4410 batches in 3924.66s\ttraining loss:\t2.435413\n",
      "Done 4415 batches in 3928.81s\ttraining loss:\t2.435633\n",
      "Done 4420 batches in 3933.17s\ttraining loss:\t2.436368\n",
      "Done 4425 batches in 3937.56s\ttraining loss:\t2.436667\n",
      "Done 4430 batches in 3942.36s\ttraining loss:\t2.437249\n",
      "Done 4435 batches in 3947.06s\ttraining loss:\t2.437589\n",
      "Done 4440 batches in 3952.00s\ttraining loss:\t2.438490\n",
      "Done 4445 batches in 3956.97s\ttraining loss:\t2.438649\n",
      "Done 4450 batches in 3961.45s\ttraining loss:\t2.439280\n",
      "Done 4455 batches in 3965.69s\ttraining loss:\t2.439840\n",
      "Done 4460 batches in 3970.16s\ttraining loss:\t2.440646\n",
      "Done 4465 batches in 3974.82s\ttraining loss:\t2.441332\n",
      "Done 4470 batches in 3979.11s\ttraining loss:\t2.441895\n",
      "Done 4475 batches in 3983.39s\ttraining loss:\t2.441878\n",
      "Done 4480 batches in 3987.69s\ttraining loss:\t2.441738\n",
      "Done 4485 batches in 3992.29s\ttraining loss:\t2.441926\n",
      "Done 4490 batches in 3996.77s\ttraining loss:\t2.442525\n",
      "Done 4495 batches in 4001.25s\ttraining loss:\t2.443173\n",
      "Done 4500 batches in 4005.71s\ttraining loss:\t2.443687\n",
      "Done 4505 batches in 4010.01s\ttraining loss:\t2.443916\n",
      "Done 4510 batches in 4014.91s\ttraining loss:\t2.444101\n",
      "Done 4515 batches in 4019.62s\ttraining loss:\t2.443746\n",
      "Done 4520 batches in 4024.38s\ttraining loss:\t2.443749\n",
      "Done 4525 batches in 4029.27s\ttraining loss:\t2.443549\n",
      "Done 4530 batches in 4033.76s\ttraining loss:\t2.443709\n",
      "Done 4535 batches in 4038.04s\ttraining loss:\t2.443851\n",
      "Done 4540 batches in 4041.95s\ttraining loss:\t2.443934\n",
      "Done 4545 batches in 4046.20s\ttraining loss:\t2.444181\n",
      "Done 4550 batches in 4050.56s\ttraining loss:\t2.444490\n",
      "Done 4555 batches in 4054.55s\ttraining loss:\t2.444679\n",
      "Done 4560 batches in 4059.16s\ttraining loss:\t2.444829\n",
      "Done 4565 batches in 4063.67s\ttraining loss:\t2.444521\n",
      "Done 4570 batches in 4067.71s\ttraining loss:\t2.444845\n",
      "Done 4575 batches in 4072.00s\ttraining loss:\t2.444690\n",
      "Done 4580 batches in 4076.12s\ttraining loss:\t2.444420\n",
      "Done 4585 batches in 4080.48s\ttraining loss:\t2.444281\n",
      "Done 4590 batches in 4085.01s\ttraining loss:\t2.444563\n",
      "Done 4595 batches in 4089.56s\ttraining loss:\t2.444480\n",
      "Done 4600 batches in 4094.06s\ttraining loss:\t2.445230\n",
      "Done 4605 batches in 4098.46s\ttraining loss:\t2.445053\n",
      "Done 4610 batches in 4102.55s\ttraining loss:\t2.445017\n",
      "Done 4615 batches in 4107.36s\ttraining loss:\t2.445420\n",
      "Done 4620 batches in 4111.50s\ttraining loss:\t2.445356\n",
      "Done 4625 batches in 4115.80s\ttraining loss:\t2.445760\n",
      "Done 4630 batches in 4119.91s\ttraining loss:\t2.446145\n",
      "Done 4635 batches in 4124.67s\ttraining loss:\t2.446071\n",
      "Done 4640 batches in 4129.29s\ttraining loss:\t2.446391\n",
      "Done 4645 batches in 4133.59s\ttraining loss:\t2.446234\n",
      "Done 4650 batches in 4137.85s\ttraining loss:\t2.446272\n",
      "Done 4655 batches in 4142.28s\ttraining loss:\t2.446537\n",
      "Done 4660 batches in 4146.78s\ttraining loss:\t2.446435\n",
      "Done 4665 batches in 4151.20s\ttraining loss:\t2.446426\n",
      "Done 4670 batches in 4155.59s\ttraining loss:\t2.446354\n",
      "Done 4675 batches in 4160.13s\ttraining loss:\t2.446493\n",
      "Done 4680 batches in 4164.68s\ttraining loss:\t2.446500\n",
      "Done 4685 batches in 4168.73s\ttraining loss:\t2.446508\n",
      "Done 4690 batches in 4172.88s\ttraining loss:\t2.446649\n",
      "Done 4695 batches in 4177.33s\ttraining loss:\t2.446564\n",
      "Done 4700 batches in 4181.38s\ttraining loss:\t2.446656\n",
      "Done 4705 batches in 4185.41s\ttraining loss:\t2.446186\n",
      "Done 4710 batches in 4190.12s\ttraining loss:\t2.446640\n",
      "Done 4715 batches in 4194.46s\ttraining loss:\t2.446640\n",
      "Done 4720 batches in 4198.89s\ttraining loss:\t2.446939\n",
      "Done 4725 batches in 4203.13s\ttraining loss:\t2.447005\n",
      "Done 4730 batches in 4207.27s\ttraining loss:\t2.447599\n",
      "Done 4735 batches in 4212.09s\ttraining loss:\t2.447670\n",
      "Done 4740 batches in 4216.18s\ttraining loss:\t2.447776\n",
      "Done 4745 batches in 4220.22s\ttraining loss:\t2.447272\n",
      "Done 4750 batches in 4224.15s\ttraining loss:\t2.448002\n",
      "Done 4755 batches in 4228.42s\ttraining loss:\t2.447793\n",
      "Done 4760 batches in 4232.69s\ttraining loss:\t2.447750\n",
      "Done 4765 batches in 4237.52s\ttraining loss:\t2.447718\n",
      "Done 4770 batches in 4241.57s\ttraining loss:\t2.447202\n",
      "Done 4775 batches in 4246.00s\ttraining loss:\t2.447011\n",
      "Done 4780 batches in 4250.25s\ttraining loss:\t2.446825\n",
      "Done 4785 batches in 4254.91s\ttraining loss:\t2.446859\n",
      "Done 4790 batches in 4259.45s\ttraining loss:\t2.447027\n",
      "Done 4795 batches in 4264.73s\ttraining loss:\t2.447413\n",
      "Done 4800 batches in 4269.20s\ttraining loss:\t2.447748\n",
      "Done 4805 batches in 4273.75s\ttraining loss:\t2.447600\n",
      "Done 4810 batches in 4277.83s\ttraining loss:\t2.447791\n",
      "Done 4815 batches in 4282.24s\ttraining loss:\t2.447883\n",
      "Done 4820 batches in 4286.51s\ttraining loss:\t2.447810\n",
      "Done 4825 batches in 4291.22s\ttraining loss:\t2.447602\n",
      "Done 4830 batches in 4295.18s\ttraining loss:\t2.447842\n",
      "Done 4835 batches in 4299.38s\ttraining loss:\t2.448036\n",
      "Done 4840 batches in 4304.04s\ttraining loss:\t2.448381\n",
      "Done 4845 batches in 4308.56s\ttraining loss:\t2.448960\n",
      "Done 4850 batches in 4313.12s\ttraining loss:\t2.449202\n",
      "Done 4855 batches in 4317.54s\ttraining loss:\t2.449320\n",
      "Done 4860 batches in 4322.17s\ttraining loss:\t2.449276\n",
      "Done 4865 batches in 4326.28s\ttraining loss:\t2.449555\n",
      "Done 4870 batches in 4331.04s\ttraining loss:\t2.449528\n",
      "Done 4875 batches in 4335.47s\ttraining loss:\t2.449854\n",
      "Done 4880 batches in 4339.92s\ttraining loss:\t2.449772\n",
      "Done 4885 batches in 4344.37s\ttraining loss:\t2.449864\n",
      "Done 4890 batches in 4349.40s\ttraining loss:\t2.450212\n",
      "Done 4895 batches in 4354.03s\ttraining loss:\t2.449905\n",
      "Done 4900 batches in 4358.19s\ttraining loss:\t2.449842\n",
      "Done 4905 batches in 4362.22s\ttraining loss:\t2.449888\n",
      "Done 4910 batches in 4366.98s\ttraining loss:\t2.449928\n",
      "Done 4915 batches in 4371.44s\ttraining loss:\t2.449755\n",
      "Done 4920 batches in 4375.59s\ttraining loss:\t2.450085\n",
      "Done 4925 batches in 4379.62s\ttraining loss:\t2.450232\n",
      "Done 4930 batches in 4384.07s\ttraining loss:\t2.450184\n",
      "Done 4935 batches in 4388.73s\ttraining loss:\t2.450432\n",
      "Done 4940 batches in 4393.33s\ttraining loss:\t2.450789\n",
      "Done 4945 batches in 4398.18s\ttraining loss:\t2.450679\n",
      "Done 4950 batches in 4402.45s\ttraining loss:\t2.450784\n",
      "Done 4955 batches in 4407.26s\ttraining loss:\t2.450869\n",
      "Done 4960 batches in 4411.97s\ttraining loss:\t2.451216\n",
      "Done 4965 batches in 4416.76s\ttraining loss:\t2.450998\n",
      "Done 4970 batches in 4421.75s\ttraining loss:\t2.451525\n",
      "Done 4975 batches in 4426.30s\ttraining loss:\t2.451744\n",
      "Done 4980 batches in 4430.71s\ttraining loss:\t2.451420\n",
      "Done 4985 batches in 4434.99s\ttraining loss:\t2.451184\n",
      "Done 4990 batches in 4439.21s\ttraining loss:\t2.451431\n",
      "Done 4995 batches in 4443.92s\ttraining loss:\t2.450946\n",
      "Done 5000 batches in 4448.53s\ttraining loss:\t2.451099\n",
      "Done 5005 batches in 4453.22s\ttraining loss:\t2.450896\n",
      "Done 5010 batches in 4457.17s\ttraining loss:\t2.451118\n",
      "Done 5015 batches in 4461.52s\ttraining loss:\t2.451402\n",
      "Done 5020 batches in 4466.02s\ttraining loss:\t2.451174\n",
      "Done 5025 batches in 4470.66s\ttraining loss:\t2.451466\n",
      "Done 5030 batches in 4475.55s\ttraining loss:\t2.451314\n",
      "Done 5035 batches in 4480.20s\ttraining loss:\t2.451428\n",
      "Done 5040 batches in 4484.85s\ttraining loss:\t2.451598\n",
      "Done 5045 batches in 4489.44s\ttraining loss:\t2.451762\n",
      "Done 5050 batches in 4493.70s\ttraining loss:\t2.451616\n",
      "Done 5055 batches in 4498.13s\ttraining loss:\t2.451752\n",
      "Done 5060 batches in 4502.10s\ttraining loss:\t2.451528\n",
      "Done 5065 batches in 4506.35s\ttraining loss:\t2.451517\n",
      "Done 5070 batches in 4510.23s\ttraining loss:\t2.451800\n",
      "Done 5075 batches in 4514.39s\ttraining loss:\t2.452365\n",
      "Done 5080 batches in 4519.01s\ttraining loss:\t2.452413\n",
      "Done 5085 batches in 4523.39s\ttraining loss:\t2.452151\n",
      "Done 5090 batches in 4527.78s\ttraining loss:\t2.452351\n",
      "Done 5095 batches in 4531.93s\ttraining loss:\t2.452341\n",
      "Done 5100 batches in 4536.14s\ttraining loss:\t2.452233\n",
      "Done 5105 batches in 4540.47s\ttraining loss:\t2.452031\n",
      "Done 5110 batches in 4545.23s\ttraining loss:\t2.452193\n",
      "Done 5115 batches in 4549.78s\ttraining loss:\t2.452145\n",
      "Done 5120 batches in 4554.06s\ttraining loss:\t2.451914\n",
      "Done 5125 batches in 4558.47s\ttraining loss:\t2.451776\n",
      "Done 5130 batches in 4562.50s\ttraining loss:\t2.451926\n",
      "Done 5135 batches in 4567.14s\ttraining loss:\t2.451800\n",
      "Done 5140 batches in 4571.85s\ttraining loss:\t2.452440\n",
      "Done 5145 batches in 4576.04s\ttraining loss:\t2.452496\n",
      "Done 5150 batches in 4580.22s\ttraining loss:\t2.451892\n",
      "Done 5155 batches in 4584.69s\ttraining loss:\t2.451993\n",
      "Done 5160 batches in 4588.70s\ttraining loss:\t2.451389\n",
      "Done 5165 batches in 4593.16s\ttraining loss:\t2.451391\n",
      "Done 5170 batches in 4597.77s\ttraining loss:\t2.451389\n",
      "Done 5175 batches in 4602.08s\ttraining loss:\t2.451273\n",
      "Done 5180 batches in 4606.45s\ttraining loss:\t2.451794\n",
      "Done 5185 batches in 4610.90s\ttraining loss:\t2.452140\n",
      "Done 5190 batches in 4615.49s\ttraining loss:\t2.452218\n",
      "Done 5195 batches in 4620.32s\ttraining loss:\t2.452154\n",
      "Done 5200 batches in 4625.38s\ttraining loss:\t2.452213\n",
      "Done 5205 batches in 4629.94s\ttraining loss:\t2.452083\n",
      "Done 5210 batches in 4634.36s\ttraining loss:\t2.452414\n",
      "Done 5215 batches in 4638.99s\ttraining loss:\t2.452545\n",
      "Done 5220 batches in 4643.60s\ttraining loss:\t2.452767\n",
      "Done 5225 batches in 4648.31s\ttraining loss:\t2.452817\n",
      "Done 5230 batches in 4652.61s\ttraining loss:\t2.452585\n",
      "Done 5235 batches in 4657.31s\ttraining loss:\t2.452984\n",
      "Done 5240 batches in 4661.98s\ttraining loss:\t2.452731\n",
      "Done 5245 batches in 4666.88s\ttraining loss:\t2.452524\n",
      "Done 5250 batches in 4671.65s\ttraining loss:\t2.452632\n",
      "Done 5255 batches in 4676.01s\ttraining loss:\t2.452663\n",
      "Done 5260 batches in 4680.77s\ttraining loss:\t2.453011\n",
      "Done 5265 batches in 4684.91s\ttraining loss:\t2.453003\n",
      "Done 5270 batches in 4689.42s\ttraining loss:\t2.453180\n",
      "Done 5275 batches in 4693.62s\ttraining loss:\t2.453060\n",
      "Done 5280 batches in 4697.50s\ttraining loss:\t2.453006\n",
      "Done 5285 batches in 4702.09s\ttraining loss:\t2.452820\n",
      "Done 5290 batches in 4706.67s\ttraining loss:\t2.452555\n",
      "Done 5295 batches in 4711.20s\ttraining loss:\t2.452653\n",
      "Done 5300 batches in 4715.70s\ttraining loss:\t2.452555\n",
      "Done 5305 batches in 4720.18s\ttraining loss:\t2.452730\n",
      "Done 5310 batches in 4724.33s\ttraining loss:\t2.452866\n",
      "Done 5315 batches in 4728.56s\ttraining loss:\t2.453026\n",
      "Done 5320 batches in 4732.76s\ttraining loss:\t2.453196\n",
      "Done 5325 batches in 4736.80s\ttraining loss:\t2.452980\n",
      "Done 5330 batches in 4741.51s\ttraining loss:\t2.452951\n",
      "Done 5335 batches in 4746.16s\ttraining loss:\t2.453036\n",
      "Done 5340 batches in 4750.49s\ttraining loss:\t2.453121\n",
      "Done 5345 batches in 4754.57s\ttraining loss:\t2.452912\n",
      "Done 5350 batches in 4758.89s\ttraining loss:\t2.453101\n",
      "Done 5355 batches in 4763.84s\ttraining loss:\t2.453421\n",
      "Done 5360 batches in 4768.60s\ttraining loss:\t2.453588\n",
      "Done 5365 batches in 4773.39s\ttraining loss:\t2.453606\n",
      "Done 5370 batches in 4777.96s\ttraining loss:\t2.453444\n",
      "Done 5375 batches in 4782.01s\ttraining loss:\t2.453517\n",
      "Done 5380 batches in 4785.97s\ttraining loss:\t2.453485\n",
      "Done 5385 batches in 4790.70s\ttraining loss:\t2.453733\n",
      "Done 5390 batches in 4795.20s\ttraining loss:\t2.453566\n",
      "Done 5395 batches in 4799.54s\ttraining loss:\t2.453708\n",
      "Done 5400 batches in 4803.98s\ttraining loss:\t2.453827\n",
      "Done 5405 batches in 4808.68s\ttraining loss:\t2.453953\n",
      "Done 5410 batches in 4813.49s\ttraining loss:\t2.453574\n",
      "Done 5415 batches in 4817.91s\ttraining loss:\t2.453778\n",
      "Done 5420 batches in 4822.73s\ttraining loss:\t2.453421\n",
      "Done 5425 batches in 4826.92s\ttraining loss:\t2.453624\n",
      "Done 5430 batches in 4831.30s\ttraining loss:\t2.453581\n",
      "Done 5435 batches in 4835.93s\ttraining loss:\t2.453943\n",
      "Done 5440 batches in 4840.44s\ttraining loss:\t2.453740\n",
      "Done 5445 batches in 4844.53s\ttraining loss:\t2.453530\n",
      "Done 5450 batches in 4849.29s\ttraining loss:\t2.453462\n",
      "Done 5455 batches in 4853.86s\ttraining loss:\t2.453591\n",
      "Done 5460 batches in 4857.86s\ttraining loss:\t2.453713\n",
      "Done 5465 batches in 4862.16s\ttraining loss:\t2.453839\n",
      "Done 5470 batches in 4866.44s\ttraining loss:\t2.453885\n",
      "Done 5475 batches in 4870.78s\ttraining loss:\t2.453715\n",
      "Done 5480 batches in 4875.02s\ttraining loss:\t2.453867\n",
      "Done 5485 batches in 4879.76s\ttraining loss:\t2.453966\n",
      "Done 5490 batches in 4883.71s\ttraining loss:\t2.454346\n",
      "Done 5495 batches in 4888.04s\ttraining loss:\t2.454939\n",
      "Done 5500 batches in 4892.53s\ttraining loss:\t2.455161\n",
      "Done 5505 batches in 4897.24s\ttraining loss:\t2.455334\n",
      "Done 5510 batches in 4901.66s\ttraining loss:\t2.455312\n",
      "Done 5515 batches in 4905.91s\ttraining loss:\t2.455114\n",
      "Done 5520 batches in 4910.18s\ttraining loss:\t2.455151\n",
      "Done 5525 batches in 4914.35s\ttraining loss:\t2.455054\n",
      "Done 5530 batches in 4918.56s\ttraining loss:\t2.454712\n",
      "Done 5535 batches in 4922.97s\ttraining loss:\t2.454922\n",
      "Done 5540 batches in 4927.22s\ttraining loss:\t2.454642\n",
      "Done 5545 batches in 4931.98s\ttraining loss:\t2.454531\n",
      "Done 5550 batches in 4936.25s\ttraining loss:\t2.454672\n",
      "Done 5555 batches in 4941.02s\ttraining loss:\t2.454474\n",
      "Done 5560 batches in 4946.02s\ttraining loss:\t2.454756\n",
      "Done 5565 batches in 4950.53s\ttraining loss:\t2.454796\n",
      "Done 5570 batches in 4955.06s\ttraining loss:\t2.454568\n",
      "Done 5575 batches in 4959.39s\ttraining loss:\t2.454038\n",
      "Done 5580 batches in 4963.34s\ttraining loss:\t2.454333\n",
      "Done 5585 batches in 4966.90s\ttraining loss:\t2.454861\n",
      "Done 5590 batches in 4970.98s\ttraining loss:\t2.455206\n",
      "Done 5595 batches in 4975.37s\ttraining loss:\t2.455270\n",
      "Done 5600 batches in 4979.39s\ttraining loss:\t2.455248\n",
      "Done 5605 batches in 4984.31s\ttraining loss:\t2.455212\n",
      "Done 5610 batches in 4988.98s\ttraining loss:\t2.455235\n",
      "Done 5615 batches in 4992.80s\ttraining loss:\t2.454915\n",
      "Done 5620 batches in 4997.64s\ttraining loss:\t2.454905\n",
      "Done 5625 batches in 5001.82s\ttraining loss:\t2.454882\n",
      "Done 5630 batches in 5006.39s\ttraining loss:\t2.454465\n",
      "Done 5635 batches in 5010.86s\ttraining loss:\t2.454525\n",
      "Done 5640 batches in 5015.21s\ttraining loss:\t2.454424\n",
      "Done 5645 batches in 5019.73s\ttraining loss:\t2.454193\n",
      "Done 5650 batches in 5024.18s\ttraining loss:\t2.453938\n",
      "Done 5655 batches in 5028.57s\ttraining loss:\t2.453825\n",
      "Done 5660 batches in 5032.89s\ttraining loss:\t2.454058\n",
      "Done 5665 batches in 5037.01s\ttraining loss:\t2.454342\n",
      "Done 5670 batches in 5041.69s\ttraining loss:\t2.454728\n",
      "Done 5675 batches in 5046.34s\ttraining loss:\t2.454792\n",
      "Done 5680 batches in 5050.90s\ttraining loss:\t2.454784\n",
      "Done 5685 batches in 5055.59s\ttraining loss:\t2.454755\n",
      "Done 5690 batches in 5059.86s\ttraining loss:\t2.454725\n",
      "Done 5695 batches in 5064.48s\ttraining loss:\t2.454512\n",
      "Done 5700 batches in 5069.06s\ttraining loss:\t2.454403\n",
      "Done 5705 batches in 5073.30s\ttraining loss:\t2.454137\n",
      "Done 5710 batches in 5077.57s\ttraining loss:\t2.454293\n",
      "Done 5715 batches in 5082.06s\ttraining loss:\t2.453974\n",
      "Done 5720 batches in 5086.59s\ttraining loss:\t2.453762\n",
      "Done 5725 batches in 5091.01s\ttraining loss:\t2.453913\n",
      "Done 5730 batches in 5095.89s\ttraining loss:\t2.453947\n",
      "Done 5735 batches in 5100.40s\ttraining loss:\t2.453684\n",
      "Done 5740 batches in 5104.35s\ttraining loss:\t2.453628\n",
      "Done 5745 batches in 5108.78s\ttraining loss:\t2.453354\n",
      "Done 5750 batches in 5113.57s\ttraining loss:\t2.453228\n",
      "Done 5755 batches in 5117.72s\ttraining loss:\t2.453590\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.453853207685309"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z char-embeddings\n",
    "\n",
    "# 2 ep\n",
    "\n",
    "# dane s przycite do dugoci 300 (jeli odpowied si nie mieci, to pytanie jest usuwane z danych)\n",
    "# przycito okoo 1400 prbek, usunito 119\n",
    "\n",
    "# gdzie si da: glove init, reszta: losowo, trenujemy wszystkie sowa, char-embeddings ma losowy init\n",
    "\n",
    "qa_net.train_one_epoch(data, 15, log_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qa_net.save_params('trained_models/test_params_char_emb_fixed_2ep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# qa_net.load_params('test_params_3ep.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 5 batches in 4.76s\ttraining loss:\t1.967765\n",
      "Done 10 batches in 8.98s\ttraining loss:\t1.789092\n",
      "Done 15 batches in 13.79s\ttraining loss:\t1.879856\n",
      "Done 20 batches in 18.41s\ttraining loss:\t1.920875\n",
      "Done 25 batches in 23.06s\ttraining loss:\t1.824911\n",
      "Done 30 batches in 27.20s\ttraining loss:\t1.797667\n",
      "Done 35 batches in 31.60s\ttraining loss:\t1.752268\n",
      "Done 40 batches in 36.46s\ttraining loss:\t1.734237\n",
      "Done 45 batches in 40.70s\ttraining loss:\t1.730883\n",
      "Done 50 batches in 45.06s\ttraining loss:\t1.747018\n",
      "Done 55 batches in 49.39s\ttraining loss:\t1.723154\n",
      "Done 60 batches in 54.32s\ttraining loss:\t1.760780\n",
      "Done 65 batches in 58.55s\ttraining loss:\t1.734702\n",
      "Done 70 batches in 63.20s\ttraining loss:\t1.748750\n",
      "Done 75 batches in 67.94s\ttraining loss:\t1.766842\n",
      "Done 80 batches in 72.43s\ttraining loss:\t1.775458\n",
      "Done 85 batches in 76.76s\ttraining loss:\t1.769615\n",
      "Done 90 batches in 81.45s\ttraining loss:\t1.764126\n",
      "Done 95 batches in 86.00s\ttraining loss:\t1.752024\n",
      "Done 100 batches in 90.34s\ttraining loss:\t1.752533\n",
      "Done 105 batches in 94.78s\ttraining loss:\t1.769961\n",
      "Done 110 batches in 98.77s\ttraining loss:\t1.769287\n",
      "Done 115 batches in 103.22s\ttraining loss:\t1.761479\n",
      "Done 120 batches in 107.34s\ttraining loss:\t1.759563\n",
      "Done 125 batches in 111.76s\ttraining loss:\t1.764792\n",
      "Done 130 batches in 115.55s\ttraining loss:\t1.765735\n",
      "Done 135 batches in 120.18s\ttraining loss:\t1.758320\n",
      "Done 140 batches in 124.77s\ttraining loss:\t1.755632\n",
      "Done 145 batches in 128.74s\ttraining loss:\t1.749385\n",
      "Done 150 batches in 133.50s\ttraining loss:\t1.745079\n",
      "Done 155 batches in 138.09s\ttraining loss:\t1.738551\n",
      "Done 160 batches in 142.11s\ttraining loss:\t1.741104\n",
      "Done 165 batches in 146.84s\ttraining loss:\t1.741457\n",
      "Done 170 batches in 151.80s\ttraining loss:\t1.736152\n",
      "Done 175 batches in 156.43s\ttraining loss:\t1.732464\n",
      "Done 180 batches in 160.84s\ttraining loss:\t1.723689\n",
      "Done 185 batches in 165.32s\ttraining loss:\t1.719587\n",
      "Done 190 batches in 170.26s\ttraining loss:\t1.714234\n",
      "Done 195 batches in 174.77s\ttraining loss:\t1.714978\n",
      "Done 200 batches in 179.66s\ttraining loss:\t1.718232\n",
      "Done 205 batches in 184.32s\ttraining loss:\t1.717065\n",
      "Done 210 batches in 188.71s\ttraining loss:\t1.723329\n",
      "Done 215 batches in 193.01s\ttraining loss:\t1.721156\n",
      "Done 220 batches in 197.56s\ttraining loss:\t1.714164\n",
      "Done 225 batches in 202.03s\ttraining loss:\t1.711701\n",
      "Done 230 batches in 206.02s\ttraining loss:\t1.713110\n",
      "Done 235 batches in 210.52s\ttraining loss:\t1.712111\n",
      "Done 240 batches in 214.76s\ttraining loss:\t1.716560\n",
      "Done 245 batches in 218.96s\ttraining loss:\t1.715776\n",
      "Done 250 batches in 223.70s\ttraining loss:\t1.710788\n",
      "Done 255 batches in 227.69s\ttraining loss:\t1.711937\n",
      "Done 260 batches in 232.19s\ttraining loss:\t1.708901\n",
      "Done 265 batches in 236.82s\ttraining loss:\t1.710668\n",
      "Done 270 batches in 241.40s\ttraining loss:\t1.707035\n",
      "Done 275 batches in 245.71s\ttraining loss:\t1.706508\n",
      "Done 280 batches in 250.21s\ttraining loss:\t1.705924\n",
      "Done 285 batches in 254.81s\ttraining loss:\t1.710616\n",
      "Done 290 batches in 259.38s\ttraining loss:\t1.714524\n",
      "Done 295 batches in 263.45s\ttraining loss:\t1.722109\n",
      "Done 300 batches in 267.36s\ttraining loss:\t1.720784\n",
      "Done 305 batches in 271.60s\ttraining loss:\t1.720311\n",
      "Done 310 batches in 275.76s\ttraining loss:\t1.725920\n",
      "Done 315 batches in 280.56s\ttraining loss:\t1.720722\n",
      "Done 320 batches in 285.42s\ttraining loss:\t1.720995\n",
      "Done 325 batches in 289.96s\ttraining loss:\t1.723306\n",
      "Done 330 batches in 294.13s\ttraining loss:\t1.720341\n",
      "Done 335 batches in 298.68s\ttraining loss:\t1.721113\n",
      "Done 340 batches in 303.32s\ttraining loss:\t1.722614\n",
      "Done 345 batches in 307.59s\ttraining loss:\t1.718727\n",
      "Done 350 batches in 311.96s\ttraining loss:\t1.722965\n",
      "Done 355 batches in 316.22s\ttraining loss:\t1.725708\n",
      "Done 360 batches in 320.81s\ttraining loss:\t1.726050\n",
      "Done 365 batches in 324.84s\ttraining loss:\t1.726352\n",
      "Done 370 batches in 329.79s\ttraining loss:\t1.722212\n",
      "Done 375 batches in 333.92s\ttraining loss:\t1.728034\n",
      "Done 380 batches in 338.61s\ttraining loss:\t1.728373\n",
      "Done 385 batches in 342.75s\ttraining loss:\t1.729997\n",
      "Done 390 batches in 347.47s\ttraining loss:\t1.730323\n",
      "Done 395 batches in 351.95s\ttraining loss:\t1.730961\n",
      "Done 400 batches in 356.65s\ttraining loss:\t1.727324\n",
      "Done 405 batches in 360.99s\ttraining loss:\t1.722666\n",
      "Done 410 batches in 365.16s\ttraining loss:\t1.719700\n",
      "Done 415 batches in 369.39s\ttraining loss:\t1.717682\n",
      "Done 420 batches in 374.15s\ttraining loss:\t1.716151\n",
      "Done 425 batches in 378.49s\ttraining loss:\t1.716264\n",
      "Done 430 batches in 382.75s\ttraining loss:\t1.715151\n",
      "Done 435 batches in 387.17s\ttraining loss:\t1.716615\n",
      "Done 440 batches in 392.14s\ttraining loss:\t1.714405\n",
      "Done 445 batches in 396.95s\ttraining loss:\t1.713541\n",
      "Done 450 batches in 401.44s\ttraining loss:\t1.715311\n",
      "Done 455 batches in 405.94s\ttraining loss:\t1.718552\n",
      "Done 460 batches in 410.15s\ttraining loss:\t1.723526\n",
      "Done 465 batches in 414.71s\ttraining loss:\t1.726372\n",
      "Done 470 batches in 419.24s\ttraining loss:\t1.725402\n",
      "Done 475 batches in 423.91s\ttraining loss:\t1.725768\n",
      "Done 480 batches in 428.52s\ttraining loss:\t1.724895\n",
      "Done 485 batches in 432.98s\ttraining loss:\t1.728284\n",
      "Done 490 batches in 437.23s\ttraining loss:\t1.727427\n",
      "Done 495 batches in 441.57s\ttraining loss:\t1.726425\n",
      "Done 500 batches in 445.88s\ttraining loss:\t1.725672\n",
      "Done 505 batches in 450.60s\ttraining loss:\t1.722431\n",
      "Done 510 batches in 454.57s\ttraining loss:\t1.726509\n",
      "Done 515 batches in 458.65s\ttraining loss:\t1.726215\n",
      "Done 520 batches in 463.02s\ttraining loss:\t1.725934\n",
      "Done 525 batches in 467.87s\ttraining loss:\t1.727131\n",
      "Done 530 batches in 472.27s\ttraining loss:\t1.727367\n",
      "Done 535 batches in 477.15s\ttraining loss:\t1.726956\n",
      "Done 540 batches in 481.79s\ttraining loss:\t1.727062\n",
      "Done 545 batches in 486.19s\ttraining loss:\t1.725287\n",
      "Done 550 batches in 490.24s\ttraining loss:\t1.726352\n",
      "Done 555 batches in 494.57s\ttraining loss:\t1.726535\n",
      "Done 560 batches in 499.24s\ttraining loss:\t1.727005\n",
      "Done 565 batches in 504.32s\ttraining loss:\t1.727059\n",
      "Done 570 batches in 508.56s\ttraining loss:\t1.726710\n",
      "Done 575 batches in 512.96s\ttraining loss:\t1.727500\n",
      "Done 580 batches in 517.34s\ttraining loss:\t1.728138\n",
      "Done 585 batches in 522.17s\ttraining loss:\t1.725767\n",
      "Done 590 batches in 526.80s\ttraining loss:\t1.727845\n",
      "Done 595 batches in 530.94s\ttraining loss:\t1.729064\n",
      "Done 600 batches in 535.64s\ttraining loss:\t1.731631\n",
      "Done 605 batches in 540.63s\ttraining loss:\t1.729653\n",
      "Done 610 batches in 544.96s\ttraining loss:\t1.729303\n",
      "Done 615 batches in 548.96s\ttraining loss:\t1.729142\n",
      "Done 620 batches in 553.61s\ttraining loss:\t1.731965\n",
      "Done 625 batches in 558.29s\ttraining loss:\t1.733735\n",
      "Done 630 batches in 562.67s\ttraining loss:\t1.731854\n",
      "Done 635 batches in 567.24s\ttraining loss:\t1.729403\n",
      "Done 640 batches in 572.11s\ttraining loss:\t1.727217\n",
      "Done 645 batches in 576.68s\ttraining loss:\t1.729310\n",
      "Done 650 batches in 580.77s\ttraining loss:\t1.728670\n",
      "Done 655 batches in 585.28s\ttraining loss:\t1.726502\n",
      "Done 660 batches in 589.74s\ttraining loss:\t1.727815\n",
      "Done 665 batches in 594.11s\ttraining loss:\t1.726662\n",
      "Done 670 batches in 598.65s\ttraining loss:\t1.730379\n",
      "Done 675 batches in 603.16s\ttraining loss:\t1.732340\n",
      "Done 680 batches in 607.81s\ttraining loss:\t1.731738\n",
      "Done 685 batches in 612.13s\ttraining loss:\t1.734270\n",
      "Done 690 batches in 616.70s\ttraining loss:\t1.733806\n",
      "Done 695 batches in 621.23s\ttraining loss:\t1.732149\n",
      "Done 700 batches in 625.66s\ttraining loss:\t1.730960\n",
      "Done 705 batches in 630.33s\ttraining loss:\t1.731935\n",
      "Done 710 batches in 634.89s\ttraining loss:\t1.731700\n",
      "Done 715 batches in 639.08s\ttraining loss:\t1.731211\n",
      "Done 720 batches in 643.73s\ttraining loss:\t1.731647\n",
      "Done 725 batches in 648.33s\ttraining loss:\t1.729446\n",
      "Done 730 batches in 652.76s\ttraining loss:\t1.729381\n",
      "Done 735 batches in 657.32s\ttraining loss:\t1.731227\n",
      "Done 740 batches in 661.82s\ttraining loss:\t1.731044\n",
      "Done 745 batches in 666.11s\ttraining loss:\t1.731023\n",
      "Done 750 batches in 670.51s\ttraining loss:\t1.728019\n",
      "Done 755 batches in 674.94s\ttraining loss:\t1.727922\n",
      "Done 760 batches in 679.66s\ttraining loss:\t1.726613\n",
      "Done 765 batches in 684.08s\ttraining loss:\t1.727852\n",
      "Done 770 batches in 688.63s\ttraining loss:\t1.728221\n",
      "Done 775 batches in 693.10s\ttraining loss:\t1.726207\n",
      "Done 780 batches in 697.69s\ttraining loss:\t1.724138\n",
      "Done 785 batches in 702.09s\ttraining loss:\t1.724108\n",
      "Done 790 batches in 706.60s\ttraining loss:\t1.725047\n",
      "Done 795 batches in 710.91s\ttraining loss:\t1.724843\n",
      "Done 800 batches in 715.47s\ttraining loss:\t1.725263\n",
      "Done 805 batches in 720.03s\ttraining loss:\t1.727778\n",
      "Done 810 batches in 723.95s\ttraining loss:\t1.725479\n",
      "Done 815 batches in 728.66s\ttraining loss:\t1.725066\n",
      "Done 820 batches in 733.46s\ttraining loss:\t1.726201\n",
      "Done 825 batches in 737.84s\ttraining loss:\t1.724762\n",
      "Done 830 batches in 742.66s\ttraining loss:\t1.723769\n",
      "Done 835 batches in 747.33s\ttraining loss:\t1.723815\n",
      "Done 840 batches in 752.02s\ttraining loss:\t1.721763\n",
      "Done 845 batches in 756.53s\ttraining loss:\t1.721629\n",
      "Done 850 batches in 761.00s\ttraining loss:\t1.721641\n",
      "Done 855 batches in 765.14s\ttraining loss:\t1.720695\n",
      "Done 860 batches in 769.04s\ttraining loss:\t1.718760\n",
      "Done 865 batches in 773.78s\ttraining loss:\t1.719453\n",
      "Done 870 batches in 778.58s\ttraining loss:\t1.721431\n",
      "Done 875 batches in 783.23s\ttraining loss:\t1.722166\n",
      "Done 880 batches in 787.67s\ttraining loss:\t1.721730\n",
      "Done 885 batches in 792.10s\ttraining loss:\t1.721937\n",
      "Done 890 batches in 796.77s\ttraining loss:\t1.722834\n",
      "Done 895 batches in 801.13s\ttraining loss:\t1.725045\n",
      "Done 900 batches in 805.62s\ttraining loss:\t1.724471\n",
      "Done 905 batches in 809.87s\ttraining loss:\t1.726921\n",
      "Done 910 batches in 814.05s\ttraining loss:\t1.726564\n",
      "Done 915 batches in 818.71s\ttraining loss:\t1.726151\n",
      "Done 920 batches in 823.39s\ttraining loss:\t1.728257\n",
      "Done 925 batches in 827.83s\ttraining loss:\t1.727404\n",
      "Done 930 batches in 831.99s\ttraining loss:\t1.727362\n",
      "Done 935 batches in 836.48s\ttraining loss:\t1.727023\n",
      "Done 940 batches in 840.95s\ttraining loss:\t1.726704\n",
      "Done 945 batches in 845.60s\ttraining loss:\t1.727216\n",
      "Done 950 batches in 850.17s\ttraining loss:\t1.727574\n",
      "Done 955 batches in 854.38s\ttraining loss:\t1.727476\n",
      "Done 960 batches in 859.24s\ttraining loss:\t1.727465\n",
      "Done 965 batches in 863.74s\ttraining loss:\t1.727558\n",
      "Done 970 batches in 868.12s\ttraining loss:\t1.726824\n",
      "Done 975 batches in 872.46s\ttraining loss:\t1.725662\n",
      "Done 980 batches in 877.25s\ttraining loss:\t1.725922\n",
      "Done 985 batches in 881.91s\ttraining loss:\t1.726372\n",
      "Done 990 batches in 886.32s\ttraining loss:\t1.727020\n",
      "Done 995 batches in 891.29s\ttraining loss:\t1.726730\n",
      "Done 1000 batches in 895.62s\ttraining loss:\t1.727024\n",
      "Done 1005 batches in 899.74s\ttraining loss:\t1.726167\n",
      "Done 1010 batches in 903.95s\ttraining loss:\t1.724752\n",
      "Done 1015 batches in 908.62s\ttraining loss:\t1.723761\n",
      "Done 1020 batches in 913.55s\ttraining loss:\t1.723274\n",
      "Done 1025 batches in 917.95s\ttraining loss:\t1.722476\n",
      "Done 1030 batches in 922.50s\ttraining loss:\t1.722413\n",
      "Done 1035 batches in 926.97s\ttraining loss:\t1.721750\n",
      "Done 1040 batches in 931.63s\ttraining loss:\t1.723303\n",
      "Done 1045 batches in 936.02s\ttraining loss:\t1.723468\n",
      "Done 1050 batches in 940.69s\ttraining loss:\t1.722162\n",
      "Done 1055 batches in 944.84s\ttraining loss:\t1.720979\n",
      "Done 1060 batches in 949.47s\ttraining loss:\t1.720030\n",
      "Done 1065 batches in 953.76s\ttraining loss:\t1.719654\n",
      "Done 1070 batches in 958.69s\ttraining loss:\t1.719527\n",
      "Done 1075 batches in 963.08s\ttraining loss:\t1.718478\n",
      "Done 1080 batches in 967.51s\ttraining loss:\t1.717871\n",
      "Done 1085 batches in 971.76s\ttraining loss:\t1.717923\n",
      "Done 1090 batches in 976.24s\ttraining loss:\t1.718701\n",
      "Done 1095 batches in 980.70s\ttraining loss:\t1.719176\n",
      "Done 1100 batches in 985.34s\ttraining loss:\t1.718960\n",
      "Done 1105 batches in 989.86s\ttraining loss:\t1.717917\n",
      "Done 1110 batches in 994.34s\ttraining loss:\t1.717687\n",
      "Done 1115 batches in 998.78s\ttraining loss:\t1.718924\n",
      "Done 1120 batches in 1003.23s\ttraining loss:\t1.718123\n",
      "Done 1125 batches in 1007.73s\ttraining loss:\t1.718561\n",
      "Done 1130 batches in 1012.47s\ttraining loss:\t1.717007\n",
      "Done 1135 batches in 1016.83s\ttraining loss:\t1.716088\n",
      "Done 1140 batches in 1021.56s\ttraining loss:\t1.715799\n",
      "Done 1145 batches in 1026.09s\ttraining loss:\t1.716324\n",
      "Done 1150 batches in 1030.73s\ttraining loss:\t1.716729\n",
      "Done 1155 batches in 1035.55s\ttraining loss:\t1.717572\n",
      "Done 1160 batches in 1039.84s\ttraining loss:\t1.716790\n",
      "Done 1165 batches in 1044.28s\ttraining loss:\t1.717641\n",
      "Done 1170 batches in 1048.28s\ttraining loss:\t1.717289\n",
      "Done 1175 batches in 1052.88s\ttraining loss:\t1.717741\n",
      "Done 1180 batches in 1057.75s\ttraining loss:\t1.718500\n",
      "Done 1185 batches in 1062.56s\ttraining loss:\t1.718256\n",
      "Done 1190 batches in 1066.65s\ttraining loss:\t1.718629\n",
      "Done 1195 batches in 1071.35s\ttraining loss:\t1.718130\n",
      "Done 1200 batches in 1076.01s\ttraining loss:\t1.718456\n",
      "Done 1205 batches in 1080.93s\ttraining loss:\t1.718076\n",
      "Done 1210 batches in 1085.33s\ttraining loss:\t1.717730\n",
      "Done 1215 batches in 1089.84s\ttraining loss:\t1.718594\n",
      "Done 1220 batches in 1094.44s\ttraining loss:\t1.719727\n",
      "Done 1225 batches in 1098.93s\ttraining loss:\t1.720356\n",
      "Done 1230 batches in 1103.09s\ttraining loss:\t1.720265\n",
      "Done 1235 batches in 1107.26s\ttraining loss:\t1.721288\n",
      "Done 1240 batches in 1111.43s\ttraining loss:\t1.720008\n",
      "Done 1245 batches in 1115.82s\ttraining loss:\t1.719227\n",
      "Done 1250 batches in 1119.89s\ttraining loss:\t1.718302\n",
      "Done 1255 batches in 1124.64s\ttraining loss:\t1.718076\n",
      "Done 1260 batches in 1129.03s\ttraining loss:\t1.716991\n",
      "Done 1265 batches in 1133.68s\ttraining loss:\t1.716993\n",
      "Done 1270 batches in 1138.19s\ttraining loss:\t1.717270\n",
      "Done 1275 batches in 1142.47s\ttraining loss:\t1.718177\n",
      "Done 1280 batches in 1147.15s\ttraining loss:\t1.717564\n",
      "Done 1285 batches in 1151.79s\ttraining loss:\t1.716711\n",
      "Done 1290 batches in 1156.64s\ttraining loss:\t1.715972\n",
      "Done 1295 batches in 1161.09s\ttraining loss:\t1.716481\n",
      "Done 1300 batches in 1165.65s\ttraining loss:\t1.715751\n",
      "Done 1305 batches in 1170.11s\ttraining loss:\t1.714243\n",
      "Done 1310 batches in 1174.67s\ttraining loss:\t1.714557\n",
      "Done 1315 batches in 1179.55s\ttraining loss:\t1.716543\n",
      "Done 1320 batches in 1184.37s\ttraining loss:\t1.716513\n",
      "Done 1325 batches in 1188.87s\ttraining loss:\t1.717271\n",
      "Done 1330 batches in 1193.72s\ttraining loss:\t1.718397\n",
      "Done 1335 batches in 1198.27s\ttraining loss:\t1.719199\n",
      "Done 1340 batches in 1202.83s\ttraining loss:\t1.719389\n",
      "Done 1345 batches in 1207.20s\ttraining loss:\t1.718798\n",
      "Done 1350 batches in 1211.48s\ttraining loss:\t1.718376\n",
      "Done 1355 batches in 1216.28s\ttraining loss:\t1.719349\n",
      "Done 1360 batches in 1220.71s\ttraining loss:\t1.718581\n",
      "Done 1365 batches in 1225.26s\ttraining loss:\t1.718681\n",
      "Done 1370 batches in 1229.94s\ttraining loss:\t1.719241\n",
      "Done 1375 batches in 1234.84s\ttraining loss:\t1.720000\n",
      "Done 1380 batches in 1239.57s\ttraining loss:\t1.720122\n",
      "Done 1385 batches in 1243.81s\ttraining loss:\t1.720381\n",
      "Done 1390 batches in 1248.72s\ttraining loss:\t1.719540\n",
      "Done 1395 batches in 1253.57s\ttraining loss:\t1.719472\n",
      "Done 1400 batches in 1257.76s\ttraining loss:\t1.718818\n",
      "Done 1405 batches in 1261.81s\ttraining loss:\t1.718607\n",
      "Done 1410 batches in 1266.20s\ttraining loss:\t1.719579\n",
      "Done 1415 batches in 1270.25s\ttraining loss:\t1.720359\n",
      "Done 1420 batches in 1274.67s\ttraining loss:\t1.719301\n",
      "Done 1425 batches in 1278.93s\ttraining loss:\t1.720097\n",
      "Done 1430 batches in 1283.14s\ttraining loss:\t1.720122\n",
      "Done 1435 batches in 1287.28s\ttraining loss:\t1.719913\n",
      "Done 1440 batches in 1291.88s\ttraining loss:\t1.720262\n",
      "Done 1445 batches in 1296.04s\ttraining loss:\t1.719255\n",
      "Done 1450 batches in 1300.61s\ttraining loss:\t1.718242\n",
      "Done 1455 batches in 1305.03s\ttraining loss:\t1.718439\n",
      "Done 1460 batches in 1309.37s\ttraining loss:\t1.718779\n",
      "Done 1465 batches in 1313.93s\ttraining loss:\t1.719308\n",
      "Done 1470 batches in 1318.79s\ttraining loss:\t1.718866\n",
      "Done 1475 batches in 1323.44s\ttraining loss:\t1.719596\n",
      "Done 1480 batches in 1328.13s\ttraining loss:\t1.719357\n",
      "Done 1485 batches in 1332.65s\ttraining loss:\t1.719455\n",
      "Done 1490 batches in 1336.98s\ttraining loss:\t1.720462\n",
      "Done 1495 batches in 1341.30s\ttraining loss:\t1.719842\n",
      "Done 1500 batches in 1345.58s\ttraining loss:\t1.719702\n",
      "Done 1505 batches in 1350.11s\ttraining loss:\t1.719426\n",
      "Done 1510 batches in 1354.06s\ttraining loss:\t1.719060\n",
      "Done 1515 batches in 1358.59s\ttraining loss:\t1.719502\n",
      "Done 1520 batches in 1363.11s\ttraining loss:\t1.719495\n",
      "Done 1525 batches in 1367.26s\ttraining loss:\t1.718592\n",
      "Done 1530 batches in 1371.76s\ttraining loss:\t1.719398\n",
      "Done 1535 batches in 1376.10s\ttraining loss:\t1.718873\n",
      "Done 1540 batches in 1380.05s\ttraining loss:\t1.718805\n",
      "Done 1545 batches in 1383.87s\ttraining loss:\t1.719685\n",
      "Done 1550 batches in 1388.53s\ttraining loss:\t1.719620\n",
      "Done 1555 batches in 1393.19s\ttraining loss:\t1.719902\n",
      "Done 1560 batches in 1397.92s\ttraining loss:\t1.720693\n",
      "Done 1565 batches in 1402.29s\ttraining loss:\t1.720824\n",
      "Done 1570 batches in 1406.67s\ttraining loss:\t1.720079\n",
      "Done 1575 batches in 1411.55s\ttraining loss:\t1.718761\n",
      "Done 1580 batches in 1416.43s\ttraining loss:\t1.719670\n",
      "Done 1585 batches in 1421.10s\ttraining loss:\t1.720186\n",
      "Done 1590 batches in 1425.65s\ttraining loss:\t1.721011\n",
      "Done 1595 batches in 1430.13s\ttraining loss:\t1.722197\n",
      "Done 1600 batches in 1434.40s\ttraining loss:\t1.721325\n",
      "Done 1605 batches in 1439.29s\ttraining loss:\t1.721848\n",
      "Done 1610 batches in 1443.63s\ttraining loss:\t1.722654\n",
      "Done 1615 batches in 1448.32s\ttraining loss:\t1.723081\n",
      "Done 1620 batches in 1452.92s\ttraining loss:\t1.723271\n",
      "Done 1625 batches in 1457.12s\ttraining loss:\t1.723054\n",
      "Done 1630 batches in 1461.60s\ttraining loss:\t1.723553\n",
      "Done 1635 batches in 1465.91s\ttraining loss:\t1.723529\n",
      "Done 1640 batches in 1470.70s\ttraining loss:\t1.723718\n",
      "Done 1645 batches in 1474.68s\ttraining loss:\t1.723996\n",
      "Done 1650 batches in 1478.98s\ttraining loss:\t1.724197\n",
      "Done 1655 batches in 1483.90s\ttraining loss:\t1.723231\n",
      "Done 1660 batches in 1488.13s\ttraining loss:\t1.724842\n",
      "Done 1665 batches in 1493.12s\ttraining loss:\t1.724937\n",
      "Done 1670 batches in 1497.46s\ttraining loss:\t1.725531\n",
      "Done 1675 batches in 1502.04s\ttraining loss:\t1.725536\n",
      "Done 1680 batches in 1506.21s\ttraining loss:\t1.725384\n",
      "Done 1685 batches in 1510.68s\ttraining loss:\t1.725709\n",
      "Done 1690 batches in 1515.17s\ttraining loss:\t1.725926\n",
      "Done 1695 batches in 1519.70s\ttraining loss:\t1.726357\n",
      "Done 1700 batches in 1524.56s\ttraining loss:\t1.727277\n",
      "Done 1705 batches in 1528.68s\ttraining loss:\t1.727040\n",
      "Done 1710 batches in 1533.53s\ttraining loss:\t1.726558\n",
      "Done 1715 batches in 1538.17s\ttraining loss:\t1.727108\n",
      "Done 1720 batches in 1542.55s\ttraining loss:\t1.727249\n",
      "Done 1725 batches in 1547.42s\ttraining loss:\t1.728342\n",
      "Done 1730 batches in 1551.62s\ttraining loss:\t1.727735\n",
      "Done 1735 batches in 1556.68s\ttraining loss:\t1.728672\n",
      "Done 1740 batches in 1561.21s\ttraining loss:\t1.728527\n",
      "Done 1745 batches in 1564.96s\ttraining loss:\t1.728480\n",
      "Done 1750 batches in 1569.36s\ttraining loss:\t1.727572\n",
      "Done 1755 batches in 1574.16s\ttraining loss:\t1.727691\n",
      "Done 1760 batches in 1578.73s\ttraining loss:\t1.727780\n",
      "Done 1765 batches in 1583.17s\ttraining loss:\t1.728202\n",
      "Done 1770 batches in 1587.80s\ttraining loss:\t1.728117\n",
      "Done 1775 batches in 1592.47s\ttraining loss:\t1.728716\n",
      "Done 1780 batches in 1596.99s\ttraining loss:\t1.728135\n",
      "Done 1785 batches in 1601.53s\ttraining loss:\t1.728467\n",
      "Done 1790 batches in 1606.17s\ttraining loss:\t1.727665\n",
      "Done 1795 batches in 1610.87s\ttraining loss:\t1.727679\n",
      "Done 1800 batches in 1615.26s\ttraining loss:\t1.727728\n",
      "Done 1805 batches in 1619.93s\ttraining loss:\t1.728953\n",
      "Done 1810 batches in 1624.29s\ttraining loss:\t1.729675\n",
      "Done 1815 batches in 1629.34s\ttraining loss:\t1.730248\n",
      "Done 1820 batches in 1633.95s\ttraining loss:\t1.731062\n",
      "Done 1825 batches in 1638.52s\ttraining loss:\t1.730688\n",
      "Done 1830 batches in 1642.85s\ttraining loss:\t1.731334\n",
      "Done 1835 batches in 1647.40s\ttraining loss:\t1.731667\n",
      "Done 1840 batches in 1651.78s\ttraining loss:\t1.731914\n",
      "Done 1845 batches in 1656.73s\ttraining loss:\t1.731153\n",
      "Done 1850 batches in 1661.32s\ttraining loss:\t1.732697\n",
      "Done 1855 batches in 1665.72s\ttraining loss:\t1.732249\n",
      "Done 1860 batches in 1670.36s\ttraining loss:\t1.732021\n",
      "Done 1865 batches in 1675.24s\ttraining loss:\t1.732048\n",
      "Done 1870 batches in 1679.52s\ttraining loss:\t1.731358\n",
      "Done 1875 batches in 1684.17s\ttraining loss:\t1.731638\n",
      "Done 1880 batches in 1688.91s\ttraining loss:\t1.731967\n",
      "Done 1885 batches in 1693.32s\ttraining loss:\t1.732385\n",
      "Done 1890 batches in 1697.89s\ttraining loss:\t1.731840\n",
      "Done 1895 batches in 1702.71s\ttraining loss:\t1.731598\n",
      "Done 1900 batches in 1707.22s\ttraining loss:\t1.731610\n",
      "Done 1905 batches in 1711.93s\ttraining loss:\t1.731036\n",
      "Done 1910 batches in 1716.30s\ttraining loss:\t1.730048\n",
      "Done 1915 batches in 1720.43s\ttraining loss:\t1.729605\n",
      "Done 1920 batches in 1724.83s\ttraining loss:\t1.729482\n",
      "Done 1925 batches in 1729.77s\ttraining loss:\t1.730783\n",
      "Done 1930 batches in 1734.11s\ttraining loss:\t1.730042\n",
      "Done 1935 batches in 1738.34s\ttraining loss:\t1.729492\n",
      "Done 1940 batches in 1742.65s\ttraining loss:\t1.729443\n",
      "Done 1945 batches in 1747.07s\ttraining loss:\t1.728961\n",
      "Done 1950 batches in 1751.45s\ttraining loss:\t1.729631\n",
      "Done 1955 batches in 1756.10s\ttraining loss:\t1.729083\n",
      "Done 1960 batches in 1760.73s\ttraining loss:\t1.729079\n",
      "Done 1965 batches in 1765.24s\ttraining loss:\t1.728336\n",
      "Done 1970 batches in 1770.27s\ttraining loss:\t1.728866\n",
      "Done 1975 batches in 1774.55s\ttraining loss:\t1.728457\n",
      "Done 1980 batches in 1779.22s\ttraining loss:\t1.727932\n",
      "Done 1985 batches in 1783.38s\ttraining loss:\t1.728529\n",
      "Done 1990 batches in 1787.86s\ttraining loss:\t1.728418\n",
      "Done 1995 batches in 1792.36s\ttraining loss:\t1.729317\n",
      "Done 2000 batches in 1796.98s\ttraining loss:\t1.728733\n",
      "Done 2005 batches in 1801.08s\ttraining loss:\t1.729631\n",
      "Done 2010 batches in 1805.70s\ttraining loss:\t1.729609\n",
      "Done 2015 batches in 1809.95s\ttraining loss:\t1.728563\n",
      "Done 2020 batches in 1814.27s\ttraining loss:\t1.728438\n",
      "Done 2025 batches in 1818.76s\ttraining loss:\t1.728411\n",
      "Done 2030 batches in 1823.25s\ttraining loss:\t1.728474\n",
      "Done 2035 batches in 1827.51s\ttraining loss:\t1.728160\n",
      "Done 2040 batches in 1831.94s\ttraining loss:\t1.728441\n",
      "Done 2045 batches in 1836.72s\ttraining loss:\t1.728509\n",
      "Done 2050 batches in 1841.56s\ttraining loss:\t1.729161\n",
      "Done 2055 batches in 1846.42s\ttraining loss:\t1.729401\n",
      "Done 2060 batches in 1851.12s\ttraining loss:\t1.730310\n",
      "Done 2065 batches in 1856.00s\ttraining loss:\t1.729999\n",
      "Done 2070 batches in 1860.40s\ttraining loss:\t1.729368\n",
      "Done 2075 batches in 1864.66s\ttraining loss:\t1.728681\n",
      "Done 2080 batches in 1869.70s\ttraining loss:\t1.728673\n",
      "Done 2085 batches in 1873.85s\ttraining loss:\t1.728080\n",
      "Done 2090 batches in 1878.02s\ttraining loss:\t1.728582\n",
      "Done 2095 batches in 1882.46s\ttraining loss:\t1.729178\n",
      "Done 2100 batches in 1886.75s\ttraining loss:\t1.730142\n",
      "Done 2105 batches in 1891.70s\ttraining loss:\t1.729798\n",
      "Done 2110 batches in 1895.70s\ttraining loss:\t1.729462\n",
      "Done 2115 batches in 1900.27s\ttraining loss:\t1.729861\n",
      "Done 2120 batches in 1905.30s\ttraining loss:\t1.730326\n",
      "Done 2125 batches in 1909.46s\ttraining loss:\t1.730524\n",
      "Done 2130 batches in 1913.85s\ttraining loss:\t1.729748\n",
      "Done 2135 batches in 1918.35s\ttraining loss:\t1.729433\n",
      "Done 2140 batches in 1923.14s\ttraining loss:\t1.728672\n",
      "Done 2145 batches in 1927.08s\ttraining loss:\t1.728484\n",
      "Done 2150 batches in 1931.18s\ttraining loss:\t1.728523\n",
      "Done 2155 batches in 1935.53s\ttraining loss:\t1.728102\n",
      "Done 2160 batches in 1940.03s\ttraining loss:\t1.727940\n",
      "Done 2165 batches in 1943.85s\ttraining loss:\t1.727540\n",
      "Done 2170 batches in 1948.49s\ttraining loss:\t1.728141\n",
      "Done 2175 batches in 1952.83s\ttraining loss:\t1.727548\n",
      "Done 2180 batches in 1957.05s\ttraining loss:\t1.727655\n",
      "Done 2185 batches in 1961.43s\ttraining loss:\t1.727677\n",
      "Done 2190 batches in 1965.55s\ttraining loss:\t1.727973\n",
      "Done 2195 batches in 1969.63s\ttraining loss:\t1.728545\n",
      "Done 2200 batches in 1974.37s\ttraining loss:\t1.728693\n",
      "Done 2205 batches in 1979.24s\ttraining loss:\t1.727705\n",
      "Done 2210 batches in 1984.09s\ttraining loss:\t1.727303\n",
      "Done 2215 batches in 1988.49s\ttraining loss:\t1.727490\n",
      "Done 2220 batches in 1993.06s\ttraining loss:\t1.727607\n",
      "Done 2225 batches in 1997.67s\ttraining loss:\t1.727245\n",
      "Done 2230 batches in 2002.04s\ttraining loss:\t1.726787\n",
      "Done 2235 batches in 2007.01s\ttraining loss:\t1.726480\n",
      "Done 2240 batches in 2011.55s\ttraining loss:\t1.727028\n",
      "Done 2245 batches in 2015.95s\ttraining loss:\t1.726951\n",
      "Done 2250 batches in 2020.73s\ttraining loss:\t1.727182\n",
      "Done 2255 batches in 2025.10s\ttraining loss:\t1.726367\n",
      "Done 2260 batches in 2029.55s\ttraining loss:\t1.726140\n",
      "Done 2265 batches in 2034.18s\ttraining loss:\t1.726128\n",
      "Done 2270 batches in 2038.88s\ttraining loss:\t1.726023\n",
      "Done 2275 batches in 2043.15s\ttraining loss:\t1.725826\n",
      "Done 2280 batches in 2047.61s\ttraining loss:\t1.725535\n",
      "Done 2285 batches in 2051.81s\ttraining loss:\t1.725605\n",
      "Done 2290 batches in 2055.81s\ttraining loss:\t1.725162\n",
      "Done 2295 batches in 2060.25s\ttraining loss:\t1.725458\n",
      "Done 2300 batches in 2064.34s\ttraining loss:\t1.725380\n",
      "Done 2305 batches in 2069.31s\ttraining loss:\t1.725835\n",
      "Done 2310 batches in 2073.43s\ttraining loss:\t1.725361\n",
      "Done 2315 batches in 2078.36s\ttraining loss:\t1.725519\n",
      "Done 2320 batches in 2082.54s\ttraining loss:\t1.726277\n",
      "Done 2325 batches in 2086.94s\ttraining loss:\t1.726034\n",
      "Done 2330 batches in 2091.30s\ttraining loss:\t1.725989\n",
      "Done 2335 batches in 2095.60s\ttraining loss:\t1.725892\n",
      "Done 2340 batches in 2100.02s\ttraining loss:\t1.726985\n",
      "Done 2345 batches in 2104.10s\ttraining loss:\t1.727172\n",
      "Done 2350 batches in 2108.13s\ttraining loss:\t1.726868\n",
      "Done 2355 batches in 2112.15s\ttraining loss:\t1.726598\n",
      "Done 2360 batches in 2116.53s\ttraining loss:\t1.727391\n",
      "Done 2365 batches in 2121.07s\ttraining loss:\t1.727503\n",
      "Done 2370 batches in 2125.42s\ttraining loss:\t1.727958\n",
      "Done 2375 batches in 2129.88s\ttraining loss:\t1.727599\n",
      "Done 2380 batches in 2134.03s\ttraining loss:\t1.727311\n",
      "Done 2385 batches in 2138.41s\ttraining loss:\t1.727442\n",
      "Done 2390 batches in 2142.59s\ttraining loss:\t1.727485\n",
      "Done 2395 batches in 2147.01s\ttraining loss:\t1.727462\n",
      "Done 2400 batches in 2151.15s\ttraining loss:\t1.727324\n",
      "Done 2405 batches in 2155.53s\ttraining loss:\t1.727217\n",
      "Done 2410 batches in 2160.02s\ttraining loss:\t1.727254\n",
      "Done 2415 batches in 2164.60s\ttraining loss:\t1.727841\n",
      "Done 2420 batches in 2168.93s\ttraining loss:\t1.727608\n",
      "Done 2425 batches in 2173.18s\ttraining loss:\t1.727250\n",
      "Done 2430 batches in 2177.88s\ttraining loss:\t1.727014\n",
      "Done 2435 batches in 2182.37s\ttraining loss:\t1.726561\n",
      "Done 2440 batches in 2186.87s\ttraining loss:\t1.726459\n",
      "Done 2445 batches in 2191.49s\ttraining loss:\t1.726608\n",
      "Done 2450 batches in 2196.17s\ttraining loss:\t1.726314\n",
      "Done 2455 batches in 2201.17s\ttraining loss:\t1.726082\n",
      "Done 2460 batches in 2206.05s\ttraining loss:\t1.725444\n",
      "Done 2465 batches in 2211.47s\ttraining loss:\t1.725309\n",
      "Done 2470 batches in 2215.49s\ttraining loss:\t1.725227\n",
      "Done 2475 batches in 2220.17s\ttraining loss:\t1.724981\n",
      "Done 2480 batches in 2224.86s\ttraining loss:\t1.724907\n",
      "Done 2485 batches in 2229.57s\ttraining loss:\t1.724878\n",
      "Done 2490 batches in 2234.40s\ttraining loss:\t1.724075\n",
      "Done 2495 batches in 2239.20s\ttraining loss:\t1.723439\n",
      "Done 2500 batches in 2244.18s\ttraining loss:\t1.723691\n",
      "Done 2505 batches in 2248.95s\ttraining loss:\t1.724228\n",
      "Done 2510 batches in 2253.38s\ttraining loss:\t1.724487\n",
      "Done 2515 batches in 2257.45s\ttraining loss:\t1.723684\n",
      "Done 2520 batches in 2261.50s\ttraining loss:\t1.724141\n",
      "Done 2525 batches in 2266.16s\ttraining loss:\t1.723854\n",
      "Done 2530 batches in 2270.36s\ttraining loss:\t1.724434\n",
      "Done 2535 batches in 2274.97s\ttraining loss:\t1.724015\n",
      "Done 2540 batches in 2280.24s\ttraining loss:\t1.724186\n",
      "Done 2545 batches in 2284.49s\ttraining loss:\t1.724452\n",
      "Done 2550 batches in 2289.74s\ttraining loss:\t1.723968\n",
      "Done 2555 batches in 2294.11s\ttraining loss:\t1.724046\n",
      "Done 2560 batches in 2298.06s\ttraining loss:\t1.724407\n",
      "Done 2565 batches in 2302.74s\ttraining loss:\t1.723890\n",
      "Done 2570 batches in 2307.19s\ttraining loss:\t1.723661\n",
      "Done 2575 batches in 2311.71s\ttraining loss:\t1.723828\n",
      "Done 2580 batches in 2315.99s\ttraining loss:\t1.723866\n",
      "Done 2585 batches in 2320.68s\ttraining loss:\t1.723826\n",
      "Done 2590 batches in 2325.21s\ttraining loss:\t1.724027\n",
      "Done 2595 batches in 2329.29s\ttraining loss:\t1.724457\n",
      "Done 2600 batches in 2333.84s\ttraining loss:\t1.724070\n",
      "Done 2605 batches in 2338.52s\ttraining loss:\t1.723890\n",
      "Done 2610 batches in 2342.98s\ttraining loss:\t1.724595\n",
      "Done 2615 batches in 2347.44s\ttraining loss:\t1.725077\n",
      "Done 2620 batches in 2352.08s\ttraining loss:\t1.724775\n",
      "Done 2625 batches in 2356.27s\ttraining loss:\t1.724554\n",
      "Done 2630 batches in 2360.84s\ttraining loss:\t1.724273\n",
      "Done 2635 batches in 2365.12s\ttraining loss:\t1.724439\n",
      "Done 2640 batches in 2369.68s\ttraining loss:\t1.724676\n",
      "Done 2645 batches in 2374.12s\ttraining loss:\t1.724913\n",
      "Done 2650 batches in 2378.62s\ttraining loss:\t1.725314\n",
      "Done 2655 batches in 2383.26s\ttraining loss:\t1.724743\n",
      "Done 2660 batches in 2387.77s\ttraining loss:\t1.724673\n",
      "Done 2665 batches in 2392.48s\ttraining loss:\t1.724291\n",
      "Done 2670 batches in 2396.35s\ttraining loss:\t1.724681\n",
      "Done 2675 batches in 2400.67s\ttraining loss:\t1.724044\n",
      "Done 2680 batches in 2405.58s\ttraining loss:\t1.723881\n",
      "Done 2685 batches in 2409.92s\ttraining loss:\t1.723422\n",
      "Done 2690 batches in 2414.57s\ttraining loss:\t1.723376\n",
      "Done 2695 batches in 2418.91s\ttraining loss:\t1.723492\n",
      "Done 2700 batches in 2423.07s\ttraining loss:\t1.724249\n",
      "Done 2705 batches in 2427.53s\ttraining loss:\t1.724298\n",
      "Done 2710 batches in 2432.27s\ttraining loss:\t1.723948\n",
      "Done 2715 batches in 2436.94s\ttraining loss:\t1.724850\n",
      "Done 2720 batches in 2441.29s\ttraining loss:\t1.725395\n",
      "Done 2725 batches in 2445.60s\ttraining loss:\t1.725391\n",
      "Done 2730 batches in 2450.12s\ttraining loss:\t1.726045\n",
      "Done 2735 batches in 2454.18s\ttraining loss:\t1.725249\n",
      "Done 2740 batches in 2458.38s\ttraining loss:\t1.726036\n",
      "Done 2745 batches in 2462.65s\ttraining loss:\t1.725484\n",
      "Done 2750 batches in 2467.28s\ttraining loss:\t1.725596\n",
      "Done 2755 batches in 2471.82s\ttraining loss:\t1.725525\n",
      "Done 2760 batches in 2476.01s\ttraining loss:\t1.724962\n",
      "Done 2765 batches in 2480.52s\ttraining loss:\t1.724399\n",
      "Done 2770 batches in 2484.76s\ttraining loss:\t1.723964\n",
      "Done 2775 batches in 2489.15s\ttraining loss:\t1.724043\n",
      "Done 2780 batches in 2493.44s\ttraining loss:\t1.723820\n",
      "Done 2785 batches in 2497.84s\ttraining loss:\t1.723956\n",
      "Done 2790 batches in 2502.41s\ttraining loss:\t1.723500\n",
      "Done 2795 batches in 2506.59s\ttraining loss:\t1.723772\n",
      "Done 2800 batches in 2511.37s\ttraining loss:\t1.724035\n",
      "Done 2805 batches in 2515.66s\ttraining loss:\t1.724388\n",
      "Done 2810 batches in 2519.72s\ttraining loss:\t1.724220\n",
      "Done 2815 batches in 2524.40s\ttraining loss:\t1.723688\n",
      "Done 2820 batches in 2528.94s\ttraining loss:\t1.723741\n",
      "Done 2825 batches in 2532.98s\ttraining loss:\t1.723772\n",
      "Done 2830 batches in 2538.06s\ttraining loss:\t1.723708\n",
      "Done 2835 batches in 2542.51s\ttraining loss:\t1.723085\n",
      "Done 2840 batches in 2546.70s\ttraining loss:\t1.722548\n",
      "Done 2845 batches in 2550.92s\ttraining loss:\t1.721988\n",
      "Done 2850 batches in 2555.56s\ttraining loss:\t1.722869\n",
      "Done 2855 batches in 2559.86s\ttraining loss:\t1.723271\n",
      "Done 2860 batches in 2564.32s\ttraining loss:\t1.723467\n",
      "Done 2865 batches in 2568.43s\ttraining loss:\t1.723245\n",
      "Done 2870 batches in 2573.14s\ttraining loss:\t1.723451\n",
      "Done 2875 batches in 2577.79s\ttraining loss:\t1.723301\n",
      "Done 2880 batches in 2581.98s\ttraining loss:\t1.722918\n",
      "Done 2885 batches in 2586.89s\ttraining loss:\t1.723001\n",
      "Done 2890 batches in 2591.62s\ttraining loss:\t1.722702\n",
      "Done 2895 batches in 2596.22s\ttraining loss:\t1.722849\n",
      "Done 2900 batches in 2600.38s\ttraining loss:\t1.723670\n",
      "Done 2905 batches in 2604.84s\ttraining loss:\t1.724174\n",
      "Done 2910 batches in 2609.03s\ttraining loss:\t1.724848\n",
      "Done 2915 batches in 2613.22s\ttraining loss:\t1.724367\n",
      "Done 2920 batches in 2617.68s\ttraining loss:\t1.724057\n",
      "Done 2925 batches in 2621.78s\ttraining loss:\t1.723800\n",
      "Done 2930 batches in 2626.15s\ttraining loss:\t1.723601\n",
      "Done 2935 batches in 2630.42s\ttraining loss:\t1.723510\n",
      "Done 2940 batches in 2634.92s\ttraining loss:\t1.723287\n",
      "Done 2945 batches in 2639.57s\ttraining loss:\t1.722979\n",
      "Done 2950 batches in 2644.10s\ttraining loss:\t1.722822\n",
      "Done 2955 batches in 2648.30s\ttraining loss:\t1.722500\n",
      "Done 2960 batches in 2652.22s\ttraining loss:\t1.722252\n",
      "Done 2965 batches in 2656.57s\ttraining loss:\t1.722063\n",
      "Done 2970 batches in 2661.07s\ttraining loss:\t1.721968\n",
      "Done 2975 batches in 2665.67s\ttraining loss:\t1.722018\n",
      "Done 2980 batches in 2670.10s\ttraining loss:\t1.722344\n",
      "Done 2985 batches in 2674.87s\ttraining loss:\t1.721899\n",
      "Done 2990 batches in 2679.65s\ttraining loss:\t1.722200\n",
      "Done 2995 batches in 2683.69s\ttraining loss:\t1.722034\n",
      "Done 3000 batches in 2688.17s\ttraining loss:\t1.721289\n",
      "Done 3005 batches in 2692.66s\ttraining loss:\t1.721539\n",
      "Done 3010 batches in 2697.44s\ttraining loss:\t1.721879\n",
      "Done 3015 batches in 2701.99s\ttraining loss:\t1.721152\n",
      "Done 3020 batches in 2706.25s\ttraining loss:\t1.720969\n",
      "Done 3025 batches in 2710.87s\ttraining loss:\t1.720809\n",
      "Done 3030 batches in 2715.49s\ttraining loss:\t1.720684\n",
      "Done 3035 batches in 2719.92s\ttraining loss:\t1.720790\n",
      "Done 3040 batches in 2724.26s\ttraining loss:\t1.720484\n",
      "Done 3045 batches in 2728.61s\ttraining loss:\t1.720001\n",
      "Done 3050 batches in 2733.10s\ttraining loss:\t1.719882\n",
      "Done 3055 batches in 2737.48s\ttraining loss:\t1.719791\n",
      "Done 3060 batches in 2742.24s\ttraining loss:\t1.720186\n",
      "Done 3065 batches in 2746.67s\ttraining loss:\t1.720043\n",
      "Done 3070 batches in 2751.16s\ttraining loss:\t1.720063\n",
      "Done 3075 batches in 2755.75s\ttraining loss:\t1.719941\n",
      "Done 3080 batches in 2760.25s\ttraining loss:\t1.720121\n",
      "Done 3085 batches in 2764.68s\ttraining loss:\t1.719804\n",
      "Done 3090 batches in 2769.11s\ttraining loss:\t1.719807\n",
      "Done 3095 batches in 2773.50s\ttraining loss:\t1.719631\n",
      "Done 3100 batches in 2778.29s\ttraining loss:\t1.719434\n",
      "Done 3105 batches in 2782.49s\ttraining loss:\t1.719050\n",
      "Done 3110 batches in 2786.78s\ttraining loss:\t1.718896\n",
      "Done 3115 batches in 2791.39s\ttraining loss:\t1.719758\n",
      "Done 3120 batches in 2796.08s\ttraining loss:\t1.720296\n",
      "Done 3125 batches in 2800.48s\ttraining loss:\t1.719727\n",
      "Done 3130 batches in 2804.40s\ttraining loss:\t1.720137\n",
      "Done 3135 batches in 2808.94s\ttraining loss:\t1.720447\n",
      "Done 3140 batches in 2812.84s\ttraining loss:\t1.720195\n",
      "Done 3145 batches in 2817.36s\ttraining loss:\t1.720098\n",
      "Done 3150 batches in 2822.39s\ttraining loss:\t1.720408\n",
      "Done 3155 batches in 2827.09s\ttraining loss:\t1.720954\n",
      "Done 3160 batches in 2831.61s\ttraining loss:\t1.720942\n",
      "Done 3165 batches in 2836.16s\ttraining loss:\t1.720953\n",
      "Done 3170 batches in 2840.27s\ttraining loss:\t1.720777\n",
      "Done 3175 batches in 2845.00s\ttraining loss:\t1.720657\n",
      "Done 3180 batches in 2849.72s\ttraining loss:\t1.721173\n",
      "Done 3185 batches in 2853.83s\ttraining loss:\t1.721315\n",
      "Done 3190 batches in 2858.00s\ttraining loss:\t1.722111\n",
      "Done 3195 batches in 2862.48s\ttraining loss:\t1.722406\n",
      "Done 3200 batches in 2866.91s\ttraining loss:\t1.722474\n",
      "Done 3205 batches in 2871.33s\ttraining loss:\t1.722303\n",
      "Done 3210 batches in 2876.17s\ttraining loss:\t1.722507\n",
      "Done 3215 batches in 2880.72s\ttraining loss:\t1.722742\n",
      "Done 3220 batches in 2885.78s\ttraining loss:\t1.722781\n",
      "Done 3225 batches in 2890.29s\ttraining loss:\t1.723108\n",
      "Done 3230 batches in 2894.98s\ttraining loss:\t1.723265\n",
      "Done 3235 batches in 2899.50s\ttraining loss:\t1.723410\n",
      "Done 3240 batches in 2903.73s\ttraining loss:\t1.723084\n",
      "Done 3245 batches in 2907.73s\ttraining loss:\t1.723232\n",
      "Done 3250 batches in 2912.03s\ttraining loss:\t1.723224\n",
      "Done 3255 batches in 2916.15s\ttraining loss:\t1.723429\n",
      "Done 3260 batches in 2920.48s\ttraining loss:\t1.723707\n",
      "Done 3265 batches in 2925.04s\ttraining loss:\t1.723432\n",
      "Done 3270 batches in 2928.97s\ttraining loss:\t1.723340\n",
      "Done 3275 batches in 2933.79s\ttraining loss:\t1.723336\n",
      "Done 3280 batches in 2938.68s\ttraining loss:\t1.723817\n",
      "Done 3285 batches in 2943.07s\ttraining loss:\t1.724074\n",
      "Done 3290 batches in 2947.95s\ttraining loss:\t1.724092\n",
      "Done 3295 batches in 2952.62s\ttraining loss:\t1.724133\n",
      "Done 3300 batches in 2956.74s\ttraining loss:\t1.724318\n",
      "Done 3305 batches in 2961.48s\ttraining loss:\t1.724822\n",
      "Done 3310 batches in 2966.30s\ttraining loss:\t1.724505\n",
      "Done 3315 batches in 2971.07s\ttraining loss:\t1.724912\n",
      "Done 3320 batches in 2975.83s\ttraining loss:\t1.725107\n",
      "Done 3325 batches in 2980.13s\ttraining loss:\t1.724610\n",
      "Done 3330 batches in 2984.78s\ttraining loss:\t1.724676\n",
      "Done 3335 batches in 2989.04s\ttraining loss:\t1.724574\n",
      "Done 3340 batches in 2993.74s\ttraining loss:\t1.724960\n",
      "Done 3345 batches in 2998.71s\ttraining loss:\t1.724568\n",
      "Done 3350 batches in 3002.85s\ttraining loss:\t1.724843\n",
      "Done 3355 batches in 3007.15s\ttraining loss:\t1.724967\n",
      "Done 3360 batches in 3011.79s\ttraining loss:\t1.724755\n",
      "Done 3365 batches in 3016.57s\ttraining loss:\t1.724295\n",
      "Done 3370 batches in 3021.10s\ttraining loss:\t1.725000\n",
      "Done 3375 batches in 3025.48s\ttraining loss:\t1.725114\n",
      "Done 3380 batches in 3030.06s\ttraining loss:\t1.724706\n",
      "Done 3385 batches in 3034.57s\ttraining loss:\t1.724878\n",
      "Done 3390 batches in 3039.32s\ttraining loss:\t1.725120\n",
      "Done 3395 batches in 3043.74s\ttraining loss:\t1.724984\n",
      "Done 3400 batches in 3048.39s\ttraining loss:\t1.724666\n",
      "Done 3405 batches in 3053.23s\ttraining loss:\t1.724916\n",
      "Done 3410 batches in 3058.05s\ttraining loss:\t1.724879\n",
      "Done 3415 batches in 3062.45s\ttraining loss:\t1.724890\n",
      "Done 3420 batches in 3067.00s\ttraining loss:\t1.725330\n",
      "Done 3425 batches in 3070.99s\ttraining loss:\t1.725531\n",
      "Done 3430 batches in 3075.05s\ttraining loss:\t1.725446\n",
      "Done 3435 batches in 3079.50s\ttraining loss:\t1.725207\n",
      "Done 3440 batches in 3083.67s\ttraining loss:\t1.725078\n",
      "Done 3445 batches in 3088.05s\ttraining loss:\t1.725609\n",
      "Done 3450 batches in 3092.90s\ttraining loss:\t1.725063\n",
      "Done 3455 batches in 3097.40s\ttraining loss:\t1.725173\n",
      "Done 3460 batches in 3102.26s\ttraining loss:\t1.724946\n",
      "Done 3465 batches in 3107.00s\ttraining loss:\t1.724753\n",
      "Done 3470 batches in 3111.58s\ttraining loss:\t1.724720\n",
      "Done 3475 batches in 3116.02s\ttraining loss:\t1.724430\n",
      "Done 3480 batches in 3120.23s\ttraining loss:\t1.724825\n",
      "Done 3485 batches in 3124.81s\ttraining loss:\t1.724748\n",
      "Done 3490 batches in 3129.20s\ttraining loss:\t1.724695\n",
      "Done 3495 batches in 3134.09s\ttraining loss:\t1.724449\n",
      "Done 3500 batches in 3138.52s\ttraining loss:\t1.724233\n",
      "Done 3505 batches in 3142.68s\ttraining loss:\t1.724427\n",
      "Done 3510 batches in 3147.20s\ttraining loss:\t1.724480\n",
      "Done 3515 batches in 3151.67s\ttraining loss:\t1.725119\n",
      "Done 3520 batches in 3156.23s\ttraining loss:\t1.725540\n",
      "Done 3525 batches in 3160.33s\ttraining loss:\t1.725083\n",
      "Done 3530 batches in 3164.70s\ttraining loss:\t1.724720\n",
      "Done 3535 batches in 3169.27s\ttraining loss:\t1.725371\n",
      "Done 3540 batches in 3173.66s\ttraining loss:\t1.725367\n",
      "Done 3545 batches in 3178.00s\ttraining loss:\t1.725843\n",
      "Done 3550 batches in 3182.20s\ttraining loss:\t1.726017\n",
      "Done 3555 batches in 3186.56s\ttraining loss:\t1.725944\n",
      "Done 3560 batches in 3191.52s\ttraining loss:\t1.725814\n",
      "Done 3565 batches in 3195.91s\ttraining loss:\t1.725497\n",
      "Done 3570 batches in 3200.78s\ttraining loss:\t1.725460\n",
      "Done 3575 batches in 3205.28s\ttraining loss:\t1.725323\n",
      "Done 3580 batches in 3209.98s\ttraining loss:\t1.725831\n",
      "Done 3585 batches in 3214.91s\ttraining loss:\t1.725929\n",
      "Done 3590 batches in 3219.44s\ttraining loss:\t1.725655\n",
      "Done 3595 batches in 3223.76s\ttraining loss:\t1.724958\n",
      "Done 3600 batches in 3228.68s\ttraining loss:\t1.725446\n",
      "Done 3605 batches in 3233.12s\ttraining loss:\t1.725664\n",
      "Done 3610 batches in 3237.63s\ttraining loss:\t1.725239\n",
      "Done 3615 batches in 3242.15s\ttraining loss:\t1.725125\n",
      "Done 3620 batches in 3246.49s\ttraining loss:\t1.725267\n",
      "Done 3625 batches in 3250.94s\ttraining loss:\t1.725377\n",
      "Done 3630 batches in 3255.75s\ttraining loss:\t1.725606\n",
      "Done 3635 batches in 3260.15s\ttraining loss:\t1.726118\n",
      "Done 3640 batches in 3264.13s\ttraining loss:\t1.726252\n",
      "Done 3645 batches in 3268.02s\ttraining loss:\t1.726689\n",
      "Done 3650 batches in 3273.03s\ttraining loss:\t1.726717\n",
      "Done 3655 batches in 3277.39s\ttraining loss:\t1.726742\n",
      "Done 3660 batches in 3281.67s\ttraining loss:\t1.726405\n",
      "Done 3665 batches in 3286.43s\ttraining loss:\t1.725984\n",
      "Done 3670 batches in 3290.78s\ttraining loss:\t1.725917\n",
      "Done 3675 batches in 3295.48s\ttraining loss:\t1.725645\n",
      "Done 3680 batches in 3299.35s\ttraining loss:\t1.726009\n",
      "Done 3685 batches in 3303.86s\ttraining loss:\t1.725889\n",
      "Done 3690 batches in 3308.20s\ttraining loss:\t1.725986\n",
      "Done 3695 batches in 3312.78s\ttraining loss:\t1.726367\n",
      "Done 3700 batches in 3316.87s\ttraining loss:\t1.726567\n",
      "Done 3705 batches in 3321.76s\ttraining loss:\t1.726714\n",
      "Done 3710 batches in 3326.01s\ttraining loss:\t1.726754\n",
      "Done 3715 batches in 3330.36s\ttraining loss:\t1.727239\n",
      "Done 3720 batches in 3335.13s\ttraining loss:\t1.727110\n",
      "Done 3725 batches in 3339.66s\ttraining loss:\t1.726842\n",
      "Done 3730 batches in 3344.13s\ttraining loss:\t1.727368\n",
      "Done 3735 batches in 3348.29s\ttraining loss:\t1.727671\n",
      "Done 3740 batches in 3352.84s\ttraining loss:\t1.727613\n",
      "Done 3745 batches in 3357.24s\ttraining loss:\t1.728167\n",
      "Done 3750 batches in 3361.17s\ttraining loss:\t1.728614\n",
      "Done 3755 batches in 3365.34s\ttraining loss:\t1.728879\n",
      "Done 3760 batches in 3369.70s\ttraining loss:\t1.729023\n",
      "Done 3765 batches in 3374.19s\ttraining loss:\t1.729058\n",
      "Done 3770 batches in 3379.20s\ttraining loss:\t1.729269\n",
      "Done 3775 batches in 3383.66s\ttraining loss:\t1.729236\n",
      "Done 3780 batches in 3388.42s\ttraining loss:\t1.729705\n",
      "Done 3785 batches in 3392.96s\ttraining loss:\t1.729606\n",
      "Done 3790 batches in 3397.32s\ttraining loss:\t1.729856\n",
      "Done 3795 batches in 3401.41s\ttraining loss:\t1.730313\n",
      "Done 3800 batches in 3406.37s\ttraining loss:\t1.730500\n",
      "Done 3805 batches in 3410.45s\ttraining loss:\t1.730542\n",
      "Done 3810 batches in 3414.63s\ttraining loss:\t1.730703\n",
      "Done 3815 batches in 3419.67s\ttraining loss:\t1.731201\n",
      "Done 3820 batches in 3423.83s\ttraining loss:\t1.730864\n",
      "Done 3825 batches in 3428.37s\ttraining loss:\t1.730698\n",
      "Done 3830 batches in 3433.01s\ttraining loss:\t1.730909\n",
      "Done 3835 batches in 3437.42s\ttraining loss:\t1.730749\n",
      "Done 3840 batches in 3442.16s\ttraining loss:\t1.731068\n",
      "Done 3845 batches in 3446.06s\ttraining loss:\t1.731084\n",
      "Done 3850 batches in 3449.89s\ttraining loss:\t1.731051\n",
      "Done 3855 batches in 3454.24s\ttraining loss:\t1.731163\n",
      "Done 3860 batches in 3458.82s\ttraining loss:\t1.730709\n",
      "Done 3865 batches in 3463.80s\ttraining loss:\t1.730804\n",
      "Done 3870 batches in 3468.13s\ttraining loss:\t1.730559\n",
      "Done 3875 batches in 3472.81s\ttraining loss:\t1.730605\n",
      "Done 3880 batches in 3477.67s\ttraining loss:\t1.730613\n",
      "Done 3885 batches in 3481.91s\ttraining loss:\t1.730463\n",
      "Done 3890 batches in 3485.80s\ttraining loss:\t1.730404\n",
      "Done 3895 batches in 3490.31s\ttraining loss:\t1.730837\n",
      "Done 3900 batches in 3494.83s\ttraining loss:\t1.730877\n",
      "Done 3905 batches in 3499.11s\ttraining loss:\t1.731043\n",
      "Done 3910 batches in 3503.73s\ttraining loss:\t1.730530\n",
      "Done 3915 batches in 3508.13s\ttraining loss:\t1.730221\n",
      "Done 3920 batches in 3512.94s\ttraining loss:\t1.730472\n",
      "Done 3925 batches in 3517.51s\ttraining loss:\t1.730273\n",
      "Done 3930 batches in 3522.56s\ttraining loss:\t1.730118\n",
      "Done 3935 batches in 3526.98s\ttraining loss:\t1.729788\n",
      "Done 3940 batches in 3531.62s\ttraining loss:\t1.730248\n",
      "Done 3945 batches in 3536.38s\ttraining loss:\t1.730334\n",
      "Done 3950 batches in 3541.09s\ttraining loss:\t1.730666\n",
      "Done 3955 batches in 3545.75s\ttraining loss:\t1.730625\n",
      "Done 3960 batches in 3550.10s\ttraining loss:\t1.730566\n",
      "Done 3965 batches in 3554.46s\ttraining loss:\t1.730533\n",
      "Done 3970 batches in 3558.91s\ttraining loss:\t1.730583\n",
      "Done 3975 batches in 3563.09s\ttraining loss:\t1.730645\n",
      "Done 3980 batches in 3567.45s\ttraining loss:\t1.730854\n",
      "Done 3985 batches in 3572.25s\ttraining loss:\t1.730959\n",
      "Done 3990 batches in 3577.06s\ttraining loss:\t1.731395\n",
      "Done 3995 batches in 3581.38s\ttraining loss:\t1.731172\n",
      "Done 4000 batches in 3585.41s\ttraining loss:\t1.731171\n",
      "Done 4005 batches in 3589.64s\ttraining loss:\t1.730904\n",
      "Done 4010 batches in 3593.79s\ttraining loss:\t1.730883\n",
      "Done 4015 batches in 3598.40s\ttraining loss:\t1.730841\n",
      "Done 4020 batches in 3602.97s\ttraining loss:\t1.730945\n",
      "Done 4025 batches in 3606.94s\ttraining loss:\t1.730766\n",
      "Done 4030 batches in 3611.91s\ttraining loss:\t1.730727\n",
      "Done 4035 batches in 3616.78s\ttraining loss:\t1.730704\n",
      "Done 4040 batches in 3620.97s\ttraining loss:\t1.730682\n",
      "Done 4045 batches in 3625.48s\ttraining loss:\t1.731103\n",
      "Done 4050 batches in 3629.94s\ttraining loss:\t1.731016\n",
      "Done 4055 batches in 3634.79s\ttraining loss:\t1.730837\n",
      "Done 4060 batches in 3639.24s\ttraining loss:\t1.730678\n",
      "Done 4065 batches in 3644.07s\ttraining loss:\t1.730443\n",
      "Done 4070 batches in 3648.46s\ttraining loss:\t1.730584\n",
      "Done 4075 batches in 3652.78s\ttraining loss:\t1.730636\n",
      "Done 4080 batches in 3657.19s\ttraining loss:\t1.730617\n",
      "Done 4085 batches in 3661.69s\ttraining loss:\t1.730534\n",
      "Done 4090 batches in 3666.30s\ttraining loss:\t1.730396\n",
      "Done 4095 batches in 3670.99s\ttraining loss:\t1.730650\n",
      "Done 4100 batches in 3675.64s\ttraining loss:\t1.731091\n",
      "Done 4105 batches in 3679.98s\ttraining loss:\t1.731288\n",
      "Done 4110 batches in 3684.57s\ttraining loss:\t1.731104\n",
      "Done 4115 batches in 3688.99s\ttraining loss:\t1.730690\n",
      "Done 4120 batches in 3693.48s\ttraining loss:\t1.730777\n",
      "Done 4125 batches in 3698.38s\ttraining loss:\t1.730662\n",
      "Done 4130 batches in 3703.29s\ttraining loss:\t1.730243\n",
      "Done 4135 batches in 3707.35s\ttraining loss:\t1.730182\n",
      "Done 4140 batches in 3711.84s\ttraining loss:\t1.730047\n",
      "Done 4145 batches in 3716.03s\ttraining loss:\t1.730062\n",
      "Done 4150 batches in 3720.49s\ttraining loss:\t1.730589\n",
      "Done 4155 batches in 3724.52s\ttraining loss:\t1.730214\n",
      "Done 4160 batches in 3728.73s\ttraining loss:\t1.730418\n",
      "Done 4165 batches in 3733.05s\ttraining loss:\t1.730341\n",
      "Done 4170 batches in 3737.63s\ttraining loss:\t1.730549\n",
      "Done 4175 batches in 3742.08s\ttraining loss:\t1.730905\n",
      "Done 4180 batches in 3746.56s\ttraining loss:\t1.730927\n",
      "Done 4185 batches in 3750.83s\ttraining loss:\t1.730850\n",
      "Done 4190 batches in 3755.30s\ttraining loss:\t1.730551\n",
      "Done 4195 batches in 3759.58s\ttraining loss:\t1.730626\n",
      "Done 4200 batches in 3764.31s\ttraining loss:\t1.730458\n",
      "Done 4205 batches in 3769.07s\ttraining loss:\t1.730579\n",
      "Done 4210 batches in 3773.31s\ttraining loss:\t1.730273\n",
      "Done 4215 batches in 3777.63s\ttraining loss:\t1.730373\n",
      "Done 4220 batches in 3781.87s\ttraining loss:\t1.730344\n",
      "Done 4225 batches in 3785.89s\ttraining loss:\t1.730622\n",
      "Done 4230 batches in 3790.39s\ttraining loss:\t1.731224\n",
      "Done 4235 batches in 3794.80s\ttraining loss:\t1.731262\n",
      "Done 4240 batches in 3799.38s\ttraining loss:\t1.731941\n",
      "Done 4245 batches in 3803.92s\ttraining loss:\t1.732053\n",
      "Done 4250 batches in 3808.23s\ttraining loss:\t1.731834\n",
      "Done 4255 batches in 3812.86s\ttraining loss:\t1.731481\n",
      "Done 4260 batches in 3817.29s\ttraining loss:\t1.731332\n",
      "Done 4265 batches in 3821.67s\ttraining loss:\t1.731334\n",
      "Done 4270 batches in 3826.33s\ttraining loss:\t1.731287\n",
      "Done 4275 batches in 3830.66s\ttraining loss:\t1.731554\n",
      "Done 4280 batches in 3835.17s\ttraining loss:\t1.731641\n",
      "Done 4285 batches in 3839.63s\ttraining loss:\t1.731929\n",
      "Done 4290 batches in 3844.67s\ttraining loss:\t1.731576\n",
      "Done 4295 batches in 3849.57s\ttraining loss:\t1.731594\n",
      "Done 4300 batches in 3853.63s\ttraining loss:\t1.731651\n",
      "Done 4305 batches in 3858.54s\ttraining loss:\t1.731546\n",
      "Done 4310 batches in 3863.51s\ttraining loss:\t1.731476\n",
      "Done 4315 batches in 3867.53s\ttraining loss:\t1.731571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.7314470210038153"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3 ep\n",
    "\n",
    "# dane s przycite do dugoci 300 (jeli odpowied si nie mieci, to pytanie jest usuwane z danych)\n",
    "# przycito okoo 1400 prbek, usunito 119\n",
    "\n",
    "# gdzie si da: glove init, reszta: losowo, trenujemy wszystkie sowa\n",
    "\n",
    "qa_net.train_one_epoch(data, 20, log_interval=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QANet tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_spans(data, beam=10, batch_size=10):\n",
    "    num_examples = len(data[0])\n",
    "    \n",
    "    start_probs = qa_net.get_start_probs(data, batch_size)\n",
    "    best_starts = start_probs.argpartition(-beam, axis=1)[:, -beam:].astype(np.int32)\n",
    "    \n",
    "    scores = start_probs[np.arange(num_examples)[:, np.newaxis], best_starts]\n",
    "    scores = np.tile(scores[:, np.newaxis], (beam, 1)).transpose(0, 2, 1)\n",
    "    \n",
    "    best_ends_all = []\n",
    "    for i in xrange(beam):\n",
    "        end_probs = qa_net.get_end_probs(data, best_starts[:, i], batch_size)\n",
    "        best_ends = end_probs.argpartition(-beam, axis=1)[:, -beam:]\n",
    "        scores[:, i, :] *= end_probs[np.arange(num_examples)[:, np.newaxis], best_ends]\n",
    "        best_ends_all.append(best_ends)\n",
    "        \n",
    "    best_ends_all = np.hstack(best_ends_all)\n",
    "        \n",
    "    scores = scores.reshape(num_examples, beam**2)\n",
    "    best_spans = scores.argmax(axis=1)\n",
    "    starts = [i / beam for i in best_spans]\n",
    "    \n",
    "    starts = best_starts[np.arange(num_examples), starts]\n",
    "    ends = best_ends_all[np.arange(num_examples), best_spans]\n",
    "    \n",
    "    return starts, ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 examples\n",
      "200 examples\n",
      "300 examples\n",
      "400 examples\n",
      "500 examples\n",
      "600 examples\n",
      "700 examples\n",
      "800 examples\n",
      "900 examples\n",
      "1000 examples\n",
      "1100 examples\n",
      "1200 examples\n",
      "1300 examples\n",
      "1400 examples\n",
      "1500 examples\n",
      "1600 examples\n",
      "1700 examples\n",
      "1800 examples\n",
      "1900 examples\n",
      "2000 examples\n",
      "2100 examples\n",
      "2200 examples\n",
      "2300 examples\n",
      "2400 examples\n",
      "2500 examples\n",
      "2600 examples\n",
      "2700 examples\n",
      "2800 examples\n",
      "2900 examples\n",
      "3000 examples\n",
      "3100 examples\n",
      "3200 examples\n",
      "3300 examples\n",
      "3400 examples\n",
      "3500 examples\n",
      "3600 examples\n",
      "3700 examples\n",
      "3800 examples\n",
      "3900 examples\n",
      "4000 examples\n",
      "4100 examples\n",
      "4200 examples\n",
      "4300 examples\n",
      "4400 examples\n",
      "4500 examples\n",
      "4600 examples\n",
      "4700 examples\n",
      "4800 examples\n",
      "4900 examples\n",
      "5000 examples\n",
      "5100 examples\n",
      "5200 examples\n",
      "5300 examples\n",
      "5400 examples\n",
      "5500 examples\n",
      "5600 examples\n",
      "5700 examples\n",
      "5800 examples\n",
      "5900 examples\n",
      "6000 examples\n",
      "6100 examples\n",
      "6200 examples\n",
      "6300 examples\n",
      "6400 examples\n",
      "6500 examples\n",
      "6600 examples\n",
      "6700 examples\n",
      "6800 examples\n",
      "6900 examples\n",
      "7000 examples\n",
      "7100 examples\n",
      "7200 examples\n",
      "7300 examples\n",
      "7400 examples\n",
      "7500 examples\n",
      "7600 examples\n",
      "7700 examples\n",
      "7800 examples\n",
      "7900 examples\n",
      "8000 examples\n",
      "8100 examples\n",
      "8200 examples\n",
      "8300 examples\n",
      "8400 examples\n",
      "8500 examples\n",
      "8600 examples\n",
      "8700 examples\n",
      "8800 examples\n",
      "8900 examples\n",
      "9000 examples\n",
      "9100 examples\n",
      "9200 examples\n",
      "9300 examples\n",
      "9400 examples\n",
      "9500 examples\n",
      "9600 examples\n",
      "9700 examples\n",
      "9800 examples\n",
      "9900 examples\n",
      "10000 examples\n",
      "10100 examples\n",
      "10200 examples\n",
      "10300 examples\n",
      "10400 examples\n",
      "10500 examples\n",
      "Predictions done\n",
      "CPU times: user 1h 21min 46s, sys: 3h 27min 21s, total: 4h 49min 7s\n",
      "Wall time: 24min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted_spans = []\n",
    "batch_size = 10\n",
    "\n",
    "idx = 0\n",
    "while idx < len(data_dev):\n",
    "    spans = predict_spans((data_dev[idx:idx + batch_size], data_dev_char[idx:idx + batch_size]))\n",
    "    predicted_spans.append(np.vstack(spans))\n",
    "    idx += batch_size\n",
    "    if not idx % 100:\n",
    "        print idx, 'examples'\n",
    "        \n",
    "print 'Predictions done'\n",
    "    \n",
    "predicted_spans = np.hstack(predicted_spans).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' 4ep {\"f1\": 46.193391429490234, \"exact_match\": 31.901608325449384}'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.savez('evaluate/dev_with_training_vocab_predictions_charemb_fixed_2ep', predicted_spans)\n",
    "\n",
    "''' 1ep {\"f1\": 47.51399403406588, \"exact_match\": 34.24787133396405}'''\n",
    "''' 3ep {\"f1\": 47.31910768176034, \"exact_match\": 32.913907284768214}'''\n",
    "''' 4ep {\"f1\": 46.193391429490234, \"exact_match\": 31.901608325449384}'''\n",
    "\n",
    "# ??? co jest nie tak, powinno by lepiej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.48 s, sys: 9.56 s, total: 15 s\n",
      "Wall time: 1.27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 20)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' poprzednia wersja:\n",
    "3ep {\"f1\": 58.19603067603314, \"exact_match\": 47.74834437086093}\n",
    "1ep {\"f1\": 57.45056892138822, \"exact_match\": 47.34153263954588}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HRED v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building the model...\n",
      "Compiling theano functions...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "hred_net = HRED(voc_size=voc_size,\n",
    "                emb_size=300,\n",
    "                lv1_rec_size=300,\n",
    "                lv2_rec_size=300,\n",
    "                out_emb_size=300,\n",
    "                emb_init=glove_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.6484955 ,  0.39728042,  0.29447019,  0.21638568,  0.34087649,\n",
       "         0.42938796,  0.48248306,  0.45450044,  0.63264418], dtype=float32),\n",
       " array([ 0.43808964,  0.31143361,  0.32550314,  0.41264656,  0.40189919,\n",
       "         0.67630386,  0.55489218,  0.2793248 ,  0.29060629,  0.52473378,\n",
       "         0.40135068,  0.42928901,  0.24922398,  0.29269892,  0.26922536,\n",
       "         0.37188664], dtype=float32),\n",
       " array([ 0.48105857,  0.35441753,  0.33231139,  0.33165267,  0.25691301,\n",
       "         0.29037777,  0.38888532,  0.45136821,  0.26165053,  0.35437146,\n",
       "         0.36461809,  0.29422539,  0.29016727,  0.75232959,  0.50326639,\n",
       "         0.4879126 ,  0.19824371,  0.41686958,  0.44867724,  0.63904244,\n",
       "         0.59564072,  0.3762525 ,  0.55565   ,  0.20368919,  0.45814279,\n",
       "         0.13423204,  0.25729677,  0.28506696,  0.130806  ,  0.43678874], dtype=float32),\n",
       " array([ 0.36110905,  0.18258747,  0.213524  ,  0.28226432,  0.41397128,\n",
       "         0.32674885,  0.29788589,  0.61358768,  0.34168947,  0.22745501,\n",
       "         0.51893896,  0.23157899,  0.34638187], dtype=float32),\n",
       " array([ 0.4535464 ,  0.40573937,  0.2589761 ,  0.60236984,  0.27459544,\n",
       "         0.28887132,  0.41115206,  0.3765592 ,  0.29957584,  0.65952957,\n",
       "         0.65185702,  0.43861535,  0.41025203,  0.34527314,  0.50343114,\n",
       "         0.44476756], dtype=float32),\n",
       " array([ 0.25122422,  0.22101936,  0.24613954,  0.55296057,  0.48097202,\n",
       "         0.28318909,  0.36703086,  0.37572479,  0.13140583,  0.30395606,\n",
       "         0.22432627,  0.41698629,  0.31270713,  0.38566667,  0.34730825,\n",
       "         0.62415415,  0.41321987,  0.25133049,  0.53575897,  0.26159209,\n",
       "         0.43648791,  0.4528788 ,  0.681656  ,  0.56678104], dtype=float32),\n",
       " array([ 0.36841837,  0.21861434,  0.35582578,  0.34456691,  0.26033381,\n",
       "         0.26834723,  0.30452481,  0.29360229,  0.45077142,  0.36123395,\n",
       "         0.30142045,  0.47159186,  0.57626271,  0.47261915,  0.61174881,\n",
       "         0.5398609 ,  0.66426522,  0.60254252,  0.50124556,  0.29331216,\n",
       "         0.66241181,  0.56065774,  0.31302401,  0.34609857,  0.25346887,\n",
       "         0.28972301,  0.40852475,  0.43229055,  0.26449507,  0.49642214,\n",
       "         0.35869539,  0.39115593,  0.27490386,  0.39772668], dtype=float32)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans = hred_net.get_output(data[:12], 3)\n",
    "ans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05232447"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(chain(*ans[8]))[46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'september'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_w(list(chain(*data[8][1][1:]))[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46,\n",
       " 23,\n",
       " 45,\n",
       " 74,\n",
       " 7,\n",
       " 19,\n",
       " 49,\n",
       " 8,\n",
       " 6,\n",
       " 126,\n",
       " 10,\n",
       " 24,\n",
       " 28,\n",
       " 80,\n",
       " 9,\n",
       " 29,\n",
       " 54,\n",
       " 43,\n",
       " 31,\n",
       " 37,\n",
       " 4,\n",
       " 42,\n",
       " 123,\n",
       " 82,\n",
       " 11,\n",
       " 30,\n",
       " 111,\n",
       " 94,\n",
       " 16,\n",
       " 35,\n",
       " 81,\n",
       " 15,\n",
       " 50,\n",
       " 148,\n",
       " 62,\n",
       " 78,\n",
       " 53,\n",
       " 91,\n",
       " 204,\n",
       " 33,\n",
       " 13,\n",
       " 12,\n",
       " 105,\n",
       " 21,\n",
       " 20,\n",
       " 155,\n",
       " 152,\n",
       " 122,\n",
       " 36,\n",
       " 158,\n",
       " 125,\n",
       " 203,\n",
       " 217,\n",
       " 5,\n",
       " 22,\n",
       " 84,\n",
       " 14,\n",
       " 87,\n",
       " 27,\n",
       " 34,\n",
       " 3,\n",
       " 85,\n",
       " 107,\n",
       " 222,\n",
       " 186,\n",
       " 79,\n",
       " 25,\n",
       " 110,\n",
       " 128,\n",
       " 56,\n",
       " 175,\n",
       " 60,\n",
       " 127,\n",
       " 131,\n",
       " 52,\n",
       " 117,\n",
       " 63,\n",
       " 149,\n",
       " 112,\n",
       " 47,\n",
       " 26,\n",
       " 66,\n",
       " 55,\n",
       " 2,\n",
       " 32,\n",
       " 86,\n",
       " 197,\n",
       " 171,\n",
       " 151,\n",
       " 61,\n",
       " 108,\n",
       " 83,\n",
       " 137,\n",
       " 119,\n",
       " 124,\n",
       " 48,\n",
       " 67,\n",
       " 166,\n",
       " 201,\n",
       " 134,\n",
       " 239,\n",
       " 1,\n",
       " 172,\n",
       " 44,\n",
       " 236,\n",
       " 90,\n",
       " 233,\n",
       " 106,\n",
       " 77,\n",
       " 240,\n",
       " 104,\n",
       " 176,\n",
       " 71,\n",
       " 141,\n",
       " 75,\n",
       " 0,\n",
       " 227,\n",
       " 142,\n",
       " 101,\n",
       " 109,\n",
       " 198,\n",
       " 147,\n",
       " 202,\n",
       " 154,\n",
       " 73,\n",
       " 207,\n",
       " 18,\n",
       " 235,\n",
       " 226,\n",
       " 93,\n",
       " 65,\n",
       " 59,\n",
       " 150,\n",
       " 120,\n",
       " 194,\n",
       " 113,\n",
       " 195,\n",
       " 205,\n",
       " 121,\n",
       " 118,\n",
       " 153,\n",
       " 57,\n",
       " 232,\n",
       " 39,\n",
       " 17,\n",
       " 216,\n",
       " 132,\n",
       " 41,\n",
       " 140,\n",
       " 247,\n",
       " 116,\n",
       " 178,\n",
       " 64,\n",
       " 97,\n",
       " 179,\n",
       " 100,\n",
       " 58,\n",
       " 224,\n",
       " 76,\n",
       " 170,\n",
       " 70,\n",
       " 69,\n",
       " 51,\n",
       " 243,\n",
       " 103,\n",
       " 133,\n",
       " 169,\n",
       " 136,\n",
       " 218,\n",
       " 237,\n",
       " 238,\n",
       " 143,\n",
       " 72,\n",
       " 174,\n",
       " 167,\n",
       " 102,\n",
       " 225,\n",
       " 177,\n",
       " 200,\n",
       " 89,\n",
       " 181,\n",
       " 114,\n",
       " 146,\n",
       " 206,\n",
       " 144,\n",
       " 221,\n",
       " 173,\n",
       " 38,\n",
       " 139,\n",
       " 92,\n",
       " 99,\n",
       " 190,\n",
       " 242,\n",
       " 135,\n",
       " 193,\n",
       " 165,\n",
       " 191,\n",
       " 162,\n",
       " 163,\n",
       " 40,\n",
       " 168,\n",
       " 145,\n",
       " 130,\n",
       " 196,\n",
       " 183,\n",
       " 96,\n",
       " 199,\n",
       " 187,\n",
       " 234,\n",
       " 115,\n",
       " 180,\n",
       " 241,\n",
       " 95,\n",
       " 212,\n",
       " 244,\n",
       " 245,\n",
       " 156,\n",
       " 138,\n",
       " 246,\n",
       " 229,\n",
       " 210,\n",
       " 159,\n",
       " 189,\n",
       " 220,\n",
       " 188,\n",
       " 161,\n",
       " 88,\n",
       " 219,\n",
       " 223,\n",
       " 98,\n",
       " 160,\n",
       " 68,\n",
       " 215,\n",
       " 209,\n",
       " 228,\n",
       " 192,\n",
       " 129,\n",
       " 164,\n",
       " 213,\n",
       " 185,\n",
       " 184,\n",
       " 214,\n",
       " 231,\n",
       " 157,\n",
       " 230,\n",
       " 182,\n",
       " 208,\n",
       " 248,\n",
       " 211]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(reversed(np.array(list(chain(*ans[8]))).argsort()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Zwyky trening nie robi nic: wartoci zwracane przez sie dla zbioru uczcego nie miay adnego sensu.\n",
    "# Najwiksze prawdopodobiestwa byy prawie niezalene od pytania, miao ono may wpyw na to, co si dzieje.\n",
    "# Dla kosztu weighted_bin_ce mocno skrzywionego w stron sytuacji t=1 wyszo podobnie.\n",
    "#\n",
    "# Nastpny krok: uy zanurze GloVe, doda cechy z https://arxiv.org/abs/1703.04816\n",
    "#\n",
    "# UPDATE:\n",
    "# Obie cechy podobiestwa kontekstu do pytania dodane.\n",
    "# W glove.6B jest okoo 70% sw ze zbioru uczcego SQuAD.\n",
    "# W tej chwili te wektory bior z glove.6B.300d, a pozostaym daj losowy init, \n",
    "# po czym wszystkie zanurzenia s dalej uczone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 20 batches in 7.96s\ttraining loss:\t0.813393\n",
      "Done 40 batches in 16.89s\ttraining loss:\t0.618499\n",
      "Done 60 batches in 25.51s\ttraining loss:\t0.537964\n",
      "Done 80 batches in 33.72s\ttraining loss:\t0.476587\n",
      "Done 100 batches in 41.79s\ttraining loss:\t0.436442\n",
      "Done 120 batches in 48.56s\ttraining loss:\t0.419979\n",
      "Done 140 batches in 57.10s\ttraining loss:\t0.409618\n",
      "Done 160 batches in 65.07s\ttraining loss:\t0.408088\n",
      "Done 180 batches in 74.05s\ttraining loss:\t0.397444\n",
      "Done 200 batches in 81.68s\ttraining loss:\t0.401948\n",
      "Done 220 batches in 90.27s\ttraining loss:\t0.401396\n",
      "Done 240 batches in 98.95s\ttraining loss:\t0.410639\n",
      "Done 260 batches in 107.43s\ttraining loss:\t0.418680\n",
      "Done 280 batches in 115.18s\ttraining loss:\t0.427639\n",
      "Done 300 batches in 123.33s\ttraining loss:\t0.426674\n",
      "Done 320 batches in 131.00s\ttraining loss:\t0.422292\n",
      "Done 340 batches in 139.84s\ttraining loss:\t0.424456\n",
      "Done 360 batches in 148.91s\ttraining loss:\t0.415105\n",
      "Done 380 batches in 156.67s\ttraining loss:\t0.416838\n",
      "Done 400 batches in 163.90s\ttraining loss:\t0.418870\n",
      "Done 420 batches in 171.42s\ttraining loss:\t0.419026\n",
      "Done 440 batches in 180.23s\ttraining loss:\t0.417331\n",
      "Done 460 batches in 189.66s\ttraining loss:\t0.419791\n",
      "Done 480 batches in 198.85s\ttraining loss:\t0.420404\n",
      "Done 500 batches in 206.97s\ttraining loss:\t0.416177\n",
      "Done 520 batches in 214.49s\ttraining loss:\t0.415342\n",
      "Done 540 batches in 222.22s\ttraining loss:\t0.416872\n",
      "Done 560 batches in 229.79s\ttraining loss:\t0.416093\n",
      "Done 580 batches in 237.12s\ttraining loss:\t0.408871\n",
      "Done 600 batches in 244.83s\ttraining loss:\t0.408386\n",
      "Done 620 batches in 252.93s\ttraining loss:\t0.406717\n",
      "Done 640 batches in 260.90s\ttraining loss:\t0.407531\n",
      "Done 660 batches in 269.79s\ttraining loss:\t0.405836\n",
      "Done 680 batches in 277.40s\ttraining loss:\t0.407107\n",
      "Done 700 batches in 284.92s\ttraining loss:\t0.408122\n",
      "Done 720 batches in 292.42s\ttraining loss:\t0.411832\n",
      "Done 740 batches in 300.17s\ttraining loss:\t0.412702\n",
      "Done 760 batches in 308.85s\ttraining loss:\t0.413090\n",
      "Done 780 batches in 317.01s\ttraining loss:\t0.412683\n",
      "Done 800 batches in 325.79s\ttraining loss:\t0.411818\n",
      "Done 820 batches in 334.14s\ttraining loss:\t0.410459\n",
      "Done 840 batches in 343.48s\ttraining loss:\t0.407757\n",
      "Done 860 batches in 351.91s\ttraining loss:\t0.407190\n",
      "Done 880 batches in 358.82s\ttraining loss:\t0.406966\n",
      "Done 900 batches in 366.59s\ttraining loss:\t0.405027\n",
      "Done 920 batches in 375.56s\ttraining loss:\t0.404316\n",
      "Done 940 batches in 383.60s\ttraining loss:\t0.402061\n",
      "Done 960 batches in 392.46s\ttraining loss:\t0.399332\n",
      "Done 980 batches in 400.47s\ttraining loss:\t0.404043\n",
      "Done 1000 batches in 407.74s\ttraining loss:\t0.409821\n",
      "Done 1020 batches in 415.21s\ttraining loss:\t0.416582\n",
      "Done 1040 batches in 422.79s\ttraining loss:\t0.422596\n",
      "Done 1060 batches in 431.41s\ttraining loss:\t0.423714\n",
      "Done 1080 batches in 439.96s\ttraining loss:\t0.424034\n",
      "Done 1100 batches in 448.43s\ttraining loss:\t0.427439\n",
      "Done 1120 batches in 457.71s\ttraining loss:\t0.429270\n",
      "Done 1140 batches in 466.58s\ttraining loss:\t0.433387\n",
      "Done 1160 batches in 475.75s\ttraining loss:\t0.436254\n",
      "Done 1180 batches in 482.82s\ttraining loss:\t0.441358\n",
      "Done 1200 batches in 491.42s\ttraining loss:\t0.438379\n",
      "Done 1220 batches in 499.96s\ttraining loss:\t0.435319\n",
      "Done 1240 batches in 508.77s\ttraining loss:\t0.434273\n",
      "Done 1260 batches in 517.57s\ttraining loss:\t0.433936\n",
      "Done 1280 batches in 526.22s\ttraining loss:\t0.436965\n",
      "Done 1300 batches in 533.97s\ttraining loss:\t0.438821\n",
      "Done 1320 batches in 541.44s\ttraining loss:\t0.439510\n",
      "Done 1340 batches in 549.06s\ttraining loss:\t0.438974\n",
      "Done 1360 batches in 558.08s\ttraining loss:\t0.437469\n",
      "Done 1380 batches in 566.40s\ttraining loss:\t0.437325\n",
      "Done 1400 batches in 574.89s\ttraining loss:\t0.435750\n",
      "Done 1420 batches in 582.96s\ttraining loss:\t0.433850\n",
      "Done 1440 batches in 590.76s\ttraining loss:\t0.434015\n",
      "Done 1460 batches in 598.12s\ttraining loss:\t0.434544\n",
      "Done 1480 batches in 604.34s\ttraining loss:\t0.434400\n",
      "Done 1500 batches in 611.19s\ttraining loss:\t0.434535\n",
      "Done 1520 batches in 619.96s\ttraining loss:\t0.432634\n",
      "Done 1540 batches in 628.15s\ttraining loss:\t0.431123\n",
      "Done 1560 batches in 636.14s\ttraining loss:\t0.430235\n",
      "Done 1580 batches in 644.25s\ttraining loss:\t0.429545\n",
      "Done 1600 batches in 652.91s\ttraining loss:\t0.429710\n",
      "Done 1620 batches in 659.96s\ttraining loss:\t0.429660\n",
      "Done 1640 batches in 667.16s\ttraining loss:\t0.428866\n",
      "Done 1660 batches in 675.28s\ttraining loss:\t0.425995\n",
      "Done 1680 batches in 683.37s\ttraining loss:\t0.424745\n",
      "Done 1700 batches in 692.16s\ttraining loss:\t0.422782\n",
      "Done 1720 batches in 700.82s\ttraining loss:\t0.423887\n",
      "Done 1740 batches in 709.13s\ttraining loss:\t0.430753\n",
      "Done 1760 batches in 719.00s\ttraining loss:\t0.434818\n",
      "Done 1780 batches in 726.48s\ttraining loss:\t0.441409\n",
      "Done 1800 batches in 734.22s\ttraining loss:\t0.441740\n",
      "Done 1820 batches in 742.53s\ttraining loss:\t0.441645\n",
      "Done 1840 batches in 752.19s\ttraining loss:\t0.441036\n",
      "Done 1860 batches in 760.79s\ttraining loss:\t0.440892\n",
      "Done 1880 batches in 768.95s\ttraining loss:\t0.439639\n",
      "Done 1900 batches in 776.61s\ttraining loss:\t0.439933\n",
      "Done 1920 batches in 784.30s\ttraining loss:\t0.439975\n",
      "Done 1940 batches in 790.83s\ttraining loss:\t0.440051\n",
      "Done 1960 batches in 799.31s\ttraining loss:\t0.439937\n",
      "Done 1980 batches in 807.31s\ttraining loss:\t0.439576\n",
      "Done 2000 batches in 815.80s\ttraining loss:\t0.439899\n",
      "Done 2020 batches in 823.95s\ttraining loss:\t0.441725\n",
      "Done 2040 batches in 832.07s\ttraining loss:\t0.442261\n",
      "Done 2060 batches in 841.26s\ttraining loss:\t0.443953\n",
      "Done 2080 batches in 849.59s\ttraining loss:\t0.445274\n",
      "Done 2100 batches in 857.71s\ttraining loss:\t0.445195\n",
      "Done 2120 batches in 865.53s\ttraining loss:\t0.444672\n",
      "Done 2140 batches in 873.78s\ttraining loss:\t0.443986\n",
      "Done 2160 batches in 882.04s\ttraining loss:\t0.444449\n",
      "Done 2180 batches in 889.96s\ttraining loss:\t0.445268\n",
      "Done 2200 batches in 898.35s\ttraining loss:\t0.445640\n",
      "Done 2220 batches in 905.98s\ttraining loss:\t0.446017\n",
      "Done 2240 batches in 912.56s\ttraining loss:\t0.446291\n",
      "Done 2260 batches in 920.56s\ttraining loss:\t0.444988\n",
      "Done 2280 batches in 929.12s\ttraining loss:\t0.443758\n",
      "Done 2300 batches in 937.52s\ttraining loss:\t0.442648\n",
      "Done 2320 batches in 945.58s\ttraining loss:\t0.442092\n",
      "Done 2340 batches in 953.16s\ttraining loss:\t0.441642\n",
      "Done 2360 batches in 960.39s\ttraining loss:\t0.441188\n",
      "Done 2380 batches in 968.62s\ttraining loss:\t0.440855\n",
      "Done 2400 batches in 976.90s\ttraining loss:\t0.440366\n",
      "Done 2420 batches in 985.54s\ttraining loss:\t0.440122\n",
      "Done 2440 batches in 994.25s\ttraining loss:\t0.439948\n",
      "Done 2460 batches in 1002.95s\ttraining loss:\t0.439623\n",
      "Done 2480 batches in 1011.30s\ttraining loss:\t0.438941\n",
      "Done 2500 batches in 1018.73s\ttraining loss:\t0.438774\n",
      "Done 2520 batches in 1027.22s\ttraining loss:\t0.438608\n",
      "Done 2540 batches in 1034.78s\ttraining loss:\t0.438557\n",
      "Done 2560 batches in 1042.20s\ttraining loss:\t0.438424\n",
      "Done 2580 batches in 1051.12s\ttraining loss:\t0.438551\n",
      "Done 2600 batches in 1058.51s\ttraining loss:\t0.438934\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-64ada69a1fb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhred_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_trimmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/pio/scratch/1/i258346/masters_thesis/SQuAD/HRED_v2.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(self, train_data, batch_size, log_interval)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mnum_batch_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_mask\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mnum_training_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hred_net.train_one_epoch(data_trimmed, 5, log_interval=20) # weighted cost, random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 20 batches in 2.25s\ttraining loss:\t0.228349\n",
      "Done 40 batches in 5.03s\ttraining loss:\t0.154742\n",
      "Done 60 batches in 7.65s\ttraining loss:\t0.126147\n",
      "Done 80 batches in 10.05s\ttraining loss:\t0.106807\n",
      "Done 100 batches in 12.52s\ttraining loss:\t0.095184\n",
      "Done 120 batches in 14.64s\ttraining loss:\t0.090502\n",
      "Done 140 batches in 17.25s\ttraining loss:\t0.087078\n",
      "Done 160 batches in 19.51s\ttraining loss:\t0.085557\n",
      "Done 180 batches in 22.31s\ttraining loss:\t0.082334\n",
      "Done 200 batches in 24.40s\ttraining loss:\t0.082256\n",
      "Done 220 batches in 26.96s\ttraining loss:\t0.081215\n",
      "Done 240 batches in 29.60s\ttraining loss:\t0.082376\n",
      "Done 260 batches in 32.51s\ttraining loss:\t0.083784\n",
      "Done 280 batches in 34.97s\ttraining loss:\t0.084956\n",
      "Done 300 batches in 37.36s\ttraining loss:\t0.084330\n",
      "Done 320 batches in 39.52s\ttraining loss:\t0.083391\n",
      "Done 340 batches in 42.24s\ttraining loss:\t0.083563\n",
      "Done 360 batches in 44.99s\ttraining loss:\t0.081207\n",
      "Done 380 batches in 47.14s\ttraining loss:\t0.081512\n",
      "Done 400 batches in 49.09s\ttraining loss:\t0.081817\n",
      "Done 420 batches in 51.60s\ttraining loss:\t0.081576\n",
      "Done 440 batches in 54.40s\ttraining loss:\t0.081043\n",
      "Done 460 batches in 57.43s\ttraining loss:\t0.081339\n",
      "Done 480 batches in 60.30s\ttraining loss:\t0.081267\n",
      "Done 500 batches in 62.65s\ttraining loss:\t0.080241\n",
      "Done 520 batches in 64.71s\ttraining loss:\t0.079872\n",
      "Done 540 batches in 66.90s\ttraining loss:\t0.080126\n",
      "Done 560 batches in 69.28s\ttraining loss:\t0.079805\n",
      "Done 580 batches in 71.59s\ttraining loss:\t0.078206\n",
      "Done 600 batches in 73.70s\ttraining loss:\t0.078006\n",
      "Done 620 batches in 76.02s\ttraining loss:\t0.077540\n",
      "Done 640 batches in 78.29s\ttraining loss:\t0.077661\n",
      "Done 660 batches in 81.00s\ttraining loss:\t0.077267\n",
      "Done 680 batches in 83.10s\ttraining loss:\t0.077439\n",
      "Done 700 batches in 85.16s\ttraining loss:\t0.077608\n",
      "Done 720 batches in 87.61s\ttraining loss:\t0.078403\n",
      "Done 740 batches in 90.07s\ttraining loss:\t0.078511\n",
      "Done 760 batches in 92.63s\ttraining loss:\t0.078503\n",
      "Done 780 batches in 94.92s\ttraining loss:\t0.078355\n",
      "Done 800 batches in 97.58s\ttraining loss:\t0.078138\n",
      "Done 820 batches in 100.06s\ttraining loss:\t0.077781\n",
      "Done 840 batches in 103.01s\ttraining loss:\t0.077201\n",
      "Done 860 batches in 105.70s\ttraining loss:\t0.077054\n",
      "Done 880 batches in 107.92s\ttraining loss:\t0.076950\n",
      "Done 900 batches in 110.14s\ttraining loss:\t0.076493\n",
      "Done 920 batches in 112.94s\ttraining loss:\t0.076241\n",
      "Done 940 batches in 115.26s\ttraining loss:\t0.075677\n",
      "Done 960 batches in 117.99s\ttraining loss:\t0.075053\n",
      "Done 980 batches in 120.26s\ttraining loss:\t0.076049\n",
      "Done 1000 batches in 122.23s\ttraining loss:\t0.077339\n",
      "Done 1020 batches in 124.55s\ttraining loss:\t0.078867\n",
      "Done 1040 batches in 127.04s\ttraining loss:\t0.080173\n",
      "Done 1060 batches in 129.60s\ttraining loss:\t0.080350\n",
      "Done 1080 batches in 132.17s\ttraining loss:\t0.080329\n",
      "Done 1100 batches in 134.68s\ttraining loss:\t0.081008\n",
      "Done 1120 batches in 137.63s\ttraining loss:\t0.081333\n",
      "Done 1140 batches in 140.35s\ttraining loss:\t0.082265\n",
      "Done 1160 batches in 143.38s\ttraining loss:\t0.082898\n",
      "Done 1180 batches in 145.65s\ttraining loss:\t0.083972\n",
      "Done 1200 batches in 148.24s\ttraining loss:\t0.083225\n",
      "Done 1220 batches in 150.71s\ttraining loss:\t0.082501\n",
      "Done 1240 batches in 153.33s\ttraining loss:\t0.082238\n",
      "Done 1260 batches in 155.95s\ttraining loss:\t0.082117\n",
      "Done 1280 batches in 158.54s\ttraining loss:\t0.082770\n",
      "Done 1300 batches in 160.68s\ttraining loss:\t0.083157\n",
      "Done 1320 batches in 163.10s\ttraining loss:\t0.083262\n",
      "Done 1340 batches in 165.49s\ttraining loss:\t0.083107\n",
      "Done 1360 batches in 168.28s\ttraining loss:\t0.082768\n",
      "Done 1380 batches in 170.73s\ttraining loss:\t0.082702\n",
      "Done 1400 batches in 173.25s\ttraining loss:\t0.082357\n",
      "Done 1420 batches in 175.58s\ttraining loss:\t0.081953\n",
      "Done 1440 batches in 177.77s\ttraining loss:\t0.082008\n",
      "Done 1460 batches in 179.76s\ttraining loss:\t0.082140\n",
      "Done 1480 batches in 181.61s\ttraining loss:\t0.082130\n",
      "Done 1500 batches in 183.58s\ttraining loss:\t0.082174\n",
      "Done 1520 batches in 186.24s\ttraining loss:\t0.081775\n",
      "Done 1540 batches in 188.61s\ttraining loss:\t0.081439\n",
      "Done 1560 batches in 190.90s\ttraining loss:\t0.081231\n",
      "Done 1580 batches in 193.30s\ttraining loss:\t0.081040\n",
      "Done 1600 batches in 195.93s\ttraining loss:\t0.081032\n",
      "Done 1620 batches in 197.83s\ttraining loss:\t0.081020\n",
      "Done 1640 batches in 200.18s\ttraining loss:\t0.080836\n",
      "Done 1660 batches in 202.61s\ttraining loss:\t0.080271\n",
      "Done 1680 batches in 204.94s\ttraining loss:\t0.080008\n",
      "Done 1700 batches in 207.61s\ttraining loss:\t0.079575\n",
      "Done 1720 batches in 210.23s\ttraining loss:\t0.079748\n",
      "Done 1740 batches in 212.68s\ttraining loss:\t0.081393\n",
      "Done 1760 batches in 215.40s\ttraining loss:\t0.082344\n",
      "Done 1780 batches in 217.83s\ttraining loss:\t0.084007\n",
      "Done 1800 batches in 220.25s\ttraining loss:\t0.083921\n",
      "Done 1820 batches in 222.67s\ttraining loss:\t0.083847\n",
      "Done 1840 batches in 225.76s\ttraining loss:\t0.083691\n",
      "Done 1860 batches in 228.37s\ttraining loss:\t0.083606\n",
      "Done 1880 batches in 230.73s\ttraining loss:\t0.083308\n",
      "Done 1900 batches in 232.90s\ttraining loss:\t0.083360\n",
      "Done 1920 batches in 235.19s\ttraining loss:\t0.083349\n",
      "Done 1940 batches in 237.19s\ttraining loss:\t0.083379\n",
      "Done 1960 batches in 239.77s\ttraining loss:\t0.083347\n",
      "Done 1980 batches in 242.10s\ttraining loss:\t0.083264\n",
      "Done 2000 batches in 244.65s\ttraining loss:\t0.083313\n",
      "Done 2020 batches in 246.98s\ttraining loss:\t0.083742\n",
      "Done 2040 batches in 249.30s\ttraining loss:\t0.083832\n",
      "Done 2060 batches in 252.19s\ttraining loss:\t0.084214\n",
      "Done 2080 batches in 255.02s\ttraining loss:\t0.084517\n",
      "Done 2100 batches in 257.63s\ttraining loss:\t0.084477\n",
      "Done 2120 batches in 259.75s\ttraining loss:\t0.084350\n",
      "Done 2140 batches in 262.12s\ttraining loss:\t0.084199\n",
      "Done 2160 batches in 264.48s\ttraining loss:\t0.084294\n",
      "Done 2180 batches in 266.70s\ttraining loss:\t0.084472\n",
      "Done 2200 batches in 269.17s\ttraining loss:\t0.084553\n",
      "Done 2220 batches in 271.34s\ttraining loss:\t0.084627\n",
      "Done 2240 batches in 273.79s\ttraining loss:\t0.084675\n",
      "Done 2260 batches in 276.32s\ttraining loss:\t0.084386\n",
      "Done 2280 batches in 278.86s\ttraining loss:\t0.084116\n",
      "Done 2300 batches in 281.38s\ttraining loss:\t0.083881\n",
      "Done 2320 batches in 283.70s\ttraining loss:\t0.083758\n",
      "Done 2340 batches in 285.80s\ttraining loss:\t0.083667\n",
      "Done 2360 batches in 287.73s\ttraining loss:\t0.083566\n",
      "Done 2380 batches in 290.19s\ttraining loss:\t0.083491\n",
      "Done 2400 batches in 293.14s\ttraining loss:\t0.083378\n",
      "Done 2420 batches in 295.73s\ttraining loss:\t0.083295\n",
      "Done 2440 batches in 298.31s\ttraining loss:\t0.083252\n",
      "Done 2460 batches in 300.87s\ttraining loss:\t0.083187\n",
      "Done 2480 batches in 303.39s\ttraining loss:\t0.083052\n",
      "Done 2500 batches in 305.45s\ttraining loss:\t0.083019\n",
      "Done 2520 batches in 307.97s\ttraining loss:\t0.082983\n",
      "Done 2540 batches in 310.39s\ttraining loss:\t0.082987\n",
      "Done 2560 batches in 312.73s\ttraining loss:\t0.082951\n",
      "Done 2580 batches in 315.43s\ttraining loss:\t0.082987\n",
      "Done 2600 batches in 317.41s\ttraining loss:\t0.083058\n",
      "Done 2620 batches in 319.94s\ttraining loss:\t0.083174\n",
      "Done 2640 batches in 322.48s\ttraining loss:\t0.083268\n",
      "Done 2660 batches in 324.62s\ttraining loss:\t0.083458\n",
      "Done 2680 batches in 326.73s\ttraining loss:\t0.083409\n",
      "Done 2700 batches in 329.01s\ttraining loss:\t0.083184\n",
      "Done 2720 batches in 331.53s\ttraining loss:\t0.082915\n",
      "Done 2740 batches in 334.17s\ttraining loss:\t0.082666\n",
      "Done 2760 batches in 336.83s\ttraining loss:\t0.082375\n",
      "Done 2780 batches in 339.10s\ttraining loss:\t0.082695\n",
      "Done 2800 batches in 341.49s\ttraining loss:\t0.082578\n",
      "Done 2820 batches in 343.95s\ttraining loss:\t0.082545\n",
      "Done 2840 batches in 346.30s\ttraining loss:\t0.082490\n",
      "Done 2860 batches in 348.47s\ttraining loss:\t0.082513\n",
      "Done 2880 batches in 350.55s\ttraining loss:\t0.082629\n",
      "Done 2900 batches in 352.74s\ttraining loss:\t0.082627\n",
      "Done 2920 batches in 355.22s\ttraining loss:\t0.082777\n",
      "Done 2940 batches in 357.38s\ttraining loss:\t0.082989\n",
      "Done 2960 batches in 360.03s\ttraining loss:\t0.082944\n",
      "Done 2980 batches in 362.65s\ttraining loss:\t0.082840\n",
      "Done 3000 batches in 365.10s\ttraining loss:\t0.082781\n",
      "Done 3020 batches in 367.54s\ttraining loss:\t0.082870\n",
      "Done 3040 batches in 369.62s\ttraining loss:\t0.083219\n",
      "Done 3060 batches in 372.31s\ttraining loss:\t0.083346\n",
      "Done 3080 batches in 374.34s\ttraining loss:\t0.083250\n",
      "Done 3100 batches in 376.43s\ttraining loss:\t0.083245\n",
      "Done 3120 batches in 378.61s\ttraining loss:\t0.083298\n",
      "Done 3140 batches in 380.43s\ttraining loss:\t0.083373\n",
      "Done 3160 batches in 382.61s\ttraining loss:\t0.083474\n",
      "Done 3180 batches in 385.21s\ttraining loss:\t0.083542\n",
      "Done 3200 batches in 387.36s\ttraining loss:\t0.083450\n",
      "Done 3220 batches in 389.75s\ttraining loss:\t0.083319\n",
      "Done 3240 batches in 392.17s\ttraining loss:\t0.083259\n",
      "Done 3260 batches in 394.37s\ttraining loss:\t0.083201\n",
      "Done 3280 batches in 397.29s\ttraining loss:\t0.083164\n",
      "Done 3300 batches in 399.99s\ttraining loss:\t0.083116\n",
      "Done 3320 batches in 402.28s\ttraining loss:\t0.083230\n",
      "Done 3340 batches in 404.82s\ttraining loss:\t0.083286\n",
      "Done 3360 batches in 407.30s\ttraining loss:\t0.083341\n",
      "Done 3380 batches in 409.63s\ttraining loss:\t0.083381\n",
      "Done 3400 batches in 411.81s\ttraining loss:\t0.083656\n",
      "Done 3420 batches in 414.28s\ttraining loss:\t0.083676\n",
      "Done 3440 batches in 416.79s\ttraining loss:\t0.083656\n",
      "Done 3460 batches in 419.58s\ttraining loss:\t0.083481\n",
      "Done 3480 batches in 421.96s\ttraining loss:\t0.083374\n",
      "Done 3500 batches in 424.48s\ttraining loss:\t0.083406\n",
      "Done 3520 batches in 426.69s\ttraining loss:\t0.083517\n",
      "Done 3540 batches in 428.92s\ttraining loss:\t0.083598\n",
      "Done 3560 batches in 431.32s\ttraining loss:\t0.083693\n",
      "Done 3580 batches in 433.80s\ttraining loss:\t0.083768\n",
      "Done 3600 batches in 436.10s\ttraining loss:\t0.083722\n",
      "Done 3620 batches in 438.77s\ttraining loss:\t0.083722\n",
      "Done 3640 batches in 441.50s\ttraining loss:\t0.083669\n",
      "Done 3660 batches in 444.50s\ttraining loss:\t0.083569\n",
      "Done 3680 batches in 447.27s\ttraining loss:\t0.083521\n",
      "Done 3700 batches in 449.57s\ttraining loss:\t0.083480\n",
      "Done 3720 batches in 452.27s\ttraining loss:\t0.083580\n",
      "Done 3740 batches in 454.56s\ttraining loss:\t0.083695\n",
      "Done 3760 batches in 456.78s\ttraining loss:\t0.083617\n",
      "Done 3780 batches in 459.11s\ttraining loss:\t0.083555\n",
      "Done 3800 batches in 461.14s\ttraining loss:\t0.083450\n",
      "Done 3820 batches in 463.86s\ttraining loss:\t0.083416\n",
      "Done 3840 batches in 465.90s\ttraining loss:\t0.083386\n",
      "Done 3860 batches in 468.20s\ttraining loss:\t0.083561\n",
      "Done 3880 batches in 470.61s\ttraining loss:\t0.083721\n",
      "Done 3900 batches in 472.78s\ttraining loss:\t0.083661\n",
      "Done 3920 batches in 475.16s\ttraining loss:\t0.083431\n",
      "Done 3940 batches in 477.57s\ttraining loss:\t0.083281\n",
      "Done 3960 batches in 479.76s\ttraining loss:\t0.083151\n",
      "Done 3980 batches in 481.76s\ttraining loss:\t0.083135\n",
      "Done 4000 batches in 483.84s\ttraining loss:\t0.083099\n",
      "Done 4020 batches in 486.14s\ttraining loss:\t0.083242\n",
      "Done 4040 batches in 488.55s\ttraining loss:\t0.083450\n",
      "Done 4060 batches in 491.19s\ttraining loss:\t0.083640\n",
      "Done 4080 batches in 493.71s\ttraining loss:\t0.083431\n",
      "Done 4100 batches in 496.13s\ttraining loss:\t0.083501\n",
      "Done 4120 batches in 498.46s\ttraining loss:\t0.083661\n",
      "Done 4140 batches in 500.75s\ttraining loss:\t0.083733\n",
      "Done 4160 batches in 503.19s\ttraining loss:\t0.083856\n",
      "Done 4180 batches in 505.25s\ttraining loss:\t0.084116\n",
      "Done 4200 batches in 507.40s\ttraining loss:\t0.084245\n",
      "Done 4220 batches in 509.21s\ttraining loss:\t0.084350\n",
      "Done 4240 batches in 510.99s\ttraining loss:\t0.084466\n",
      "Done 4260 batches in 512.63s\ttraining loss:\t0.084566\n",
      "Done 4280 batches in 514.65s\ttraining loss:\t0.084610\n",
      "Done 4300 batches in 516.76s\ttraining loss:\t0.084608\n",
      "Done 4320 batches in 519.22s\ttraining loss:\t0.084525\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-64ada69a1fb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhred_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_trimmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/pio/scratch/1/i258346/masters_thesis/SQuAD/HRED_v2.pyc\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(self, train_data, batch_size, log_interval)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mnum_batch_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 133\u001b[1;33m             \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_mask\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    134\u001b[0m             \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m             \u001b[0mnum_training_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/gof/op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNoParams\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    859\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 860\u001b[1;33m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    861\u001b[0m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    862\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hred_net.train_one_epoch(data_trimmed, 5, log_interval=20) # normal binary cross-entropy with random init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done 20 batches in 2.66s\ttraining loss:\t0.247073\n",
      "Done 40 batches in 5.97s\ttraining loss:\t0.187469\n",
      "Done 60 batches in 9.10s\ttraining loss:\t0.148785\n",
      "Done 80 batches in 11.96s\ttraining loss:\t0.123427\n",
      "Done 100 batches in 14.91s\ttraining loss:\t0.108109\n",
      "Done 120 batches in 17.43s\ttraining loss:\t0.102956\n",
      "Done 140 batches in 20.55s\ttraining loss:\t0.097911\n",
      "Done 160 batches in 23.25s\ttraining loss:\t0.095246\n",
      "Done 180 batches in 26.59s\ttraining loss:\t0.090887\n",
      "Done 200 batches in 30.07s\ttraining loss:\t0.090136\n",
      "Done 220 batches in 33.13s\ttraining loss:\t0.088647\n",
      "Done 240 batches in 36.28s\ttraining loss:\t0.089258\n",
      "Done 260 batches in 39.74s\ttraining loss:\t0.091964\n",
      "Done 280 batches in 42.68s\ttraining loss:\t0.093008\n",
      "Done 300 batches in 45.52s\ttraining loss:\t0.091937\n",
      "Done 320 batches in 48.11s\ttraining loss:\t0.090498\n",
      "Done 340 batches in 51.34s\ttraining loss:\t0.090224\n",
      "Done 360 batches in 54.62s\ttraining loss:\t0.087337\n",
      "Done 380 batches in 57.19s\ttraining loss:\t0.087323\n",
      "Done 400 batches in 59.53s\ttraining loss:\t0.087407\n",
      "Done 420 batches in 62.52s\ttraining loss:\t0.086922\n",
      "Done 440 batches in 65.86s\ttraining loss:\t0.086167\n",
      "Done 460 batches in 69.47s\ttraining loss:\t0.086232\n",
      "Done 480 batches in 72.88s\ttraining loss:\t0.086210\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-acdb6acd611e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhred_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_trimmed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# normal binary cross-entropy with glove init\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/pio/scratch/1/i258346/masters_thesis/SQuAD/HRED_v2.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[1;34m(self, train_data, batch_size, log_interval)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mnum_batch_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m             \u001b[0mtrain_err\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbin_feat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext_mask\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mtrain_batches\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[0mnum_training_words\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnum_batch_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    882\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    883\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 884\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[1;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    885\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[0;32m    987\u001b[0m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0;32m    988\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m--> 989\u001b[1;33m             \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    990\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    991\u001b[0m                 \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/i258346/.local/lib/python2.7/site-packages/theano/scan_module/scan_op.pyc\u001b[0m in \u001b[0;36mp\u001b[1;34m(node, args, outs)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                                 \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m                                                 \u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m                                                 self, node)\n\u001b[0m\u001b[0;32m    979\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgof\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMissingGXX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hred_net.train_one_epoch(data_trimmed, 5, log_interval=20) # normal binary cross-entropy with glove init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hred_net.save_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
